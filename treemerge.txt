
########## ./README.md

# SteadyState

**SteadyState** is a set of free software tools for creating **reproducible**, **ephemeral**, and
**collaborative** development environments directly from Git repositories. It
consists of two main components:

* **SteadyState CLI** â€” a Rust-based command-line client used to authenticate,
  launch, and manage sessions.
* **SteadyState Backend** â€” a Rust/Axum service that handles authentication,
  environment orchestration, and session lifecycle.

The entire system is designed to provide a fast, minimal, transparent way to
enter fully configured cloud developer environments with zero manual setup.

---

## What SteadyState Provides

### 1. Reproducible Environments

SteadyState builds development environments from repository definitions such as:

* `flake.nix`
* `default.nix` / `shell.nix`
* (future) `requirements.txt`, `renv.lock`, `uv.lock`, etc.

These environments run remotely and are defined declaratively for consistent, deterministic behavior.

### 2. Ephemeral, On-Demand Sessions

Users can request a fresh environment at any time.
Each session:

* Runs in the cloud (initial target: Hetzner)
* Is short-lived by default
* Automatically cleans up after use
* Provides a complete dev environment without any local dependencies

### 3. Collaborative Access

Each session exposes a multi-user SSH endpoint through an embedded collaboration
layer (using [Upterm](https://upterm.dev/)). This allows:

* Pair programming
* Live debugging
* Working from any editor (VS Code, Neovim, Emacs, etc.)

Environments ship with [ne](https://github.com/vigna/ne/), the nice editor, for
users who want a minimal in-terminal editor without needing an SSH-aware GUI.

### 4. Authentication System

The backend implements OAuth Device Flow authentication.
The architecture is designed to support:

* GitHub (MVP)
* GitLab (future)
* Orchid (future)
* Enterprise identity providers (future)

Authentication uses short-lived JWTs with long-lived refresh tokens stored securely by the CLI.

---

## Repository Structure

```
steadystate/
â”œâ”€â”€ cli/            # Rust CLI: authentication, session management
â”œâ”€â”€ backend/        # Rust backend: auth providers, session orchestration
â”œâ”€â”€ common/         # Shared types and utilities (planned)
â””â”€â”€ flake.nix       # Reproducible development environment for all components
```

* The **CLI** and **Backend** are independent Rust crates.
* The **flake.nix** provides consistent dev and build tooling across the project.
* A shared crate (`common/`) will consolidate types, JWT logic, and error structures across components.

---

## Design Philosophy

### Minimal local state

Everything needed for development lives in the remote environment.

### Ephemeral by default

Every session starts clean and predictable.

### Transparent and scriptable

SteadyState is a *tool*, not a platform you must commit to.
Both CLI and backend expose simple interfaces that integrate with existing workflows.

### Reproducibility above all

Nix is used to define and build environments in a controlled, deterministic way.
For non-Nix users, `steadystate` will try to bootstrap the environment from
common lock files.

### Editor-agnostic collaboration

SSH is the backbone. Bring your own editor, or use `ne`.

### Collaboration Modes

SteadyState supports two distinct modes for collaboration:

#### 1. Pair Programming (`--mode=pair`)
*   **Powered by Upterm**: Creates a secure SSH tunnel to the host's terminal.
*   **Shared Terminal**: All users share the same terminal session and file system state.
*   **Ideal for**: Real-time pair programming, debugging, and teaching.

#### 2. Shared Workspace (`--mode=collab`)
*   **Shared Host, Isolated Worktrees**: All users connect to the same compute instance but work in their own isolated Git worktrees.
*   **Canonical Repository**: A central bare repository (`canonical`) acts as the synchronization point.
*   **Conflict-Free Sync Model**:
    *   **Session Branch Isolation**: Each session operates on a dedicated Git branch (`steadystate/collab/<session_id>`), isolating session work from the upstream `main` branch.
    *   **Y-CRDT Merge Engine**: `steadystate sync` uses a Yjs/Yrs-based CRDT merge engine to perform conflict-free 3-way merges (Base, Local, Canonical) for text files. This ensures that concurrent edits from multiple users are merged deterministically without manual conflict resolution.
    *   **Git Integration**: The merged result is committed to the session branch. Users can merge the session branch back to `main` via standard Pull Requests after the session.
*   **Ideal for**: Async collaboration, dividing work on the same feature, and avoiding "it works on my machine" issues.

---

## Components

### CLI

* Handles OAuth device flow login (`login`)
* Shows the authenticated identity (`whoami`)
* Refreshes and revokes tokens
* Creates development sessions (`up`)
* Supports a `--noenv` mode for fast, editor-only sessions

### Backend

* Manages authentication via modular providers
* Issues and validates JWTs
* Creates and tracks ephemeral dev sessions
* Provides an API consumed by the CLI

---

## Roadmap

### Authentication

* [x] GitHub OAuth Device Flow
* [ ] GitLab
* [ ] Orchid
* [ ] Enterprise SSO

### Sessions

* [ ] Hetzner Cloud orchestration
* [ ] Optional persistent volumes
* [ ] Resource telemetry and usage reporting
* [ ] Web dashboard

### Environment Handling

* [x] Pure Nix environments
* [ ] Lightweight compatibility layers:

  * `requirements.txt`
  * `uv.lock`
  * `renv.lock`
  * `environment.yml`
* [ ] Prebuilt environment cache

---

## Contributing

### Requirements

Nix.

### How to

PRs welcome, but make sure unit tests pass (or add required unit tests if you add features).

## License

**CLI:** GNU General Public License v3.0
**Backend:** GNU Affero General Public License v3.0

Â© 2025 The Exact Computing Company

########## ./packages/common/Cargo.toml

[package]
name = "steadystate-common"
version = "0.0.1"
edition = "2024"

[lib]
path = "src/lib.rs"

[dependencies]

########## ./packages/common/src/lib.rs

//! Common utilities for the SteadyState workspace.

pub fn placeholder() {}

########## ./cli/README.md

# SteadyState CLI

The SteadyState CLI manages authentication and development sessions for the SteadyState platform.
It is a standalone binary built from the sources in this directory.

## Installation

```bash
# Clone the repository
git clone https://github.com/exactcomputing/steadystate.git
cd steadystate/cli

# Build with Cargo
cargo install --path .
```

Alternatively, run the CLI directly with `cargo run -- <command>`.

## Commands

| Command | Description |
| --- | --- |
| `steadystate login` | Start the OAuth device flow and store the resulting session. |
| `steadystate whoami` | Show the currently authenticated user. Add `--json` for machine-readable output. |
| `steadystate refresh` | Force-refresh the JWT using the stored refresh token. |
| `steadystate logout` | Revoke the refresh token (if possible) and clear local session files. |
| `steadystate up <repo>` | Create a remote development session for the given repository URL. Add `--json` for structured output. |
| `steadystate --version` or `-v` | Print the CLI version. |

## Environment Variables

- `STEADYSTATE_BACKEND` â€” overrides the API base URL.
  - Defaults to `https://localhost:8080` for safety.
  - For local development against a non-TLS backend, export `STEADYSTATE_BACKEND=http://localhost:8080`.
- `RUST_LOG` â€” controls logging (e.g. `RUST_LOG=info` or `RUST_LOG=steadystate=debug`). When set to `debug`, the CLI emits a warning because logs may contain sensitive tokens.
- `STEADYSTATE_CONFIG_DIR` â€” optional override for the configuration directory (used mainly for testing).

## Troubleshooting

- **Backend unreachable**: Ensure the backend is running and confirm `STEADYSTATE_BACKEND` points to the correct URL.
- **Refresh token expired**: Run `steadystate login` to start a new session.
- **Permission denied writing session file**: The CLI attempts to enforce `0600` permissions. On unusual filesystems manually adjust permissions and retry.
- **Debug logs leaking tokens**: Avoid `RUST_LOG=debug` in production environments; tokens can appear in logs.

For more detailed walkthroughs, see the top-level repository README.

## License
Copyright (C) 2025 The Exact Computing Company

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License version 3,
as published by the Free Software Foundation.



########## ./cli/tests/cli_coverage.rs

use std::fs;
use std::io::{Read, Write};
use std::net::TcpListener;
use std::process::Command;
use std::time::Duration;

use tempfile::TempDir;

fn write_session(tempdir: &TempDir, login: &str, jwt: &str, jwt_exp: Option<u64>) {
    let service_dir = tempdir.path().join("steadystate");
    fs::create_dir_all(&service_dir).expect("create service dir");
    let session_path = service_dir.join("session.json");
    let session = serde_json::json!({
        "login": login,
        "jwt": jwt,
        "jwt_exp": jwt_exp,
    });
    fs::write(&session_path, serde_json::to_vec_pretty(&session).unwrap())
        .expect("write session file");
}

fn run_cli(
    tempdir: Option<&TempDir>,
    extra_env: &[(&str, String)],
    args: &[&str],
) -> std::process::Output {
    let mut cmd = Command::new(env!("CARGO_BIN_EXE_steadystate"));
    if let Some(dir) = tempdir {
        cmd.env("STEADYSTATE_CONFIG_DIR", dir.path());
    }
    for (key, value) in extra_env {
        cmd.env(key, value);
    }
    cmd.args(args);
    cmd.output().expect("run steadystate cli")
}

#[test]
fn whoami_displays_human_output_when_logged_in() {
    let tempdir = TempDir::new().expect("tempdir");
    write_session(&tempdir, "test-user", "unused", None);

    let output = run_cli(Some(&tempdir), &[], &["whoami"]);
    assert!(output.status.success());
    let stdout = String::from_utf8(output.stdout).unwrap();
    assert!(stdout.contains("Logged in as: test-user"));
    assert!(!stdout.contains("\"logged_in\""));
}

#[test]
fn whoami_displays_json_when_logged_in() {
    let tempdir = TempDir::new().expect("tempdir");
    write_session(&tempdir, "json-user", "unused", Some(1_234_567));

    let output = run_cli(Some(&tempdir), &[], &["whoami", "--json"]);
    assert!(output.status.success());
    let stdout = String::from_utf8(output.stdout).unwrap();
    let value: serde_json::Value = serde_json::from_str(&stdout).expect("parse json");
    assert_eq!(value["logged_in"], true);
    assert_eq!(value["login"], "json-user");
    assert_eq!(value["jwt_expires_at"], 1_234_567);
}

#[test]
fn whoami_reports_missing_session_plain_text() {
    let tempdir = TempDir::new().expect("tempdir");

    let output = run_cli(Some(&tempdir), &[], &["whoami"]);
    assert!(output.status.success());
    let stdout = String::from_utf8(output.stdout).unwrap();
    assert!(stdout.contains("No active session found. Run 'steadystate login' first."));
}

#[test]
fn whoami_reports_missing_session_json() {
    let tempdir = TempDir::new().expect("tempdir");

    let output = run_cli(Some(&tempdir), &[], &["whoami", "--json"]);
    assert!(output.status.success());
    let stdout = String::from_utf8(output.stdout).unwrap();
    let value: serde_json::Value = serde_json::from_str(&stdout).expect("parse json");
    assert_eq!(value["logged_in"], false);
    assert!(value["login"].is_null());
    assert!(value["jwt_expires_at"].is_null());
}

#[test]
fn version_flag_prints_version() {
    let output = run_cli(None, &[], &["--version"]);
    assert!(output.status.success());
    let stdout = String::from_utf8(output.stdout).unwrap();
    assert!(stdout.contains("SteadyState CLI version"));
}

#[test]
fn running_without_subcommand_prints_help() {
    let output = run_cli(None, &[], &[]);
    assert!(output.status.success());
    let stdout = String::from_utf8(output.stdout).unwrap();
    assert!(stdout.contains("SteadyState CLI â€” Exact reproducible dev envs"));
}

#[test]
fn up_rejects_invalid_repository_url() {
    let tempdir = TempDir::new().expect("tempdir");
    write_session(&tempdir, "tester", "jwt", Some(4_000_000_000));

    let output = run_cli(Some(&tempdir), &[], &["up", "not-a-url", "--env=noenv"]);
    assert!(!output.status.success());
    let stdout = String::from_utf8(output.stdout).unwrap();
    assert!(stdout.contains("Invalid repository URL"));
}

fn spawn_mock_server(response_body: String) -> (String, std::thread::JoinHandle<String>) {
    let listener = TcpListener::bind("127.0.0.1:0").expect("bind mock server");
    let addr = listener.local_addr().unwrap();
    let handle = std::thread::spawn(move || {
        let (mut stream, _) = listener.accept().expect("accept connection");
        stream
            .set_read_timeout(Some(Duration::from_secs(5)))
            .expect("set read timeout");
        let mut buffer = Vec::new();
        let request_len = loop {
            let mut chunk = [0u8; 1024];
            let n = stream.read(&mut chunk).expect("read request");
            if n == 0 {
                break buffer.len();
            }
            buffer.extend_from_slice(&chunk[..n]);
            if let Some(len) = full_request_length(&buffer) {
                break len;
            }
        };
        let request = String::from_utf8_lossy(&buffer[..request_len]).to_string();
        let response = format!(
            "HTTP/1.1 200 OK\r\nContent-Type: application/json\r\nContent-Length: {}\r\n\r\n{}",
            response_body.len(),
            response_body
        );
        stream
            .write_all(response.as_bytes())
            .expect("write response");
        request
    });
    (format!("http://{}", addr), handle)
}

fn full_request_length(buffer: &[u8]) -> Option<usize> {
    let header_end = buffer.windows(4).position(|w| w == b"\r\n\r\n")?;
    let headers = &buffer[..header_end + 4];
    let headers_str = std::str::from_utf8(headers).ok()?;
    let content_length = headers_str
        .lines()
        .find_map(|line| {
            let (name, value) = line.split_once(':')?;
            if name.trim().eq_ignore_ascii_case("Content-Length") {
                value.trim().parse::<usize>().ok()
            } else {
                None
            }
        })
        .unwrap_or(0);
    let total = header_end + 4 + content_length;
    if buffer.len() >= total {
        Some(total)
    } else {
        None
    }
}

fn create_session_with_future_expiry(tempdir: &TempDir) {
    let future = std::time::SystemTime::now()
        .checked_add(Duration::from_secs(3_600))
        .unwrap()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs();
    write_session(tempdir, "tester", "test-jwt", Some(future));
}

#[test]
fn up_prints_human_output_on_success() {
    let tempdir = TempDir::new().expect("tempdir");
    create_session_with_future_expiry(&tempdir);

    let response = r#"{"id":"session-123","state":"Running","endpoint":"ssh://example.com","message":null,"compute_provider":"local"}"#.to_string();
    let (base_url, handle) = spawn_mock_server(response);

    let output = run_cli(
        Some(&tempdir),
        &[("STEADYSTATE_BACKEND", base_url.clone())],
        &["up", "https://example.com/repo.git", "--env=noenv", "--mode=pair"],
    );
    assert!(output.status.success());

    let request = handle.join().expect("join mock server");
    assert!(request.starts_with("POST /sessions"));
    assert!(
        request
            .to_lowercase()
            .contains("authorization: bearer test-jwt")
    );

    let stdout = String::from_utf8(output.stdout).unwrap();
    assert!(stdout.contains("âœ… Session created: session-123"));
    assert!(stdout.contains("SSH: ssh://example.com"));
}

#[test]
fn up_prints_json_on_success() {
    let tempdir = TempDir::new().expect("tempdir");
    create_session_with_future_expiry(&tempdir);

    let response = r#"{"id":"session-456","state":"Running","endpoint":"ssh://json.example.com","message":null,"compute_provider":"local"}"#.to_string();
    let (base_url, handle) = spawn_mock_server(response);

    let output = run_cli(
        Some(&tempdir),
        &[("STEADYSTATE_BACKEND", base_url.clone())],
        &["up", "https://example.com/repo.git", "--env=noenv", "--json", "--mode=pair"],
    );
    assert!(output.status.success());

    let request = handle.join().expect("join mock server");
    assert!(request.starts_with("POST /sessions"));
    assert!(
        request
            .to_lowercase()
            .contains("authorization: bearer test-jwt")
    );

    let stdout = String::from_utf8(output.stdout).unwrap();
    let value: serde_json::Value = serde_json::from_str(&stdout).expect("parse json");
    assert_eq!(value["id"], "session-456");
    assert_eq!(value["endpoint"], "ssh://json.example.com");
}

########## ./cli/tests/no_keyring_test.rs

#![cfg(not(target_os = "macos"))]

use std::fs;
use std::io::{Read, Write};
use std::net::TcpListener;
use std::process::{Command, Output};
use std::time::Duration;
use tempfile::TempDir;
use serde_json::json;

// Copied helpers to avoid dependency issues and allow modification
mod helpers {
    use super::*;

    pub enum MockResponse {
        Json(serde_json::Value),
        Ok,
    }

    impl MockResponse {
        fn into_http_string(self) -> String {
            match self {
                MockResponse::Json(val) => {
                    let body = serde_json::to_string(&val).unwrap();
                    format!(
                        "HTTP/1.1 200 OK\r\n\
                         Content-Type: application/json\r\n\
                         Connection: close\r\n\
                         Content-Length: {}\r\n\r\n{}",
                        body.len(),
                        body
                    )
                }
                MockResponse::Ok => {
                    "HTTP/1.1 200 OK\r\n\
                     Connection: close\r\n\
                     Content-Length: 0\r\n\r\n"
                        .to_string()
                }
            }
        }
    }

    pub struct TestHarness {
        pub tempdir: TempDir,
        pub server_url: String,
        server_handle: Option<std::thread::JoinHandle<Vec<String>>>,
    }

    impl TestHarness {
        pub fn new(script: Vec<MockResponse>) -> Self {
            let tempdir = TempDir::new().expect("create tempdir");
            let (server_url, server_handle) = spawn_scripted_server(script);
            Self {
                tempdir,
                server_url,
                server_handle: Some(server_handle),
            }
        }

        pub fn run_cli_no_keyring(&self, args: &[&str]) -> Output {
            let mut cmd = Command::new(env!("CARGO_BIN_EXE_steadystate"));
            cmd.env("STEADYSTATE_CONFIG_DIR", self.tempdir.path());
            cmd.env("STEADYSTATE_BACKEND", &self.server_url);
            // IMPORTANT: Do NOT set STEADYSTATE_KEYRING_DIR
            // Set NO_KEYRING to force fallback
            cmd.env("STEADYSTATE_NO_KEYRING", "1");
            cmd.args(args);
            cmd.output().expect("run steadystate cli")
        }

        pub fn join_server(&mut self) -> Vec<String> {
            self.server_handle.take().unwrap().join().unwrap()
        }

        pub fn create_session(&self, login: &str, jwt: &str, jwt_exp: Option<u64>) {
            let dir = self.tempdir.path().join("steadystate");
            fs::create_dir_all(&dir).unwrap();
            let path = dir.join("session.json");
            let session = json!({ "login": login, "jwt": jwt, "jwt_exp": jwt_exp });
            fs::write(path, serde_json::to_vec_pretty(&session).unwrap()).unwrap();
        }

        pub fn create_future_session(&self) {
            let exp = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs() + 3600;
            self.create_session("tester", "test-jwt", Some(exp));
        }

        pub fn set_fallback_token(&self, token: &str) {
            let dir = self.tempdir.path().join("steadystate");
            fs::create_dir_all(&dir).unwrap();
            let path = dir.join("credentials");
            fs::write(path, token).unwrap();
        }

        pub fn fallback_token_exists(&self) -> bool {
            self.tempdir.path().join("steadystate/credentials").exists()
        }
    }

    fn spawn_scripted_server(
        responses: Vec<MockResponse>,
    ) -> (String, std::thread::JoinHandle<Vec<String>>) {
        let listener = TcpListener::bind("127.0.0.1:0").unwrap();
        let addr = listener.local_addr().unwrap();

        let handle = std::thread::spawn(move || {
            let mut reqs = Vec::new();
            for response in responses {
                let (mut stream, _) = listener.accept().unwrap();
                stream.set_read_timeout(Some(Duration::from_secs(5))).unwrap();
                let mut buf = vec![0; 1024];
                if let Ok(n) = stream.read(&mut buf) {
                    reqs.push(String::from_utf8_lossy(&buf[..n]).to_string());
                }
                let resp = response.into_http_string();
                stream.write_all(resp.as_bytes()).unwrap();
            }
            reqs
        });

        (format!("http://{}", addr), handle)
    }
}

use helpers::{MockResponse, TestHarness};

#[test]
fn logout_with_no_keyring_removes_fallback_file() {
    let script = vec![MockResponse::Ok];
    let mut harness = TestHarness::new(script);
    harness.create_future_session();
    harness.set_fallback_token("token-in-file");

    let output = harness.run_cli_no_keyring(&["logout"]);
    
    if !output.status.success() {
        eprintln!("STDOUT: {}", String::from_utf8_lossy(&output.stdout));
        eprintln!("STDERR: {}", String::from_utf8_lossy(&output.stderr));
        panic!("CLI failed");
    }

    let reqs = harness.join_server();
    assert_eq!(reqs.len(), 1);
    assert!(reqs[0].contains("token-in-file")); // Verify it read the token from file
    assert!(!harness.fallback_token_exists()); // Verify it deleted the file
}

#[test]
fn refresh_with_no_keyring_uses_fallback_file() {
    let script = vec![MockResponse::Json(json!({ "jwt": "new-jwt" }))];
    let mut harness = TestHarness::new(script);
    // Create session but with expired JWT to force refresh? 
    // Or just run `refresh` command explicitly.
    harness.create_future_session();
    harness.set_fallback_token("file-refresh-token");

    let output = harness.run_cli_no_keyring(&["refresh"]);

    if !output.status.success() {
        eprintln!("STDOUT: {}", String::from_utf8_lossy(&output.stdout));
        eprintln!("STDERR: {}", String::from_utf8_lossy(&output.stderr));
        panic!("CLI failed");
    }

    let reqs = harness.join_server();
    assert_eq!(reqs.len(), 1);
    assert!(reqs[0].contains("file-refresh-token"));
}

########## ./cli/tests/auth_integration.rs

#![cfg(not(target_os = "macos"))]

// cli/tests/auth_integration.rs

use std::fs;
use std::io::{Read, Write};
use std::net::TcpListener;
use std::process::{Command, Output};
use std::time::Duration;
use tempfile::TempDir;

// Using a module to encapsulate all the test infrastructure.
mod helpers {
    use super::*;
    use serde_json::json;

    pub enum MockResponse {
        Json(serde_json::Value),
        Unauthorized,
        Ok,
    }

    impl MockResponse {
        fn into_http_string(self) -> String {
            match self {
                MockResponse::Json(val) => {
                    let body = serde_json::to_string(&val).unwrap();
                    format!(
                        "HTTP/1.1 200 OK\r\n\
                         Content-Type: application/json\r\n\
                         Connection: close\r\n\
                         Content-Length: {}\r\n\r\n{}",
                        body.len(),
                        body
                    )
                }
                MockResponse::Unauthorized => {
                    "HTTP/1.1 401 Unauthorized\r\n\
                     Connection: close\r\n\
                     Content-Length: 0\r\n\r\n"
                        .to_string()
                }
                MockResponse::Ok => {
                    "HTTP/1.1 200 OK\r\n\
                     Connection: close\r\n\
                     Content-Length: 0\r\n\r\n"
                        .to_string()
                }
            }
        }
    }

    pub struct TestHarness {
        pub tempdir: TempDir,
        pub server_url: String, // Made public for direct use in failure tests
        server_handle: Option<std::thread::JoinHandle<Vec<String>>>,
    }

    impl TestHarness {
        pub fn new(script: Vec<MockResponse>) -> Self {
            let tempdir = TempDir::new().expect("create tempdir");
            let (server_url, server_handle) = spawn_scripted_server(script);
            Self {
                tempdir,
                server_url,
                server_handle: Some(server_handle),
            }
        }

        pub fn run_cli_and_assert_success(&mut self, args: &[&str]) -> (Output, Vec<String>) {
            let output = self.run_cli(args);
            let requests = self.join_server();

            if !output.status.success() {
                eprintln!("=== CLI STDOUT ===\n{}", String::from_utf8_lossy(&output.stdout));
                eprintln!("=== CLI STDERR ===\n{}", String::from_utf8_lossy(&output.stderr));
                eprintln!("=== SERVER REQUESTS ===");
                for (i, r) in requests.iter().enumerate() {
                    eprintln!("--- Request {} ---\n{}\n", i, r);
                }
                panic!("CLI failed unexpectedly");
            }

            (output, requests)
        }

        pub fn run_cli(&self, args: &[&str]) -> Output {
            let mut cmd = Command::new(env!("CARGO_BIN_EXE_steadystate"));
            cmd.env("STEADYSTATE_CONFIG_DIR", self.tempdir.path());
            cmd.env("STEADYSTATE_BACKEND", &self.server_url);
            // Set mock keyring dir
            cmd.env("STEADYSTATE_KEYRING_DIR", self.tempdir.path());
            cmd.args(args);
            cmd.output().expect("run steadystate cli")
        }

        pub fn join_server(&mut self) -> Vec<String> {
            self.server_handle.take().unwrap().join().unwrap()
        }

        pub fn create_session(&self, login: &str, jwt: &str, jwt_exp: Option<u64>) {
            let dir = self.tempdir.path().join("steadystate");
            fs::create_dir_all(&dir).unwrap();
            let path = dir.join("session.json");
            let session = json!({ "login": login, "jwt": jwt, "jwt_exp": jwt_exp });
            fs::write(path, serde_json::to_vec_pretty(&session).unwrap()).unwrap();
        }

        pub fn create_future_session(&self) {
            let exp = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs() + 3600;
            self.create_session("tester", "test-jwt", Some(exp));
        }

        pub fn create_expired_session(&self) {
            let exp = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs() - 10;
            self.create_session("tester", "expired-jwt", Some(exp));
        }

        pub fn set_keyring_password(&self, username: &str, password: &str) {
            // Use mock keyring file
            let path = self.tempdir.path().join(format!("{}.keyring", username));
            fs::write(path, password).unwrap();
        }
        
        pub fn get_keyring_password(&self, username: &str) -> std::io::Result<String> {
            let path = self.tempdir.path().join(format!("{}.keyring", username));
            fs::read_to_string(path)
        }
    }

    /// Minimal, robust, "headers-only" mock server.
    fn spawn_scripted_server(
        responses: Vec<MockResponse>,
    ) -> (String, std::thread::JoinHandle<Vec<String>>) {
        let listener = TcpListener::bind("127.0.0.1:0").unwrap();
        let addr = listener.local_addr().unwrap();

        let handle = std::thread::spawn(move || {
            let mut reqs = Vec::new();
            for response in responses {
                let (mut stream, _) = listener.accept().unwrap();
                stream.set_read_timeout(Some(Duration::from_secs(5))).unwrap();
                let mut buf = vec![0; 1024]; // Simple buffer is enough for headers
                if let Ok(n) = stream.read(&mut buf) {
                    reqs.push(String::from_utf8_lossy(&buf[..n]).to_string());
                }
                let resp = response.into_http_string();
                stream.write_all(resp.as_bytes()).unwrap();
            }
            reqs
        });

        (format!("http://{}", addr), handle)
    }
}

// ---------------------------------------------------------------------------
// NEW, CORRECTED INTEGRATION TESTS
// ---------------------------------------------------------------------------

use helpers::{MockResponse, TestHarness};
use serde_json::json;

#[test]
fn up_refreshes_proactively_when_jwt_expired() {
    let script = vec![
        // Proactive refresh call
        MockResponse::Json(json!({ "jwt": "fresh-jwt-123" })),
        // Session creation call
        MockResponse::Json(json!({ "id": "session-xyz", "state": "Running", "endpoint": "ssh://fresh", "message": null, "compute_provider": "local" })),
    ];
    let mut harness = TestHarness::new(script);
    harness.create_expired_session();
    harness.set_keyring_password("tester", "refresh-token-abc");

    let (out, reqs) = harness.run_cli_and_assert_success(&["up", "https://github.com/example/repo", "--env=noenv", "--mode=pair"]);

    assert_eq!(reqs.len(), 2);
    assert!(reqs[0].starts_with("POST /auth/refresh"));
    assert!(reqs[1].starts_with("POST /sessions"));
    assert!(String::from_utf8_lossy(&out.stdout).contains("session-xyz"));
}


#[test]
fn up_errors_gracefully_if_server_returns_401() {
    let script = vec![
        // The server will immediately reject our valid JWT
        MockResponse::Unauthorized,
    ];
    let mut harness = TestHarness::new(script);
    harness.create_future_session(); // Create a session with a non-expired JWT

    // Run the CLI but don't assert success
    let output = harness.run_cli(&["up", "https://github.com/example/repo", "--env=noenv", "--mode=pair"]);
    harness.join_server();

    // Assert that the command failed as expected
    assert!(!output.status.success(), "CLI should exit with a non-zero status");
    
    let stderr = String::from_utf8_lossy(&output.stderr);
    
    // This is the key part of the error message from our `anyhow::bail!` call.
    // By checking for this specific substring, we are robust against formatting
    // changes from the `tracing` crate or the error prefix in `main.rs`.
    let expected_error_substring = "Your session has expired or been revoked";

    assert!(
        stderr.contains(expected_error_substring),
        "Stderr should contain the correct error message.\n\nExpected to find: '{}'\n\nActual stderr:\n---\n{}\n---",
        expected_error_substring,
        stderr
    );
}

#[test]
fn logout_removes_session_and_revokes_refresh() {
    let script = vec![MockResponse::Ok];
    let mut harness = TestHarness::new(script);
    harness.create_future_session();
    harness.set_keyring_password("tester", "refresh-to-revoke");

    let (_, reqs) = harness.run_cli_and_assert_success(&["logout"]);

    assert_eq!(reqs.len(), 1);
    assert!(reqs[0].starts_with("POST /auth/revoke"));
    let json_path = harness.tempdir.path().join("steadystate/session.json");
    assert!(!json_path.exists());
    let res = harness.get_keyring_password("tester");
    assert!(res.is_err()); // Should be deleted
}

########## ./cli/Cargo.toml

[package]
name = "steadystate"
version = "0.0.2"
edition = "2024"

[dependencies]
# Async runtime
tokio = { version = "1.40", features = ["full"] }

# HTTP client, pure-Rust TLS
reqwest = { version = "0.12", default-features = false, features = ["json", "rustls-tls"] }

# JWT library, pure Rust only
jwt-simple = { version = "0.12", default-features = false, features = ["pure-rust"] }

# Serialization / deserialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# CLI argument parsing
clap = { version = "4.5", features = ["derive"] }

# Error handling
anyhow = "1.0"

# Secure credential storage
keyring = { version = "3.6", features = ["apple-native", "windows-native", "linux-native"] }

# Platform directories (config, cache, etc.)
dirs = "6.0"

# Logging and tracing
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Open URLs in default browser
open = "5.3"

# Progress indicators
indicatif = "0.18.2"

# Lazily initialized globals
once_cell = "1.19"

# File locking for session management
fs2 = "0.4"
url = "2.5"
ratatui = "0.29.0"
crossterm = "0.29.0"

# CRDT Merge Engine
yrs = "0.18"
similar = "2.4"
walkdir = "2"


[dev-dependencies]
tempfile = "3.10"

########## ./cli/src/sync.rs

use anyhow::{Context, Result};
use std::process::Command;
use std::path::Path;
use std::io::Write;
use std::fs;
use std::time::SystemTime;
use serde::{Deserialize, Serialize};
use crate::merge;
use walkdir::WalkDir;
use fs2::FileExt;

#[derive(Serialize, Deserialize)]
struct WorktreeMeta {
    session_branch: String,
    last_synced_commit: String,
}

pub async fn sync() -> Result<()> {
    println!("Syncing changes (Y-CRDT)...");

    // 1. Determine paths
    let repo_root = std::env::var("REPO_ROOT").unwrap_or_else(|_| "..".to_string());
    // Resolve repo_root relative to current dir if it's relative
    let repo_root_path = if Path::new(&repo_root).is_absolute() {
        Path::new(&repo_root).to_path_buf()
    } else {
        std::env::current_dir()?.join(&repo_root).canonicalize().unwrap_or_else(|_| Path::new(&repo_root).to_path_buf())
    };
    
    let canonical_path = repo_root_path.join("canonical");
    
    let worktree_path = std::env::current_dir().context("Failed to get current dir")?;
    let meta_dir = worktree_path.join(".worktree");
    let meta_path = meta_dir.join("steadystate.json");

    // 2. Load metadata
    if !meta_path.exists() {
        return Err(anyhow::anyhow!("Metadata file not found at {}. Session not initialized correctly.", meta_path.display()));
    }
    
    let content = fs::read_to_string(&meta_path).context("Failed to read metadata")?;
    let meta: WorktreeMeta = serde_json::from_str(&content).context("Failed to parse metadata")?;
    
    let base_commit = meta.last_synced_commit;
    let session_branch = meta.session_branch;

    println!("Base commit: {}", base_commit);
    println!("Session branch: {}", session_branch);

    // Scope the lock so it is released before push
    {
        // Lock canonical to prevent concurrent syncs
        let _lock = lock_canonical(&canonical_path)?;

        // 3. Fetch latest changes BEFORE materializing
        println!("Fetching latest changes...");
        let fetch_status = Command::new("git")
            .arg("-C")
            .arg(&canonical_path)
            .args(&["fetch", "origin", &session_branch])
            .status()
            .context("Failed to fetch from origin")?;
        
        let canonical_tree = if !fetch_status.success() {
            // Check if it's because the branch doesn't exist
            // If so, we treat it as a new branch (first push)
            // Canonical state is effectively the base state (no remote changes yet)
            println!("âš ï¸  Remote branch not found. Assuming first push for this session.");
            println!("   Using base commit as canonical state.");
            
            // Materialize base tree as canonical tree
            merge::materialize_git_tree(&canonical_path, &base_commit).context("Failed to materialize base tree (as canonical)")?
        } else {
            // Use origin/session_branch as canonical ref
            let canonical_ref = format!("origin/{}", session_branch);
            
            // Verify the ref exists
            let verify_status = Command::new("git")
                .arg("-C")
                .arg(&canonical_path)
                .args(&["rev-parse", "--verify", &canonical_ref])
                .status()?;
            
            if !verify_status.success() {
                // Should not happen if fetch succeeded, but just in case
                return Err(anyhow::anyhow!(
                    "Remote branch {} does not exist after successful fetch.",
                    canonical_ref
                ));
            }

            // CRITICAL FIX: Reset local branch to match remote
            println!("Updating local branch to match remote...");
            let reset_status = Command::new("git")
                .arg("-C")
                .arg(&canonical_path)
                .args(&["reset", "--hard", &canonical_ref])
                .status()
                .context("Failed to reset local branch")?;

            if !reset_status.success() {
                return Err(anyhow::anyhow!("Failed to reset to remote branch"));
            }
            
            merge::materialize_git_tree(&canonical_path, &canonical_ref).context("Failed to materialize canonical tree")?
        };

        // 4. Materialize trees
        println!("Materializing trees...");
        let base_tree = merge::materialize_git_tree(&canonical_path, &base_commit).context("Failed to materialize base tree")?;
        
        let local_tree = merge::materialize_fs_tree(&worktree_path).context("Failed to materialize local tree")?;

        // Check for changes before merging
        println!("Detecting changes...");
        let local_changed = local_tree.files.len() != base_tree.files.len() 
            || local_tree.files.iter().any(|(k, v)| base_tree.files.get(k) != Some(v));
        let remote_changed = canonical_tree.files.len() != base_tree.files.len()
            || canonical_tree.files.iter().any(|(k, v)| base_tree.files.get(k) != Some(v));

        if !local_changed && !remote_changed {
            println!("No changes detected. Already up to date.");
            return Ok(());
        }

        if local_changed && !remote_changed {
            println!("Only local changes detected.");
        } else if !local_changed && remote_changed {
            println!("Only remote changes detected.");
        } else {
            println!("Both local and remote changes detected. Merging...");
        }

        // 5. Merge
        println!("Merging...");
        let merged_tree = merge::merge_trees(&base_tree, &local_tree, &canonical_tree).context("Merge failed")?;

        // 6. Apply to canonical with safety checks and backup
        println!("Creating safety backup...");
        let backup_ref = format!("refs/backups/sync-{}", 
            SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs()
        );
        
        let current_head = get_git_head(&canonical_path)?;
        Command::new("git")
            .arg("-C")
            .arg(&canonical_path)
            .args(&["update-ref", &backup_ref, &current_head])
            .status()
            .context("Failed to create backup ref")?;

        println!("Applying to canonical...");
        if let Err(e) = apply_tree_to_canonical(&canonical_path, &merged_tree, &session_branch) {
            eprintln!("âŒ Failed to apply tree: {}", e);
            eprintln!("ðŸ”„ Attempting recovery from backup...");
            
            // Restore from backup
            let reset_status = Command::new("git")
                .arg("-C")
                .arg(&canonical_path)
                .args(&["reset", "--hard", &backup_ref])
                .status();
                
            match reset_status {
                Ok(status) if status.success() => {
                    return Err(anyhow::anyhow!(
                        "Sync failed but successfully restored previous state. Error: {}", 
                        e
                    ));
                }
                _ => {
                    return Err(anyhow::anyhow!(
                        "Sync failed AND recovery failed! Manual intervention required. \
                         Backup ref: {}. Original error: {}",
                        backup_ref,
                        e
                    ));
                }
            }
        }

        // 7. Commit
        println!("Committing...");
        if let Err(e) = commit_changes(&canonical_path, &session_branch) {
            eprintln!("âŒ Failed to commit: {}", e);
            eprintln!("ðŸ”„ Attempting recovery from backup...");
            
            // Restore from backup
            Command::new("git")
                .arg("-C")
                .arg(&canonical_path)
                .args(&["reset", "--hard", &backup_ref])
                .status()?;
                
            return Err(anyhow::anyhow!("Commit failed, restored previous state. Error: {}", e));
        }

        // Clean up backup
        Command::new("git")
            .arg("-C")
            .arg(&canonical_path)
            .args(&["update-ref", "-d", &backup_ref])
            .status()
            .ok();

        // 8. Update metadata IMMEDIATELY after commit
        let new_head = get_git_head(&canonical_path)?;
        let new_meta = WorktreeMeta { 
            session_branch: session_branch.clone(),
            last_synced_commit: new_head 
        };
        fs::create_dir_all(&meta_dir)?;
        fs::write(&meta_path, serde_json::to_string_pretty(&new_meta)?)?;
    } // Lock released here

    // 9. Push (without lock)
    println!("Pushing to remote...");
    let push_status = Command::new("git")
        .arg("-C")
        .arg(&canonical_path)
        .args(&["push", "origin", &session_branch])
        .status()?;
        
    if !push_status.success() { 
        eprintln!("âš ï¸  Warning: Push failed. Your changes are committed locally but not synced to remote.");
        eprintln!("   You can manually push from: {}", canonical_path.display());
        eprintln!("   Run: cd {} && git push origin {}", canonical_path.display(), session_branch);
    }

    // 10. Reset local worktree
    println!("Refreshing worktree...");
    sync_worktree_from_canonical(&canonical_path, &worktree_path)?;

    // 11. Update sync-log
    let sync_log_path = repo_root_path.join("sync-log");
    
    // Try to get username from session, fallback to env var
    let user = match crate::session::read_session(None).await {
        Ok(session) => session.login,
        Err(_) => std::env::var("STEADYSTATE_USERNAME")
            .or_else(|_| std::env::var("USER"))
            .unwrap_or_else(|_| "unknown".to_string()),
    };

    let timestamp = SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap_or_default()
        .as_secs();
        
    append_to_sync_log(&sync_log_path, &user, timestamp)?;

    println!("âœ… Sync complete!");
    Ok(())
}

fn commit_changes(repo_path: &Path, _branch: &str) -> Result<()> {
    let status = Command::new("git")
        .arg("-C")
        .arg(repo_path)
        .args(&["add", "-A"])
        .status()?;
    
    if !status.success() { return Err(anyhow::anyhow!("git add failed")); }

    let diff_status = Command::new("git")
        .arg("-C")
        .arg(repo_path)
        .args(&["diff", "--cached", "--quiet"])
        .status()?;

    if !diff_status.success() {
        let user = std::env::var("USER").unwrap_or_else(|_| "unknown".to_string());
        let msg = format!("sync: SteadyState session by {}", user);
        
        let commit_status = Command::new("git")
            .arg("-C")
            .arg(repo_path)
            .args(&["commit", "-m", &msg, "--author", "SteadyState Bot <bot@steadystate.dev>"])
            .status()?;
            
        if !commit_status.success() { return Err(anyhow::anyhow!("git commit failed")); }
    }
    
    Ok(())
}

fn append_to_sync_log(log_path: &Path, user: &str, timestamp: u64) -> Result<()> {
    let file = std::fs::OpenOptions::new()
        .create(true)
        .append(true)
        .open(log_path)
        .context("Failed to open sync-log")?;
    
    // Lock for append
    file.lock_exclusive()
        .context("Failed to lock sync-log")?;
    
    let log_entry = format!("{} {}\n", timestamp, user);
    (&file).write_all(log_entry.as_bytes())
        .context("Failed to write to sync-log")?;
    
    FileExt::unlock(&file).ok();
    Ok(())
}

fn get_git_head(repo_path: &Path) -> Result<String> {
    let output = Command::new("git")
        .arg("-C")
        .arg(repo_path)
        .args(&["rev-parse", "HEAD"])
        .output()?;
    if !output.status.success() { return Err(anyhow::anyhow!("git rev-parse HEAD failed")); }
    Ok(String::from_utf8_lossy(&output.stdout).trim().to_string())
}

/// WARNING: This function is DESTRUCTIVE.
/// It deletes all files in `repo_path` (except .git) and replaces them with `tree`.
/// This is intended for the ephemeral `canonical` repository used in sessions.
fn apply_tree_to_canonical(
    repo_path: &Path,
    tree: &crate::merge::TreeSnapshot,
    expected_branch: &str,
) -> Result<()> {
    // Safety check 1: Must have .git
    let git_dir = repo_path.join(".git");
    if !git_dir.exists() {
        return Err(anyhow::anyhow!(
            "Safety check failed: {} is not a git repository",
            repo_path.display()
        ));
    }
    
    // Safety check 2: Must end with /canonical
    if !repo_path.ends_with("canonical") {
        return Err(anyhow::anyhow!(
            "Safety check failed: {} does not end with 'canonical'. \
             This function should only be used on ephemeral canonical repos.",
            repo_path.display()
        ));
    }
    
    // Safety check 3: Verify we're on the expected branch
    let current_branch_output = Command::new("git")
        .arg("-C")
        .arg(repo_path)
        .args(&["rev-parse", "--abbrev-ref", "HEAD"])
        .output()
        .context("Failed to get current branch")?;
    
    if !current_branch_output.status.success() {
        return Err(anyhow::anyhow!("Failed to determine current branch"));
    }
    
    let current_branch = String::from_utf8_lossy(&current_branch_output.stdout).trim().to_string();
    if current_branch != expected_branch {
        return Err(anyhow::anyhow!(
            "Safety check failed: Expected branch '{}' but on '{}'. \
             Refusing to apply tree to wrong branch.",
            expected_branch,
            current_branch
        ));
    }
    
    // Safety check 4: Verify working tree is clean - REMOVED as redundant after reset --hard
    // We just performed a hard reset to the remote branch, so the working tree is guaranteed clean.
    
    // All safety checks passed - proceed with deletion
    tracing::info!("Applying tree to {} (passed all safety checks)", repo_path.display());
    
    // Delete everything except .git
    for entry in fs::read_dir(repo_path)? {
        let entry = entry?;
        let path = entry.path();
        if path.file_name().unwrap() == ".git" { 
            continue; 
        }
        
        if path.is_dir() {
            fs::remove_dir_all(&path)
                .with_context(|| format!("Failed to remove directory {}", path.display()))?;
        } else {
            fs::remove_file(&path)
                .with_context(|| format!("Failed to remove file {}", path.display()))?;
        }
    }

    // Write merged files
    for (rel_path, content) in &tree.files {
        let full_path = repo_path.join(rel_path);
        if let Some(parent) = full_path.parent() {
            fs::create_dir_all(parent)
                .with_context(|| format!("Failed to create directory {}", parent.display()))?;
        }
        fs::write(&full_path, content)
            .with_context(|| format!("Failed to write file {}", full_path.display()))?;
    }
    
    Ok(())
}

fn sync_worktree_from_canonical(canonical_path: &Path, worktree_path: &Path) -> Result<()> {
    // 1. Clear worktree (except .worktree and .git if it exists)
    for entry in WalkDir::new(worktree_path).min_depth(1).max_depth(1).into_iter().filter_map(|e| e.ok()) {
        let path = entry.path();
        let name = path.file_name().unwrap().to_string_lossy();
        if name == ".worktree" || name == ".git" {
            continue;
        }
        if path.is_dir() {
            fs::remove_dir_all(path)?;
        } else {
            fs::remove_file(path)?;
        }
    }

    // 2. Copy from canonical (except .git)
    for entry in WalkDir::new(canonical_path).into_iter().filter_map(|e| e.ok()) {
        let path = entry.path();
        if !path.is_file() {
            continue;
        }
        
        let rel_path = path.strip_prefix(canonical_path)?;
        let rel_path_str = rel_path.to_string_lossy();
        
        if rel_path_str.starts_with(".git") || rel_path_str.contains("/.git/") {
            continue;
        }

        let dest_path = worktree_path.join(rel_path);
        if let Some(parent) = dest_path.parent() {
            fs::create_dir_all(parent)?;
        }
        fs::copy(path, dest_path)?;
    }

    Ok(())
}

fn lock_canonical(repo_path: &Path) -> Result<std::fs::File> {
    use fs2::FileExt;
    let lock_path = repo_path.join(".steadystate.lock");
    let file = std::fs::OpenOptions::new()
        .read(true)
        .write(true)
        .create(true)
        .open(&lock_path)
        .context("Failed to open lock file")?;
    
    file.lock_exclusive().context("Failed to acquire lock")?;
    Ok(file)
}

########## ./cli/src/tui.rs

use std::io::{self, Stdout};
use std::time::Duration;
use std::path::PathBuf;

use anyhow::Result;
use crossterm::{
    event::{self, Event, KeyCode, KeyEventKind},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use ratatui::{
    prelude::*,
    widgets::{Block, Borders, List, ListItem, Paragraph, Wrap},
};

pub struct TuiApp {
    session_root: PathBuf,
    activity_log_path: PathBuf,
    sync_log_path: PathBuf,
    activities: Vec<String>,
    should_quit: bool,
    scroll: u16,
}

impl TuiApp {
    pub fn new() -> Result<Self> {
        let session_root = std::env::var("SESSION_ROOT").unwrap_or_else(|_| ".".to_string()).into();
        let activity_log_path = std::env::var("ACTIVITY_LOG").unwrap_or_else(|_| "activity-log".to_string()).into();
        let sync_log_path = std::env::var("SYNC_LOG").unwrap_or_else(|_| "sync-log".to_string()).into();

        Ok(Self {
            session_root,
            activity_log_path,
            sync_log_path,
            activities: Vec::new(),
            should_quit: false,
            scroll: 0,
        })
    }

    pub fn run(&mut self) -> Result<()> {
        enable_raw_mode()?;
        let mut stdout = io::stdout();
        execute!(stdout, EnterAlternateScreen)?;
        let backend = CrosstermBackend::new(stdout);
        let mut terminal = Terminal::new(backend)?;

        loop {
            self.refresh_data()?;
            terminal.draw(|f| self.ui(f))?;

            if event::poll(Duration::from_millis(250))? {
                if let Event::Key(key) = event::read()? {
                    if key.kind == KeyEventKind::Press {
                        match key.code {
                            KeyCode::Char('q') => self.should_quit = true,
                            KeyCode::Char('s') => {
                                // TODO: Trigger sync?
                                // For now, just let user know they can run 'steadystate sync' in another pane
                                // Or we can spawn it.
                            },
                            KeyCode::Up => {
                                if self.scroll > 0 {
                                    self.scroll -= 1;
                                }
                            },
                            KeyCode::Down => {
                                self.scroll += 1;
                            },
                            _ => {}
                        }
                    }
                }
            }

            if self.should_quit {
                break;
            }
        }

        disable_raw_mode()?;
        execute!(terminal.backend_mut(), LeaveAlternateScreen)?;
        terminal.show_cursor()?;

        Ok(())
    }

    fn refresh_data(&mut self) -> Result<()> {
        // Read activity log (tail last 20 lines)
        if self.activity_log_path.exists() {
            let content = std::fs::read_to_string(&self.activity_log_path)?;
            self.activities = content.lines().rev().take(50).map(|s| s.to_string()).collect();
        }
        Ok(())
    }

    fn ui(&self, f: &mut Frame) {
        let chunks = Layout::default()
            .direction(Direction::Vertical)
            .constraints([
                Constraint::Length(3), // Header
                Constraint::Min(0),    // Body
                Constraint::Length(3), // Footer
            ])
            .split(f.size());

        // Header
        let title = Paragraph::new(format!("SteadyState Session: {}", self.session_root.display()))
            .block(Block::default().borders(Borders::ALL).title("Status"));
        f.render_widget(title, chunks[0]);

        // Body (Activity Log)
        let items: Vec<ListItem> = self.activities
            .iter()
            .map(|i| ListItem::new(i.as_str()))
            .collect();

        let list = List::new(items)
            .block(Block::default().borders(Borders::ALL).title("Activity Log"))
            .highlight_style(Style::default().add_modifier(Modifier::BOLD));
        
        f.render_widget(list, chunks[1]);

        // Footer
        let footer = Paragraph::new("Press 'q' to quit (detach)")
            .block(Block::default().borders(Borders::ALL));
        f.render_widget(footer, chunks[2]);
    }
}

########## ./cli/src/session.rs

use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use tracing::warn;

use crate::auth::extract_exp_from_jwt;
use crate::config::{CONFIG_OVERRIDE_ENV, SERVICE_NAME};

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Session {
    pub login: String,
    pub jwt: String,
    pub jwt_exp: Option<u64>, // epoch seconds
}

impl Session {
    pub fn new(login: String, jwt: String) -> Self {
        let jwt_exp = extract_exp_from_jwt(&jwt);
        Self {
            login,
            jwt,
            jwt_exp,
        }
    }

    pub fn is_near_expiry(&self, buffer_secs: u64) -> bool {
        if let Some(exp) = self.jwt_exp {
            let now = std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .expect("System time is before UNIX EPOCH")
                .as_secs();
            exp <= now + buffer_secs
        } else {
            false
        }
    }
}

pub async fn get_cfg_dir(override_dir: Option<&PathBuf>) -> Result<PathBuf> {
    let base_dir = match override_dir {
        Some(p) => p.clone(),
        None => {
            if let Ok(override_env) = std::env::var(CONFIG_OVERRIDE_ENV) {
                PathBuf::from(override_env)
            } else {
                dirs::config_dir().context("could not determine config directory")?
            }
        }
    };

    let mut p = base_dir;
    p.push(SERVICE_NAME);
    tokio::fs::create_dir_all(&p)
        .await
        .context("create service config dir")?;
    Ok(p)
}


pub async fn session_file(override_dir: Option<&PathBuf>) -> Result<PathBuf> {
    Ok(get_cfg_dir(override_dir).await?.join("session.json"))
}

pub async fn write_session(session: &Session, override_dir: Option<&PathBuf>) -> Result<()> {
    let path = session_file(override_dir).await?;
    let data = serde_json::to_vec_pretty(session)?;

    tokio::fs::write(&path, &data)
        .await
        .context("write session file")?;

    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        if let Err(e) =
            tokio::fs::set_permissions(&path, std::fs::Permissions::from_mode(0o600)).await
        {
            warn!("Failed to set strict permissions on session file: {}", e);
        }
    }

    Ok(())
}

pub async fn read_session(override_dir: Option<&PathBuf>) -> Result<Session> {
    let path = session_file(override_dir).await?;
    let bytes = tokio::fs::read(&path).await.context("read session file")?;
    let session: Session = serde_json::from_slice(&bytes).context("parse session json")?;
    Ok(session)
}

pub async fn remove_session(override_dir: Option<&PathBuf>) -> Result<()> {
    let path = session_file(override_dir).await?;
    match tokio::fs::remove_file(path).await {
        Ok(_) => Ok(()),
        Err(e) if e.kind() == std::io::ErrorKind::NotFound => Ok(()),
        Err(e) => Err(e).context("remove session file"),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::{tempdir, TempDir};

    struct TestContext {
        // This holds the temporary directory, which is automatically deleted when TestContext goes out of scope.
        _dir: TempDir,
        // We store the path for easy access in tests.
        path: PathBuf,
    }

    impl TestContext {
        fn new() -> Self {
            let dir = tempdir().expect("create tempdir");
            let path = dir.path().to_path_buf();
            // No more environment variables! No more unsafe! No more Mutex!
            Self { _dir: dir, path }
        }
    }

#[tokio::test]
async fn test_is_near_expiry_true_when_within_buffer() {
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs();

    let exp = now + 30; // expires in 30 seconds
    let session = Session {
        login: "u".into(),
        jwt: "t".into(),
        jwt_exp: Some(exp),
    };

    assert!(session.is_near_expiry(60)); // buffer 60s â†’ should return true
}

#[tokio::test]
async fn test_is_near_expiry_false_when_outside_buffer() {
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs();

    let exp = now + 300; // expires in 5 minutes
    let session = Session {
        login: "u".into(),
        jwt: "t".into(),
        jwt_exp: Some(exp),
    };

    assert!(!session.is_near_expiry(60)); // buffer 60s â†’ should return false
}

#[tokio::test]
async fn test_is_near_expiry_none_expiry_means_false() {
    let session = Session {
        login: "u".into(),
        jwt: "t".into(),
        jwt_exp: None,
    };

    assert!(!session.is_near_expiry(60));
}

#[tokio::test]
async fn test_is_near_expiry_exact_boundary() {
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs();

    let exp = now + 60;
    let session = Session {
        login: "u".into(),
        jwt: "t".into(),
        jwt_exp: Some(exp),
    };

    // Expiration equals now + buffer â†’ treat as near expiry
    assert!(session.is_near_expiry(60));
}

    #[tokio::test]
    async fn test_write_read_cycle() {
        let ctx = TestContext::new();
        
        // Ensure no session exists by passing the temp directory path.
        remove_session(Some(&ctx.path)).await.unwrap();
        
        let session = Session {
            login: "test_user".into(),
            jwt: "fake_jwt".into(),
            jwt_exp: Some(42),
        };
        
        // Pass the temp directory path to the function being tested.
        write_session(&session, Some(&ctx.path)).await.unwrap();
        
        // Pass the temp directory path to read from the correct location.
        let loaded = read_session(Some(&ctx.path)).await.unwrap();
        
        assert_eq!(loaded.login, session.login);
        assert_eq!(loaded.jwt, session.jwt);
        assert_eq!(loaded.jwt_exp, session.jwt_exp);
    }

    #[tokio::test]
    async fn test_remove_missing_session_ok() {
        let ctx = TestContext::new();
        // Pass the temp directory path to ensure we operate in the isolated test environment.
        remove_session(Some(&ctx.path)).await.unwrap();
    }
}

########## ./cli/src/config.rs

use once_cell::sync::Lazy;

pub const SERVICE_NAME: &str = "steadystate";
pub const BACKEND_ENV: &str = "STEADYSTATE_BACKEND"; // e.g. https://api.steadystate.dev
pub const CONFIG_OVERRIDE_ENV: &str = "STEADYSTATE_CONFIG_DIR";
pub const DEFAULT_BACKEND: &str = "https://localhost:8080";
pub const JWT_REFRESH_BUFFER_SECS: u64 = 60;
pub const DEVICE_POLL_MAX_INTERVAL_SECS: u64 = 30;
pub const DEVICE_POLL_REQUEST_TIMEOUT_SECS: u64 = 10;
pub const HTTP_TIMEOUT_SECS: u64 = 30;
pub const RETRY_DELAY_MS: u64 = 500;
pub const MAX_NETWORK_RETRIES: u32 = 3;
pub const USER_AGENT: &str = "SteadyStateCLI/0.2";
pub const CLI_VERSION: &str = env!("CARGO_PKG_VERSION");

pub static BACKEND_URL: Lazy<String> =
    Lazy::new(|| std::env::var(BACKEND_ENV).unwrap_or_else(|_| DEFAULT_BACKEND.to_string()));

########## ./cli/src/merge.rs

//! # Merge Engine
//!
//! This module implements a CRDT-based 3-way merge using Yjs/Yrs.
//!
//! ## How It Works
//!
//! For each file, we perform:
//! 1. Build a base Yjs document from the last synced state
//! 2. Compute operations: base â†’ local and base â†’ canonical
//! 3. Apply both operation sets to a fresh document
//! 4. Extract the converged result
//!
//! ## CRDT Guarantees
//!
//! - **Convergence**: Both sides reach the same final state
//! - **Commutativity**: Order of applying updates doesn't matter
//! - **No conflicts**: All concurrent edits are preserved
//!
//! ## Important Behavior
//!
//! - **Concurrent inserts**: Order is deterministic but may seem arbitrary
//!   Example: Alice inserts "A" and Bob inserts "B" at same position
//!   Result: Always "AB" or always "BA" (consistent across runs)
//!
//! - **Binary files**: >1MB or containing NUL bytes treated as binary
//!   Binary conflicts require manual resolution
//!
//! - **Deletions**: Both sides must delete for file to be removed
//!
//! ## Position Tracking
//!
//! Uses UTF-16 code units to match Yjs/Yrs internal representation.
//! This ensures correct handling of:
//! - Emoji and emoticons
//! - Surrogate pairs (math symbols, rare CJK)
//! - Combining characters

use anyhow::{anyhow, Context, Result};
use std::collections::{HashMap, HashSet};
use std::path::{Path, PathBuf};
use std::process::Command;
use yrs::{Doc, Text, Transact, GetString, ReadTxn}; // Added ReadTxn
use yrs::updates::decoder::Decode; // Added Decode
use similar::{ChangeTag, TextDiff};
use walkdir::WalkDir;

pub type FilePath = String;
pub type FileContent = Vec<u8>;

#[derive(Debug, Clone)]
pub struct TreeSnapshot {
    pub files: HashMap<FilePath, FileContent>,
}

impl TreeSnapshot {
    pub fn new() -> Self {
        Self {
            files: HashMap::new(),
        }
    }

    pub fn get(&self, path: &str) -> Option<&FileContent> {
        self.files.get(path)
    }
}

/// Check if a file should be ignored by the sync engine.
fn is_ignored(path: &str) -> bool {
    let path = Path::new(path);
    let file_name = path.file_name().and_then(|s| s.to_str()).unwrap_or("");
    
    // Ignore list
    file_name == ".viminfo" ||
    file_name == ".DS_Store" ||
    file_name == "Thumbs.db" ||
    file_name.ends_with(".swp") ||
    file_name.ends_with('~') ||
    path.components().any(|c| c.as_os_str() == ".git" || c.as_os_str() == ".worktree")
}

/// Materialize a tree from a Git commit in a repository.
pub fn materialize_git_tree(repo_path: &Path, commit_hash: &str) -> Result<TreeSnapshot> {
    let mut snapshot = TreeSnapshot::new();

    // 1. List all files in the commit
    // git ls-tree -r --name-only <commit>
    let output = Command::new("git")
        .arg("-C")
        .arg(repo_path)
        .args(&["ls-tree", "-r", "--name-only", commit_hash])
        .output()
        .context("Failed to run git ls-tree")?;

    if !output.status.success() {
        return Err(anyhow!("git ls-tree failed"));
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    let files: Vec<&str> = stdout.lines().collect();

    // 2. Read content for each file
    // Optimization: Use git cat-file --batch for larger repos, but loop is fine for MVP
    for file_path in files {
        if file_path.trim().is_empty() {
            continue;
        }

        if is_ignored(file_path) {
            continue;
        }

        let content_output = Command::new("git")
            .arg("-C")
            .arg(repo_path)
            .args(&["show", &format!("{}:{}", commit_hash, file_path)])
            .output()
            .context(format!("Failed to read file {} from git", file_path))?;

        if content_output.status.success() {
            snapshot.files.insert(file_path.to_string(), content_output.stdout);
        }
    }

    Ok(snapshot)
}

/// Materialize a tree from the filesystem (worktree).
pub fn materialize_fs_tree(root_path: &Path) -> Result<TreeSnapshot> {
    let mut snapshot = TreeSnapshot::new();
    let mut file_count = 0;
    let start = std::time::Instant::now();
    
    use std::io::Write; // Ensure Write trait is available for flush

    for entry in WalkDir::new(root_path).into_iter().filter_map(|e| e.ok()) {
        // Skip symlinks explicitly
        if entry.file_type().is_symlink() {
            continue;
        }

        if !entry.file_type().is_file() {
            continue;
        }

        let path = entry.path();
        
        // Skip .git, .worktree, etc.
        let rel_path = path.strip_prefix(root_path)?;
        let rel_path_str = rel_path.to_string_lossy();

        if is_ignored(&rel_path_str) {
            continue;
        }

        let content = std::fs::read(path)?;
        snapshot.files.insert(rel_path_str.to_string(), content);

        file_count += 1;
        if file_count % 1000 == 0 {
            eprint!("\rScanning files: {}", file_count);
            std::io::stderr().flush().ok();
        }
    }

    if file_count > 0 {
        eprintln!("\rScanned {} files in {:?}", file_count, start.elapsed());
    }

    Ok(snapshot)
}

#[derive(Debug)]
enum Presence<'a> {
    Missing,
    Binary(&'a [u8]),
    Text(String),
}

/// Heuristically detect if a file should be treated as binary.
///
/// Detection rules:
/// 1. Files > 1MB are treated as binary (CRDT merge would be too expensive)
/// 2. Files containing NUL bytes in first 4KB are binary
/// 3. Files with >30% non-text bytes in first 4KB are binary
fn looks_binary(bytes: &[u8]) -> bool {
    if bytes.is_empty() {
        return false;
    }
    
    // Files larger than 1MB are treated as binary to avoid
    // expensive CRDT operations on large text files like logs
    if bytes.len() > 1024 * 1024 {
        return true;
    }

    let mut non_text = 0usize;
    for &b in bytes.iter().take(4096) {
        // NUL bytes indicate binary
        if b == 0 {
            return true;
        }
        // Count control characters (excluding whitespace)
        if (b < 0x09) || (b > 0x0D && b < 0x20) {
            non_text += 1;
        }
    }

    // If >30% of sampled bytes are non-text, treat as binary
    non_text as f64 / bytes.len().min(4096) as f64 > 0.30
}

fn classify(content: Option<&Vec<u8>>) -> Presence<'_> {
    match content {
        None => Presence::Missing,
        Some(bytes) => {
            if looks_binary(bytes) {
                return Presence::Binary(bytes);
            }
            // If it's not valid UTF-8, treat as binary
            match String::from_utf8(bytes.to_vec()) {
                Ok(s) => Presence::Text(s),
                Err(_) => Presence::Binary(bytes),
            }
        }
    }
}

/// Merge three trees using Yjs CRDT logic.
pub fn merge_trees(
    base: &TreeSnapshot,
    local: &TreeSnapshot,
    canonical: &TreeSnapshot,
) -> Result<TreeSnapshot> {
    let mut merged = TreeSnapshot::new();
    let debug_merge = std::env::var("STEADYSTATE_DEBUG_MERGE").is_ok();

    // Union of all file paths
    let mut all_files = HashSet::new();
    all_files.extend(base.files.keys());
    all_files.extend(local.files.keys());
    all_files.extend(canonical.files.keys());

    for path in all_files {
        let base_content = base.files.get(path);
        let local_content = local.files.get(path);
        let canon_content = canonical.files.get(path);

        let b = classify(base_content);
        let l = classify(local_content);
        let c = classify(canon_content);

        if debug_merge {
            tracing::info!(
                "Merge check for {}: Base={:?}, Local={:?}, Canon={:?}",
                path,
                base_content.map(|v| v.len()),
                local_content.map(|v| v.len()),
                canon_content.map(|v| v.len())
            );
        }

        // If both local and canonical explicitly delete â†’ delete in merged
        if matches!(l, Presence::Missing) && matches!(c, Presence::Missing) {
            continue; // removed from merged.files
        }

        match (b, l, c) {
            (Presence::Text(base_text), Presence::Text(local_text), Presence::Text(canon_text)) => {
                // All three are text -> CRDT merge
                let merged_text = merge_file_yjs(&base_text, &local_text, &canon_text)?;
                merged.files.insert(path.clone(), merged_text.into_bytes());
            }
            (_b_state, _l_state, _c_state) => {
                // Binary or mixed or deleted (but not both deleted)
                
                // Helper to get bytes from state, defaulting to empty if missing/text (though text should be handled above if all text)
                // Actually, if we are here, at least one is NOT text (or missing).
                // But we need bytes for binary merge.
                let _get_bytes = |state: Presence, _original: Option<&Vec<u8>>| -> Vec<u8> {
                    match state {
                        Presence::Binary(b) => b.to_vec(),
                        Presence::Text(s) => s.into_bytes(),
                        Presence::Missing => vec![],
                    }
                };

                // We need the original bytes for comparison to avoid re-encoding text if possible, 
                // but for simplicity let's use the Presence data or original content.
                let base_bytes = base_content.cloned().unwrap_or_default();
                let local_bytes = local_content.cloned().unwrap_or_default();
                let canon_bytes = canon_content.cloned().unwrap_or_default();

                // Check if both sides modified
                let local_changed = local_bytes != base_bytes;
                let canon_changed = canon_bytes != base_bytes;

                if local_changed && canon_changed {
                    // Both modified the binary file
                    if local_bytes == canon_bytes {
                        // Same change â†’ OK
                        if canon_content.is_some() {
                            merged.files.insert(path.clone(), canon_bytes);
                        }
                    } else {
                        // Different changes â†’ CONFLICT
                        return Err(anyhow!(
                            "Binary file conflict in '{}': both local and canonical modified. \
                             Local size: {} bytes, Canonical size: {} bytes. \
                             Manual resolution required.",
                            path,
                            local_bytes.len(),
                            canon_bytes.len()
                        ));
                    }
                } else if local_changed {
                    // Only local changed
                    if local_content.is_some() {
                        merged.files.insert(path.clone(), local_bytes);
                    }
                } else if canon_changed {
                    // Only canonical changed
                    if canon_content.is_some() {
                        merged.files.insert(path.clone(), canon_bytes);
                    }
                } else {
                    // Neither changed (identical or both same as base)
                    if canon_content.is_some() {
                        merged.files.insert(path.clone(), canon_bytes);
                    } else if local_content.is_some() {
                        merged.files.insert(path.clone(), local_bytes);
                    }
                }
            }
        }
    }

    Ok(merged)
}

fn build_base_update(base: &str) -> Result<Vec<u8>> {
    let doc = Doc::new();
    let txt = doc.get_or_insert_text("content");
    {
        let mut txn = doc.transact_mut();
        txt.insert(&mut txn, 0, base);
    }
    Ok(doc.transact().encode_state_as_update_v1(&yrs::StateVector::default()))
}

fn build_side_update(base_update: &[u8], base: &str, side: &str) -> Result<Vec<u8>> {
    let doc = Doc::new();
    {
        let mut txn = doc.transact_mut();
        let upd = yrs::Update::decode_v1(base_update)?;
        txn.apply_update(upd);
    }
    let txt = doc.get_or_insert_text("content");

    // Line-level diff
    let diff = TextDiff::from_lines(base, side);
    {
        let mut txn = doc.transact_mut();
        let mut pos = 0;
        
        for change in diff.iter_all_changes() {
            match change.tag() {
                ChangeTag::Equal => {
                    pos += change.value().encode_utf16().count() as u32;
                }
                ChangeTag::Delete => {
                    let len = change.value().encode_utf16().count() as u32;
                    txt.remove_range(&mut txn, pos, len);
                }
                ChangeTag::Insert => {
                    txt.insert(&mut txn, pos, change.value());
                    pos += change.value().encode_utf16().count() as u32;
                }
            }
        }
    }

    Ok(doc.transact().encode_state_as_update_v1(&yrs::StateVector::default()))
}

fn merge_updates(local_update: &[u8], canonical_update: &[u8]) -> Result<String> {
    let doc = Doc::new();
    let txt = doc.get_or_insert_text("content");
    {
        let mut txn = doc.transact_mut();
        txn.apply_update(yrs::Update::decode_v1(local_update)?);
        txn.apply_update(yrs::Update::decode_v1(canonical_update)?);
    }
    Ok(txt.get_string(&doc.transact()))
}

/// Perform a 3-way merge of text using Yrs.
pub fn merge_file_yjs(base: &str, local: &str, canonical: &str) -> Result<String> {
    let base_update = build_base_update(base)?;
    let local_u = build_side_update(&base_update, base, local)?;
    let canon_u = build_side_update(&base_update, base, canonical)?;
    
    if std::env::var("STEADYSTATE_DEBUG_MERGE").is_ok() {
        tracing::info!(
            "merge_file_yjs: local_update={} bytes, canon_update={} bytes",
            local_u.len(),
            canon_u.len()
        );
    }

    merge_updates(&local_u, &canon_u)
}

#[cfg(test)]
mod tests {
    use super::*;

    // Definitions for test variables:
    // - base: The common ancestor content (last known synced state).
    // - alice: Represents the local user's changes (passed as 'local').
    // - bob: Represents the other user's changes (passed as 'canonical').

    #[test]
    fn test_merge_no_conflict() {
        let base = "Hello World";
        let alice = "Hello World!"; // Alice added "!"
        let bob = "Hello World";    // Bob made no changes
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        // Merged: "Hello World!"
        assert_eq!(merged, "Hello World!");
    }

    #[test]
    fn test_merge_non_conflicting() {
        let base = "Hello World";
        let alice = "Hello World!"; // Alice added "!"
        let bob = "Hello World?";   // Bob added "?"
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        // Merged: "Hello World!?" or "Hello World?!" (order depends on CRDT)
        // Order depends on Yjs internals but should contain both
        assert!(merged.contains("!"));
        assert!(merged.contains("?"));
    }

    #[test]
    fn test_merge_deletion() {
        let base = "Hello World";
        let alice = "Hello";        // Alice deleted " World"
        let bob = "Hello World";    // Bob unchanged
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        // Merged: "Hello"
        assert_eq!(merged, "Hello");
    }

    #[test]
    fn test_merge_concurrent_insert() {
        let base = "";
        let alice = "A"; // Alice inserted "A"
        let bob = "B";   // Bob inserted "B"
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        // Merged: "AB" or "BA"
        assert!(merged == "AB" || merged == "BA");
    }
    
    #[test]
    fn test_merge_mixed_edit() {
        let base = "Line1\nLine2\nLine3";
        let alice = "Line1\nLine2 Modified\nLine3";   // Alice modified Line 2
        let bob = "Line1\nLine2\nLine3 Modified";     // Bob modified Line 3
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        // Merged:
        // Line1
        // Line2 Modified
        // Line3 Modified
        assert!(merged.contains("Line2 Modified"));
        assert!(merged.contains("Line3 Modified"));
    }

    #[test]
    fn test_classify_binary() {
        let binary_data = vec![0, 1, 2, 3];
        let presence = classify(Some(&binary_data));
        assert!(matches!(presence, Presence::Binary(_)));
    }

    #[test]
    fn test_classify_text() {
        let text_data = "Hello World".as_bytes().to_vec();
        let presence = classify(Some(&text_data));
        assert!(matches!(presence, Presence::Text(_)));
    }

    #[test]
    fn test_merge_with_emoji() {
        let base = "Hello World";
        let alice = "Hello ðŸ‘‹ World";
        let bob = "Hello World!";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        assert!(merged.contains("ðŸ‘‹"));
        assert!(merged.contains("!"));
    }

    #[test]
    fn test_merge_with_combining_characters() {
        let base = "cafe";
        let alice = "cafÃ©";  // e + combining acute
        let bob = "cafe!";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        assert!(merged.contains("cafÃ©"));
        assert!(merged.contains("!"));
    }

    #[test]
    fn test_merge_with_surrogate_pairs() {
        let base = "Math: H";
        let alice = "Math: ð•³";  // Mathematical bold H (U+1D573)
        let bob = "Math: H!";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        assert!(merged.contains("ð•³"));
        assert!(merged.contains("!"));
    }

    #[test]
    fn test_merge_with_cjk() {
        let base = "Hello";
        let alice = "Hello ä¸–ç•Œ";
        let bob = "Hello!";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        assert!(merged.contains("ä¸–ç•Œ"));
        assert!(merged.contains("!"));
    }

    #[test]
    fn test_binary_conflict_detection() {
        let mut base = TreeSnapshot::new();
        let mut local = TreeSnapshot::new();
        let mut canonical = TreeSnapshot::new();
        
        // Binary file (contains NUL byte)
        let base_binary = vec![0xFF, 0xD8, 0xFF, 0xE0]; // JPEG header
        let local_binary = vec![0xFF, 0xD8, 0xFF, 0xE1]; // Modified
        let canon_binary = vec![0xFF, 0xD8, 0xFF, 0xE2]; // Different modification
        
        base.files.insert("image.jpg".to_string(), base_binary);
        local.files.insert("image.jpg".to_string(), local_binary);
        canonical.files.insert("image.jpg".to_string(), canon_binary);
        
        let result = merge_trees(&base, &local, &canonical);
        
        // Should error on conflict
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Binary file conflict"));
    }

    #[test]
    fn test_binary_same_change_ok() {
        let mut base = TreeSnapshot::new();
        let mut local = TreeSnapshot::new();
        let mut canonical = TreeSnapshot::new();
        
        let base_binary = vec![0xFF, 0xD8];
        let changed_binary = vec![0xFF, 0xD9]; // Same change on both sides
        
        base.files.insert("image.jpg".to_string(), base_binary);
        local.files.insert("image.jpg".to_string(), changed_binary.clone());
        canonical.files.insert("image.jpg".to_string(), changed_binary.clone());
        
        let result = merge_trees(&base, &local, &canonical);
        assert!(result.is_ok());
        
        let merged = result.unwrap();
        assert_eq!(merged.files.get("image.jpg"), Some(&changed_binary));
    }

    #[test]
    fn test_empty_file_merge() {
        let base = "";
        let alice = "New content";
        let bob = "";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        assert_eq!(merged, "New content");
    }

    #[test]
    fn test_both_add_same_line() {
        let base = "Line 1\nLine 2";
        let alice = "Line 1\nNew Line\nLine 2";
        let bob = "Line 1\nNew Line\nLine 2";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        // Concurrent inserts of same content are NOT deduplicated by Yjs
        // So we expect 2 occurrences
        assert_eq!(merged.matches("New Line").count(), 2);
    }

    #[test]
    fn test_whitespace_only_changes() {
        let base = "foo bar";
        let alice = "foo  bar"; // Two spaces
        let bob = "foo bar";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        assert!(merged.contains("  ")); // Alice's spaces preserved
    }

    #[test]
    fn test_line_ending_normalization() {
        let base = "Line 1\nLine 2";
        let alice = "Line 1\r\nLine 2"; // Windows CRLF
        let bob = "Line 1\nLine 2\nLine 3";
        let merged = merge_file_yjs(base, alice, bob).unwrap();
        // Should handle mixed line endings gracefully
        assert!(merged.contains("Line 3"));
    }

    #[cfg(unix)]
    #[test]
    fn test_symlinks_are_ignored() {
        use std::os::unix::fs::symlink;
        
        let temp = tempfile::tempdir().unwrap();
        let temp_path = temp.path();
        
        // Create a regular file
        std::fs::write(temp_path.join("real.txt"), "content").unwrap();
        
        // Create a symlink
        symlink(
            temp_path.join("real.txt"),
            temp_path.join("link.txt")
        ).unwrap();
        
        let snapshot = materialize_fs_tree(temp_path).unwrap();
        
        // Should only have the real file, not the symlink
        assert!(snapshot.files.contains_key("real.txt"));
        assert!(!snapshot.files.contains_key("link.txt"));
    }

    #[test]
    fn test_is_ignored() {
        assert!(is_ignored(".viminfo"));
        assert!(is_ignored("path/to/.viminfo"));
        assert!(is_ignored(".DS_Store"));
        assert!(is_ignored("foo.swp"));
        assert!(is_ignored("foo.txt~"));
        assert!(is_ignored(".git/config"));
        assert!(is_ignored(".worktree/steadystate.json"));
        
        assert!(!is_ignored("foo.txt"));
        assert!(!is_ignored("src/main.rs"));
        assert!(!is_ignored(".gitignore"));
    }
}

########## ./cli/src/notify.rs

use anyhow::{Context, Result};
use std::path::Path;
use std::time::Duration;
use std::thread;

pub fn watch() -> Result<()> {
    // Clear screen
    print!("\x1B[2J\x1B[1;1H");
    
    let repo_root = std::env::var("REPO_ROOT").unwrap_or_else(|_| "..".to_string());
    let sync_log_path = Path::new(&repo_root).join("sync-log");
    let active_users_path = Path::new(&repo_root).join("active-users");
    
    // Get session info (from env or path)
    let session_id = std::env::var("SESSION_ID").unwrap_or_else(|_| "unknown".to_string());
    // Try to get repo name from git remote or just dir name
    let repo_name = std::env::current_dir()
        .map(|p| p.file_name().unwrap_or_default().to_string_lossy().to_string())
        .unwrap_or_else(|_| "unknown".to_string());

    // Wait for file to exist
    while !sync_log_path.exists() {
        println!("Waiting for sync-log at {}...", sync_log_path.display());
        thread::sleep(Duration::from_secs(1));
    }

    let _file = std::fs::File::open(&sync_log_path).context("Failed to open sync-log")?;
    
    // Seek to end initially to only show new events? 
    // Actually for dashboard we might want to show last few events.
    // Let's read last 10 lines.
    // For simplicity, let's just read from start or seek to end - 1000 bytes?
    // Let's just tail from now for the "Activity" stream, but maybe print last few lines first.
    
    // Actually, let's implement a loop that redraws every second.
    // It's easier than complex async IO for now.
    
    loop {
        // 1. Read connected users
        let mut connected_users = Vec::new();
        if active_users_path.exists() {
            if let Ok(content) = std::fs::read_to_string(&active_users_path) {
                for line in content.lines() {
                    if !line.trim().is_empty() {
                        connected_users.push(line.trim().to_string());
                    }
                }
            }
        }
        // Deduplicate
        connected_users.sort();
        connected_users.dedup();

        // 2. Read last N lines of sync log
        // Re-open to get fresh content or just seek?
        // If we want to show a scrolling log, we should keep the file open.
        // But we also want to redraw the header.
        
        // Let's clear screen and redraw header
        print!("\x1B[2J\x1B[1;1H");
        
        // Try to get repo name from env first (set by wrapper.sh)
        let display_repo_name = std::env::var("REPO_NAME").unwrap_or(repo_name.clone());

        println!("SteadyState Session: {}", session_id);
        if let Ok(link) = std::env::var("MAGIC_LINK") {
             println!("Magic Link: {}", link);
        }
        println!("Repo: {}", display_repo_name);
        println!("-------------------------------------------------------------------------------------------------");
        println!();
        println!("Connected users:");
        if connected_users.is_empty() {
            println!("  (none)");
        } else {
            for user in &connected_users {
                println!("  â€¢ {}", user);
            }
        }
        println!();
        println!("Activity (tailing sync-log):");
        
        // Read last 10 lines
        // This is inefficient but fine for a prototype
        let content = std::fs::read_to_string(&sync_log_path).unwrap_or_default();
        let lines: Vec<&str> = content.lines().collect();
        let start = if lines.len() > 10 { lines.len() - 10 } else { 0 };
        
        for line in &lines[start..] {
            // Parse timestamp
            let parts: Vec<&str> = line.split_whitespace().collect();
            if parts.len() >= 2 {
                // Format: timestamp user
                // We can format timestamp if we want, but raw is okay for now
                println!("  [{}] {} synced", parts[0], parts[1]);
            } else {
                println!("  {}", line);
            }
        }
        
        println!();
        println!("Watching for changes... (Ctrl+C to exit)");
        
        thread::sleep(Duration::from_secs(2));
    }
}

########## ./cli/src/auth.rs

use std::path::PathBuf;
use std::time::Duration;

use anyhow::{anyhow, Context, Result};
use indicatif::{ProgressBar, ProgressStyle};
use jwt_simple::prelude::*;
use keyring::Entry;
use reqwest::Client;
use serde::{de::DeserializeOwned, Deserialize, Serialize};
use tokio::{select, signal, time};
use tracing::{info, warn};

use crate::config::{
    BACKEND_URL, DEVICE_POLL_MAX_INTERVAL_SECS, DEVICE_POLL_REQUEST_TIMEOUT_SECS,
    JWT_REFRESH_BUFFER_SECS, MAX_NETWORK_RETRIES, RETRY_DELAY_MS, SERVICE_NAME,
};
use crate::session::{read_session, remove_session, write_session, Session};

#[derive(Deserialize)]
struct DeviceResponse {
    device_code: String,
    user_code: String,
    verification_uri: String,
    expires_in: u64,
    interval: Option<u64>,
}

#[derive(Deserialize)]
struct PollResponse {
    status: Option<String>,
    jwt: Option<String>,
    refresh_token: Option<String>,
    login: Option<String>,
    error: Option<String>,
}

#[derive(Deserialize, Serialize)]
pub struct UpResponse {
    pub id: String,
    pub state: String,
    pub endpoint: Option<String>,
    pub magic_link: Option<String>,
    pub message: Option<String>,
}

/// Initiates OAuth device flow authentication.
pub async fn device_login(client: &Client, provider: &str) -> Result<()> {
    // Backend: POST /auth/device?provider={provider}
    let url = format!("{}/auth/device?provider={}", &*BACKEND_URL, provider);
    let resp = send_with_retries(|| client.post(&url)).await?;

    if !resp.status().is_success() {
        let status = resp.status();
        let body = resp.text().await.unwrap_or_default();
        anyhow::bail!("device code request failed ({}): {}", status, body);
    }

    let dr: DeviceResponse = resp.json().await.context("parse device response")?;

    println!("Open the verification URL and enter the code:");
    println!("\n  {}\n", dr.verification_uri);
    println!("Code: {}\n", dr.user_code);

    if let Err(e) = open::that(&dr.verification_uri) {
        warn!("open browser failed: {}", e);
    }

    let poll_url = format!("{}/auth/poll", &*BACKEND_URL);
    let interval = dr.interval.unwrap_or(5).max(1);
    let max_interval_secs = DEVICE_POLL_MAX_INTERVAL_SECS.max(interval);
    let device_code = dr.device_code.clone();
    let expires_in = dr.expires_in;

    println!("Waiting for authorization (press Ctrl+C to cancel)...");

    let spinner = ProgressBar::new_spinner();
    spinner.set_style(
        ProgressStyle::with_template("{spinner} {msg}")
            .expect("Progress bar template is valid")
            .tick_strings(&["â ‹", "â ™", "â š", "â ž", "â –", "â ¦", "â ´", "â ²", "â ³", "â “"]),
    );
    spinner.enable_steady_tick(Duration::from_millis(120));
    let start = std::time::Instant::now();

    // --- Main polling loop -------------------------------------------------
    let poll_loop = async {
        let mut current_interval_secs = interval;

        loop {
            spinner.set_message(format!(
                "Authorizing... {}s elapsed",
                start.elapsed().as_secs()
            ));

            select! {
                _ = signal::ctrl_c() => {
                    spinner.finish_and_clear();
                    println!("\nCancelled by user");
                    return Ok(());
                }

                _ = time::sleep(Duration::from_secs(current_interval_secs)) => {
                    // Backend now expects POST with JSON { "device_code": ... }
                    let poll = send_with_retries(|| {
                        client
                            .post(&poll_url)
                            .json(&serde_json::json!({ "device_code": device_code.clone() }))
                            .timeout(Duration::from_secs(DEVICE_POLL_REQUEST_TIMEOUT_SECS))
                    })
                    .await
                    .context("poll request failed")?;

                    if !poll.status().is_success() {
                        let status = poll.status();
                        let body = poll.text().await.unwrap_or_default();
                        anyhow::bail!("poll request failed ({}): {}", status, body);
                    }

                    let out: PollResponse = poll.json().await.context("parse poll response")?;

                    match out.status.as_deref() {
                        Some("complete") => {
                            // Success: backend returns jwt, refresh_token, login
                            spinner.finish_and_clear();
                            let jwt = out.jwt.context("server did not return jwt")?;
                            let refresh = out.refresh_token.context("no refresh token returned")?;
                            let login = out.login.context("no login returned")?;

                            store_refresh_token(&login, &refresh, None).await?;

                            let session = Session::new(login.clone(), jwt.clone());
                            write_session(&session, None).await?;
                            println!("âœ… Logged in as {}", login);
                            return Ok(());
                        }
                        Some("pending") => {
                            // Still waiting. Maybe slow_down hint.
                            if let Some(err) = out.error.as_deref() {
                                match err {
                                    "slow_down" => {
                                        current_interval_secs =
                                            (current_interval_secs + 5).clamp(interval, max_interval_secs);
                                    }
                                    // You can extend with more non-fatal hints later.
                                    other => {
                                        spinner.finish_and_clear();
                                        anyhow::bail!("Authorization error: {}", other);
                                    }
                                }
                            }
                            // Otherwise: keep polling with current interval.
                        }
                        None => {
                            if let Some(err) = out.error {
                                spinner.finish_and_clear();
                                anyhow::bail!("Authorization error: {}", err);
                            } else {
                                spinner.finish_and_clear();
                                anyhow::bail!("Unexpected poll response: missing status");
                            }
                        }
                        Some(other) => {
                            spinner.finish_and_clear();
                            anyhow::bail!("Unexpected poll status: {}", other);
                        }
                    }
                }
            }
        }
    };

    match time::timeout(Duration::from_secs(expires_in), poll_loop).await {
        Ok(res) => res,
        Err(_) => {
            spinner.finish_and_clear();
            anyhow::bail!("Device code expired.")
        }
    }
}

// =======================================================================
// REFACTORED AUTHENTICATION LOGIC
// =======================================================================

#[derive(Debug, Deserialize)]
pub struct RefreshResponse {
    pub jwt: String,
}

/// Refreshes JWT using stored refresh token.
pub async fn perform_refresh(client: &Client, login_override: Option<String>, override_dir: Option<&PathBuf>) -> Result<RefreshResponse> {
    let username = match login_override {
        Some(login) => login,
        None => {
            read_session(override_dir)
                .await
                .context("No active session found. Run 'steadystate login' first.")?
                .login
        }
    };

    let refresh = get_refresh_token(&username, override_dir)
        .await?
        .ok_or_else(|| anyhow!("No refresh token found. Run `steadystate login` again."))?;

    let url = format!("{}/auth/refresh", &*BACKEND_URL);
    let resp = send_with_retries(|| {
        client
            .post(&url)
            .json(&serde_json::json!({ "refresh_token": refresh }))
    })
    .await
    .context("auth/refresh request failed")?;

    if resp.status().as_u16() == 401 {
        let _ = delete_refresh_token(&username, override_dir).await;
        let _ = remove_session(override_dir).await;
        anyhow::bail!(
            "Refresh token has expired or been revoked. Run 'steadystate login' to authenticate again."
        );
    }

    if !resp.status().is_success() {
        anyhow::bail!("Refresh failed with status {}", resp.status());
    }

    let refresh_resp: RefreshResponse = resp.json().await.context("parse refresh response")?;

    let new_session = Session::new(username, refresh_resp.jwt.clone());
    write_session(&new_session, override_dir).await?;

    Ok(refresh_resp)
}

/// Perform an authenticated request with correct semantics:
///
/// 1. If JWT is expired locally â†’ proactively refresh.
/// 2. Call API.
/// 3. If API returns 401 â†’ treat as fatal, advise user to log in again.
pub async fn request_with_auth<T, F>(
    client: &Client,
    builder_fn: F,
    _override_dir: Option<&PathBuf>, // Kept for API compatibility if needed elsewhere
) -> Result<T>
where
    T: DeserializeOwned,
    F: Fn(&Client, &str) -> reqwest::RequestBuilder,
{
    // Step 1: Load session
    let session = read_session(None)
        .await
        .context("No active session. Run `steadystate login` first.")?;
    let mut jwt = session.jwt.clone();

    // Step 2: If JWT is near expiry â†’ refresh proactively
    if session.is_near_expiry(JWT_REFRESH_BUFFER_SECS) {
        info!("JWT has expired or is near expiry, refreshing proactively");
        let refresh_resp = perform_refresh(client, Some(session.login), _override_dir)
            .await
            .context("Session expired and refresh failed")?;
        jwt = refresh_resp.jwt;
    }

    // Step 3: Perform the authenticated request
    let resp = send_with_retries(|| builder_fn(client, &jwt))
        .await
        .context("API request failed")?;

    // Step 4: If backend returns 401 â†’ fail clearly, do not retry
    // Step 4: If backend returns 401 â†’ fail clearly, do not retry
    if resp.status().as_u16() == 401 {
        let body = resp.text().await.unwrap_or_default();
        anyhow::bail!(
            "Your session has expired or been revoked. Run `steadystate login` again.\nServer says: {}", body
        );
    }

    if !resp.status().is_success() {
        let status = resp.status();
        let body = resp.text().await.unwrap_or_default();
        anyhow::bail!("API request failed with status {}: {}", status, body);
    }

    // Step 5: Parse and return
    Ok(resp
        .json::<T>()
        .await
        .context("Failed to parse server response")?)
}

// =======================================================================
// UNCHANGED HELPERS
// =======================================================================

/// Extracts expiry timestamp from JWT (no signature verification).
pub fn extract_exp_from_jwt(jwt: &str) -> Option<u64> {
    let parts: Vec<&str> = jwt.split('.').collect();
    if parts.len() != 3 {
        warn!("Invalid JWT format");
        return None;
    }
    let payload_bytes = match Base64UrlSafeNoPadding::decode_to_vec(parts[1], None) {
        Ok(bytes) => bytes,
        Err(e) => {
            warn!("Failed to decode JWT payload: {:?}", e);
            return None;
        }
    };
    match serde_json::from_slice::<serde_json::Value>(&payload_bytes) {
        Ok(payload) => payload.get("exp").and_then(|v| v.as_u64()),
        Err(e) => {
            warn!("Failed to parse JWT payload: {}", e);
            None
        }
    }
}

/// Helper to check for mock keyring environment variable
fn get_mock_keyring_path(username: &str) -> Option<PathBuf> {
    std::env::var("STEADYSTATE_KEYRING_DIR").ok().map(|d| PathBuf::from(d).join(format!("{}.keyring", username)))
}

const KEYRING_TIMEOUT_MS: u64 = 2000;

/// Helper to get the fallback file path for the refresh token
async fn get_fallback_token_path(override_dir: Option<&PathBuf>) -> Result<PathBuf> {
    let base_dir = crate::session::get_cfg_dir(override_dir).await?;
    Ok(base_dir.join("credentials"))
}

/// Stores refresh token in the OS keychain (with timeout) or fallback file.
pub async fn store_refresh_token(username: &str, token: &str, override_dir: Option<&PathBuf>) -> Result<()> {
    if token.is_empty() {
        return Err(anyhow!("refresh token cannot be empty"));
    }
    
    // Check for mock keyring first
    if let Some(path) = get_mock_keyring_path(username) {
        return std::fs::write(path, token).context("failed to write mock keyring");
    }

    let use_keyring = std::env::var("STEADYSTATE_NO_KEYRING").is_err();

    if use_keyring {
        let username = username.to_string();
        let token_val = token.to_string();

        // Try keyring with timeout
        let keyring_result = time::timeout(Duration::from_millis(KEYRING_TIMEOUT_MS), tokio::task::spawn_blocking(move || {
            let entry = Entry::new(SERVICE_NAME, &username).context("keyring entry creation failed")?;
            entry.set_password(&token_val).context("keyring set_password failed")
        })).await;

        match keyring_result {
            Ok(Ok(Ok(_))) => return Ok(()),
            Ok(Ok(Err(e))) => warn!("Keyring error: {}. Falling back to file storage.", e),
            Ok(Err(e)) => warn!("Keyring task join error: {}. Falling back to file storage.", e),
            Err(_) => warn!("Keyring operation timed out. Falling back to file storage."),
        }
    } else {
        info!("STEADYSTATE_NO_KEYRING set, skipping system keyring.");
    }

    // Fallback: Write to file
    let path = get_fallback_token_path(override_dir).await?;
    tokio::fs::write(&path, token).await.context("failed to write fallback token file")?;
    
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        if let Err(e) = tokio::fs::set_permissions(&path, std::fs::Permissions::from_mode(0o600)).await {
            warn!("Failed to set strict permissions on token file: {}", e);
        }
    }

    Ok(())
}

/// Retrieves refresh token from keychain (with timeout) or fallback file.
pub async fn get_refresh_token(username: &str, override_dir: Option<&PathBuf>) -> Result<Option<String>> {
    // Check for mock keyring first
    if let Some(path) = get_mock_keyring_path(username) {
        if path.exists() {
            return std::fs::read_to_string(path).map(Some).context("failed to read mock keyring");
        } else {
            return Ok(None);
        }
    }

    let use_keyring = std::env::var("STEADYSTATE_NO_KEYRING").is_err();

    if use_keyring {
        let username = username.to_string();

        // Try keyring with timeout
        let keyring_result = time::timeout(Duration::from_millis(KEYRING_TIMEOUT_MS), tokio::task::spawn_blocking(move || {
            let entry = Entry::new(SERVICE_NAME, &username).context("keyring entry creation failed")?;
            match entry.get_password() {
                Ok(tok) => Ok(Some(tok)),
                Err(keyring::Error::NoEntry) => Ok(None),
                Err(e) => Err(e).context("keyring get_password failed"),
            }
        })).await;

        match keyring_result {
            Ok(Ok(Ok(Some(token)))) => return Ok(Some(token)),
            Ok(Ok(Ok(None))) => {
                // Keyring worked but no token. Check fallback file just in case? 
                // Usually if we use keyring we stick to it, but if user switched envs...
                // Let's check fallback file if keyring is empty.
            }, 
            Ok(Ok(Err(e))) => warn!("Keyring error: {}. Checking fallback file.", e),
            Ok(Err(e)) => warn!("Keyring task join error: {}. Checking fallback file.", e),
            Err(_) => warn!("Keyring operation timed out. Checking fallback file."),
        }
    }

    // Fallback: Read from file
    let path = get_fallback_token_path(override_dir).await?;
    if path.exists() {
        let token = tokio::fs::read_to_string(path).await.context("failed to read fallback token file")?;
        if !token.trim().is_empty() {
             return Ok(Some(token));
        }
    }

    Ok(None)
}

/// Deletes refresh token from keychain (with timeout) and fallback file.
pub async fn delete_refresh_token(username: &str, override_dir: Option<&PathBuf>) -> Result<()> {
    // Check for mock keyring first
    if let Some(path) = get_mock_keyring_path(username) {
        if path.exists() {
            std::fs::remove_file(path).context("failed to delete mock keyring")?;
        }
        return Ok(());
    }

    let use_keyring = std::env::var("STEADYSTATE_NO_KEYRING").is_err();

    if use_keyring {
        let username = username.to_string();

        // Try keyring with timeout
        let _ = time::timeout(Duration::from_millis(KEYRING_TIMEOUT_MS), tokio::task::spawn_blocking(move || {
            if let Ok(entry) = Entry::new(SERVICE_NAME, &username) {
                let _ = entry.delete_credential();
            }
        })).await;
    }

    // Always try to delete fallback file too
    let path = get_fallback_token_path(override_dir).await?;
    if path.exists() {
        tokio::fs::remove_file(path).await.context("failed to delete fallback token file")?;
    }

    Ok(())
}

pub(crate) async fn send_with_retries<F>(mut make_request: F) -> Result<reqwest::Response>
where
    F: FnMut() -> reqwest::RequestBuilder,
{
    let mut delay = Duration::from_millis(RETRY_DELAY_MS);
    for attempt in 1..=MAX_NETWORK_RETRIES {
        let builder = make_request();
        match builder.send().await {
            Ok(resp) => return Ok(resp),
            Err(err) if attempt < MAX_NETWORK_RETRIES && (err.is_timeout() || err.is_connect()) => {
                warn!(
                    "network request failed (attempt {} of {}): {}",
                    attempt, MAX_NETWORK_RETRIES, err
                );
                time::sleep(delay).await;
                delay = delay.saturating_mul(2);
            }
            Err(err) => return Err(err.into()),
        }
    }
    unreachable!("retry loop should return before exhausting attempts");
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::{tempdir, TempDir};

    struct TestContext {
        _dir: TempDir,
        path: PathBuf,
    }

    impl TestContext {
        fn new() -> Self {
            let dir = tempdir().expect("create tempdir");
            let path = dir.path().to_path_buf();
            Self { _dir: dir, path }
        }
    }

    #[tokio::test]
    async fn test_perform_refresh_without_session() {
        let _ctx = TestContext::new();
        let client = Client::builder().pool_max_idle_per_host(0).build().unwrap();

        let result = perform_refresh(&client, None, Some(&_ctx.path)).await;
        assert!(result.is_err());
        let err_msg = format!("{:#}", result.unwrap_err());
        assert!(err_msg.contains("No active session found. Run 'steadystate login' first."));
    }

    #[tokio::test]
    async fn test_perform_refresh_without_refresh_token() {
        let ctx = TestContext::new();
        let client = Client::builder().pool_max_idle_per_host(0).build().unwrap();

        let session = Session::new("test_user".to_string(), "fake_jwt".to_string());
        write_session(&session, Some(&ctx.path))
            .await
            .expect("write session");

        let result = perform_refresh(&client, Some("test_user".to_string()), Some(&ctx.path)).await;
        assert!(result.is_err());
        let err_msg = format!("{:#}", result.unwrap_err());
        assert!(err_msg.contains("No refresh token found"));

        let _ = crate::session::remove_session(Some(&ctx.path)).await;
    }
    }

########## ./cli/src/main.rs

// cli/src/main.rs

//! SteadyState CLI - Manage reproducible development environments
//!
//! This CLI provides commands for authentication and session management.
//! See README.md for usage examples.

mod auth;
mod config;
mod session;
mod sync;
mod notify;
mod merge;

use anyhow::{Context, Result};
use clap::{CommandFactory, Parser, Subcommand};
use reqwest::{Client, Url};
use serde::Serialize;
use tokio::time::Duration;
use tracing::{error, info, warn};

use auth::{
    UpResponse, delete_refresh_token, device_login, get_refresh_token, perform_refresh,
    request_with_auth,
};
use config::{BACKEND_URL, CLI_VERSION, HTTP_TIMEOUT_SECS, USER_AGENT};
use session::{read_session, remove_session};

#[derive(Parser)]
#[command(
    name = "steadystate",
    about = "SteadyState CLI â€” Exact reproducible dev envs",
    disable_version_flag = true,
    version = CLI_VERSION
)]
struct Cli {
    #[arg(long = "version", short = 'v')]
    version: bool,

    #[command(subcommand)]
    cmd: Option<Commands>,
}

#[derive(Subcommand)]
enum Commands {
    /// Start interactive login (device flow)
    Login {
        /// Authentication provider (e.g., github, gitlab, orchid, fake)
        #[arg(long, default_value = "github")]
        provider: String,
    },
    /// Show current logged-in user (if any)
    Whoami {
        /// Output in JSON format
        #[arg(long)]
        json: bool,
    },
    /// Refresh JWT using refresh token stored in keychain
    Refresh,
    /// Logout: revoke refresh token and clear local session
    Logout,
    /// Create a remote development session for the provided repository URL
    Up {
        /// Repository URL used for the remote session
        repo: String,
        /// Output in JSON format
        #[arg(long)]
        json: bool,
        /// Allow specific GitHub users to connect. Defaults to all repository collaborators. Use "none" to restrict to host only.
        #[arg(long)]
        allow: Vec<String>,
        /// Make the session public (anyone with the link can connect)
        #[arg(long)]
        public: bool,
        /// Environment to load (e.g. "noenv")
        #[arg(long)]
        env: Option<String>,
        /// Session mode: "pair" or "collab"
        #[arg(long)]
        mode: Option<String>,
    },
    /// Join a remote session using a magic link or SSH URL
    Join {
        /// Magic link (steadystate://...) or SSH URL
        url: String,
    },
    /// Synchronize changes with other users (Collaboration Mode)
    Sync,
    /// Watch for sync events (Collaboration Mode)
    Watch,
}

#[derive(Serialize)]
struct WhoamiOutput {
    logged_in: bool,
    login: Option<String>,
    jwt_expires_at: Option<u64>,
}

async fn whoami(json_output: bool) -> Result<()> {
    match read_session(None).await {
        Ok(sess) => {
            if json_output {
                let output = WhoamiOutput {
                    logged_in: true,
                    login: Some(sess.login.clone()),
                    jwt_expires_at: sess.jwt_exp,
                };
                println!("{}", serde_json::to_string_pretty(&output)?);
            } else {
                println!("Logged in as: {}", sess.login);
                if let Some(exp) = sess.jwt_exp {
                    let now = std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .expect("System time is before UNIX EPOCH")
                        .as_secs();
                    if exp > now {
                        let remaining = exp - now;
                        println!("JWT expires in: {}s", remaining);
                    } else {
                        println!("JWT expired (will auto-refresh on next use)");
                    }
                }
            }
            Ok(())
        }
        Err(_) => {
            if json_output {
                let output = WhoamiOutput {
                    logged_in: false,
                    login: None,
                    jwt_expires_at: None,
                };
                println!("{}", serde_json::to_string_pretty(&output)?);
            } else {
                println!("No active session found. Run 'steadystate login' first.");
            }
            Ok(())
        }
    }
}

async fn logout(client: &Client) -> Result<()> {
    let session = match read_session(None).await {
        Ok(s) => s,
        Err(_) => {
            println!("No active session");
            return Ok(());
        }
    };
    let username = session.login.clone();

    if let Some(refresh) = get_refresh_token(&username, None).await? {
        let url = format!("{}/auth/revoke", &*BACKEND_URL);
        match auth::send_with_retries(|| {
            client
                .post(&url)
                .json(&serde_json::json!({ "refresh_token": refresh.clone() }))
        })
        .await
        {
            Ok(resp) if resp.status().is_success() => {
                info!("Refresh token revoked on server");
            }
            Ok(resp) => {
                warn!("Server revoke returned status: {}", resp.status());
            }
            Err(e) => {
                warn!("Failed to revoke on server: {:#}", e);
            }
        }
    }

    let _ = delete_refresh_token(&username, None).await;
    let _ = remove_session(None).await;
    println!("Logged out (local tokens removed).");
    Ok(())
}

async fn up(client: &Client, repo: String, json: bool, allow: Vec<String>, public: bool, env: Option<String>, mode: Option<String>) -> Result<()> {
    Url::parse(&repo).context(
        "Invalid repository URL. Provide a fully-qualified URL (e.g. https://github.com/user/repo).",
    )?;

    // Validate --env flag
    let env_val = match env {
        Some(e) => e,
        None => {
            eprintln!("Error: --env flag is required.");
            eprintln!("Valid options:");
            eprintln!("  --env=noenv                 Use minimal curated environment");
            eprintln!("  --env=flake                 Use repository's flake.nix");
            eprintln!("  --env=legacy-nix            Use default.nix (nix-shell)");
            eprintln!("  --env=legacy-nix[filename]  Use specified nix file (nix-shell)");
            return Ok(());
        }
    };

    // Check if env is valid
    let is_valid = env_val == "noenv" ||
                   env_val == "flake" ||
                   env_val == "legacy-nix" ||
                   (env_val.starts_with("legacy-nix[") && env_val.ends_with("]"));

    if !is_valid {
        eprintln!("Error: Invalid --env option: {}", env_val);
        eprintln!("Valid options:");
        eprintln!("  --env=noenv                 Use minimal curated environment");
        eprintln!("  --env=flake                 Use repository's flake.nix");
        eprintln!("  --env=legacy-nix            Use default.nix (nix-shell)");
        eprintln!("  --env=legacy-nix[filename]  Use specified nix file (nix-shell)");
        return Ok(());
    }

    // Validate --mode flag
    let mode_val = match mode {
        Some(m) => m,
        None => {
            eprintln!("Error: --mode flag is required.");
            eprintln!("Valid options:");
            eprintln!("  --mode=pair    Pair programming mode (Upterm)");
            eprintln!("  --mode=collab  Collaboration mode (SSH)");
            return Ok(());
        }
    };

    if mode_val != "pair" && mode_val != "collab" {
        eprintln!("Error: Invalid --mode option: {}", mode_val);
        eprintln!("Valid options:");
        eprintln!("  --mode=pair    Pair programming mode (Upterm)");
        eprintln!("  --mode=collab  Collaboration mode (SSH)");
        return Ok(());
    }

    let payload = serde_json::json!({
        "repo_url": repo,
        "allowed_users": if allow.is_empty() { None } else { Some(allow.clone()) },
        "public": public,
        "environment": env_val,
        "mode": mode_val
    });

    let resp: UpResponse = request_with_auth(
        client,
        |c, jwt| {
            c.post(format!("{}/sessions", &*BACKEND_URL))
                .bearer_auth(jwt)
                .json(&payload)
        },
        None,
    )
    .await?;

    let mut final_endpoint = resp.endpoint.clone();
    let mut final_magic_link = resp.magic_link.clone(); // Assuming UpResponse now has magic_link

    if json {
        println!("{}", serde_json::to_string_pretty(&resp)?);
    } else {
        println!("âœ… Session created: {}", resp.id);
        
        // Poll until the session is ready or fails
        if resp.endpoint.is_none() && resp.state == "Provisioning" {
            println!("â³ Provisioning session...");
            
            let mut attempts = 0;
            let max_attempts = 60; // 60 * 1s = 1 minute timeout
            
            loop {
                tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                attempts += 1;
                
                let status: UpResponse = request_with_auth( // Assuming UpResponse can also be used for status
                    client,
                    |c, jwt| {
                        c.get(format!("{}/sessions/{}", &*BACKEND_URL, resp.id))
                            .bearer_auth(jwt)
                    },
                    None,
                )
                .await?;
                
                match status.state.as_str() {
                    "Running" => {
                        final_endpoint = status.endpoint;
                        final_magic_link = status.magic_link; // Update magic link from status
                        
                        if let Some(endpoint) = &final_endpoint {
                            println!("âœ… Session ready!");
                            println!("SSH: {}", endpoint);
                            if let Some(link) = &final_magic_link {
                                println!("Magic Link: {}", link);
                            }
                        } else {
                            println!("âš ï¸  Session is running but no endpoint available");
                        }
                        break;
                    }
                    "Failed" => {
                        println!("âŒ Session provisioning failed");
                        if let Some(msg) = status.message {
                            println!("Error: {}", msg);
                        }
                        return Err(anyhow::anyhow!("Session provisioning failed"));
                    }
                    "Provisioning" => {
                        if attempts >= max_attempts {
                            println!("â±ï¸  Timed out waiting for session. Check status later with:");
                            println!("  curl -H 'Authorization: Bearer <token>' {}/sessions/{}", &*BACKEND_URL, resp.id);
                            break;
                        }
                        // Continue polling
                    }
                    other => {
                        println!("Session state: {}", other);
                    }
                }
            }
        } else if let Some(endpoint) = &resp.endpoint {
            println!("âœ… Session ready!");
            println!("SSH: {}", endpoint);
            if let Some(link) = &resp.magic_link { // Print initial magic link if available
                println!("Magic Link: {}", link);
            }
        }

        // Launch dashboard if in collab mode and we have an endpoint
        if mode_val == "collab" {
            if let Some(endpoint) = final_endpoint {
                println!("Launching dashboard...");
                // Parse endpoint to get host/port/user
                // Endpoint is ssh://steady@host:port
                // We want to run: ssh -t -p port steady@host "steadystate watch"
                
                if let Ok(url) = Url::parse(&endpoint) {
                    let host = url.host_str().unwrap_or("localhost");
                    let port = url.port().unwrap_or(22);
                    let user = url.username();
                    
                    let mut args = vec![
                        "-p".to_string(),
                        port.to_string(),
                        "-t".to_string(), // Force PTY for TUI
                        "-o".to_string(), "StrictHostKeyChecking=no".to_string(),
                        "-o".to_string(), "UserKnownHostsFile=/dev/null".to_string(),
                    ];
                    
                    let target = if !user.is_empty() {
                        format!("{}@{}", user, host)
                    } else {
                        host.to_string()
                    };
                    args.push(target);
                    
                    // Command to run
                    args.push("steadystate watch".to_string());
                    
                    println!("Connecting to dashboard...");
                    use std::os::unix::process::CommandExt;
                    let err = std::process::Command::new("ssh")
                        .args(&args)
                        .exec();
                    return Err(anyhow::anyhow!("Failed to execute ssh: {}", err));
                }
            }
        }
    }

    Ok(())
}

async fn join(url_str: String) -> Result<()> {
    if url_str.starts_with("steadystate://") {
        let url = Url::parse(&url_str).context("Failed to parse magic link")?;
        
        let mode = url.host_str().ok_or_else(|| anyhow::anyhow!("Invalid magic link: missing mode"))?;
        
        match mode {
            "pair" => {
                // Extract upterm param
                let mut pairs = url.query_pairs();
                let upterm_url = pairs
                    .find(|(key, _)| key == "upterm")
                    .map(|(_, val)| val.to_string())
                    .ok_or_else(|| anyhow::anyhow!("Invalid pair link: missing 'upterm' parameter"))?;
                
                println!("Joining pair session...");
                println!("Connecting to: {}", upterm_url);
                
                // Execute ssh
                let up_url = Url::parse(&upterm_url).context("Failed to parse upterm URL")?;
                let host = up_url.host_str().ok_or_else(|| anyhow::anyhow!("Missing host in upterm URL"))?;
                let port = up_url.port().unwrap_or(22);
                let user = up_url.username();
                
                let mut args = vec![
                    "-p".to_string(),
                    port.to_string(),
                    "-o".to_string(),
                    "StrictHostKeyChecking=no".to_string(), // For ephemeral hosts
                    "-o".to_string(),
                    "UserKnownHostsFile=/dev/null".to_string(),
                    "-t".to_string(), // Force PTY
                ];
                
                if !user.is_empty() {
                    args.push(format!("{}@{}", user, host));
                } else {
                    args.push(host.to_string());
                }
                
                use std::os::unix::process::CommandExt;
                let err = std::process::Command::new("ssh")
                    .args(&args)
                    .exec();
                    
                return Err(anyhow::anyhow!("Failed to execute ssh: {}", err));
            }
            "collab" => {
                // Extract ssh param
                let mut pairs = url.query_pairs();
                let ssh_url = pairs
                    .find(|(key, _)| key == "ssh")
                    .map(|(_, val)| val.to_string())
                    .ok_or_else(|| anyhow::anyhow!("Invalid collab link: missing 'ssh' parameter"))?;
                
                println!("Joining collaboration session...");
                println!("Connecting to: {}", ssh_url);
                
                // Execute ssh
                let up_url = Url::parse(&ssh_url).context("Failed to parse SSH URL")?;
                let host = up_url.host_str().ok_or_else(|| anyhow::anyhow!("Missing host in SSH URL"))?;
                let port = up_url.port().unwrap_or(22);
                let user = up_url.username();
                
                let mut args = vec![
                    "-p".to_string(),
                    port.to_string(),
                    "-o".to_string(),
                    "StrictHostKeyChecking=no".to_string(), // For ephemeral hosts
                    "-o".to_string(),
                    "UserKnownHostsFile=/dev/null".to_string(),
                    "-t".to_string(), // Force PTY
                ];
                
                if !user.is_empty() {
                    args.push(format!("{}@{}", user, host));
                } else {
                    args.push(host.to_string());
                }

                // Inject username if available
                let shell_cmd = if let Ok(session) = crate::session::read_session(None).await {
                    format!("export STEADYSTATE_USERNAME={}; exec $SHELL -l", session.login)
                } else {
                    "exec $SHELL -l".to_string()
                };
                args.push(shell_cmd);
                
                use std::os::unix::process::CommandExt;
                let err = std::process::Command::new("ssh")
                    .args(&args)
                    .exec();
                    
                return Err(anyhow::anyhow!("Failed to execute ssh: {}", err));
            }
            _ => {
                return Err(anyhow::anyhow!("Unknown mode: {}", mode));
            }
        }
    } else {
        // Legacy/Direct SSH URL
        println!("Joining via direct SSH...");
        
        if url_str.starts_with("ssh://") {
             let up_url = Url::parse(&url_str).context("Failed to parse SSH URL")?;
             let host = up_url.host_str().ok_or_else(|| anyhow::anyhow!("Missing host in SSH URL"))?;
             let port = up_url.port().unwrap_or(22);
             let user = up_url.username();
             
             let mut args = vec![
                "-p".to_string(),
                port.to_string(),
                "-t".to_string(),
            ];
             
             if !user.is_empty() {
                args.push(format!("{}@{}", user, host));
            } else {
                args.push(host.to_string());
            }
            
            use std::os::unix::process::CommandExt;
            let err = std::process::Command::new("ssh")
                .args(&args)
                .exec();
            return Err(anyhow::anyhow!("Failed to execute ssh: {}", err));
        } else {
            // Assume it's valid ssh arg
             use std::os::unix::process::CommandExt;
            let err = std::process::Command::new("ssh")
                .arg(&url_str)
                .exec();
            return Err(anyhow::anyhow!("Failed to execute ssh: {}", err));
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::fmt()
        .with_env_filter(
            tracing_subscriber::EnvFilter::from_default_env()
                .add_directive(tracing::Level::INFO.into()),
        )
        .init();

    if std::env::var("RUST_LOG")
        .ok()
        .map(|value| value.to_lowercase().contains("debug"))
        .unwrap_or(false)
    {
        eprintln!(
            "âš ï¸ Debug logging is enabled; JWTs and refresh tokens may appear in logs. Proceed carefully."
        );
    }

    let cli = Cli::parse();

    if cli.version {
        println!("SteadyState CLI version {}", CLI_VERSION);
        return Ok(());
    }

    let cmd = match cli.cmd {
        Some(cmd) => cmd,
        None => {
            Cli::command().print_help().ok();
            println!();
            return Ok(());
        }
    };

    // Condition: disable connection pooling during integration tests.
    let mut builder = Client::builder()
        .user_agent(USER_AGENT)
        .timeout(Duration::from_secs(HTTP_TIMEOUT_SECS));

    if std::env::var("STEADYSTATE_BACKEND").is_ok() {
        builder = builder
            .pool_max_idle_per_host(0)
            .pool_idle_timeout(None);
    }

    let client = builder.build().context("create http client")?;

    match cmd {
        Commands::Login { provider } => {
            if let Err(e) = device_login(&client, &provider).await.context(
                "Failed to reach backend. Check network connectivity and the STEADYSTATE_BACKEND environment variable.",
            ) {
                error!("login failed: {:#}", e);
                std::process::exit(1);
            }
        }
        Commands::Whoami { json } => {
            if let Err(e) = whoami(json).await {
                error!("whoami failed: {:#}", e);
                std::process::exit(1);
            }
        }
        Commands::Refresh => match perform_refresh(&client, None, None).await {
            Ok(_) => println!("Token refreshed."),
            Err(e) => {
                error!("refresh failed: {:#}", e);
                std::process::exit(1);
            }
        },
        Commands::Logout => {
            if let Err(e) = logout(&client).await {
                error!("logout failed: {:#}", e);
                std::process::exit(1);
            }
        }
        Commands::Up { repo, json, allow, public, env, mode } => {
            if let Err(e) = up(&client, repo, json, allow, public, env, mode).await {
                let msg = format!("{:#}", e);
                let usage_error = msg.contains("Invalid repository URL.");

                if usage_error {
                    println!("{}", msg);
                } else {
                    eprintln!("up failed: {}", msg);
                }

                std::process::exit(1);
            }
        }
        Commands::Join { url } => {
             if let Err(e) = join(url).await {
                eprintln!("join failed: {:#}", e);
                std::process::exit(1);
            }
        }
        Commands::Sync => {
            if let Err(e) = sync::sync().await {
                eprintln!("sync failed: {:#}", e);
                std::process::exit(1);
            }
        }
        Commands::Watch => {
            if let Err(e) = notify::watch() {
                eprintln!("watch failed: {:#}", e);
                std::process::exit(1);
            }
        }
    }

    Ok(())
} 

########## ./cli/LICENSE

                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.

########## ./.github/workflows/run-tests.yaml

name: unit-tests-cli

on:
  workflow_run:
    workflows: ["Build and Cache"]
    types: [completed]

permissions:
  contents: read

jobs:
  devtools_test:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ${{ matrix.os }}

    strategy:
      matrix:
        os: [macos-latest, ubuntu-latest]

    steps:
      - uses: actions/checkout@v5

      - name: Silence nix-channel warning
        run: mkdir -p ~/.nix-defexpr/channels

      - uses: cachix/install-nix-action@v31

      - uses: cachix/cachix-action@v15
        with:
          name: rstats-on-nix
          authToken: ${{ secrets.CACHIX_AUTH }}

      - name: Build dev shell
        run: |
          export CARGO_BUILD_JOBS=1
          export RUSTFLAGS="-C debuginfo=0 -C opt-level=0"

          SYSTEM=$([ "${{ runner.os }}" = "macOS" ] && echo "aarch64-darwin" || echo "x86_64-linux")
          nix build .#devShells.${SYSTEM}.default

      - name: Run unit tests
        run: |
          export CARGO_BUILD_JOBS=1
          export RUSTFLAGS="-C debuginfo=0 -C opt-level=0"

          nix develop .#devShells.$([ "${{ runner.os }}" = "macOS" ] && echo "aarch64-darwin" || echo "x86_64-linux").default --command cargo test -p steadystate
          nix develop .#devShells.$([ "${{ runner.os }}" = "macOS" ] && echo "aarch64-darwin" || echo "x86_64-linux").default --command cargo test -p steadystate-backend

########## ./.github/workflows/build-steadystate.yaml

name: Build and Cache

on:
  push: {}

jobs:
  build:
    strategy:
      matrix:
        system:
          - x86_64-linux
          - aarch64-darwin
    runs-on: ${{ contains(matrix.system, 'darwin') && 'macos-latest' || 'ubuntu-latest' }}

    steps:
      - uses: actions/checkout@v5
        with:
          repository: The-Exact-Computing-Company/steadystate

      - uses: cachix/install-nix-action@v31

      - uses: cachix/cachix-action@v15
        with:
          name: rstats-on-nix
          authToken: '${{ secrets.CACHIX_AUTH }}'

      - name: Build and push steadystate cli to Cachix
        run: |
          nix build .#cli --print-build-logs
          cachix push rstats-on-nix "$(readlink -f result)"

      - name: Build and push steadystate backend to Cachix
        run: |
          nix build .#backend --print-build-logs
          cachix push rstats-on-nix "$(readlink -f result)"

########## ./.github/workflows/fake-login-test.yml

name: CI Login Test

on:
  workflow_run:
    workflows: ["Build and Cache"]
    types: [completed]

jobs:
  test-fake-auth:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - uses: cachix/install-nix-action@v31

      - uses: cachix/cachix-action@v15
        with:
          name: rstats-on-nix
          authToken: ${{ secrets.CACHIX_AUTH }}

      # We do not build from scratch unless necessary â€” Cachix will provide cached store paths.
      - name: Build workspace with predictable outputs
        run: |
          nix build .#cli -o result-cli
          nix build .#backend -o result-backend

      - name: Start backend with fake auth
        run: |
          export ENABLE_FAKE_AUTH=1
          export JWT_SECRET=test-secret
          export PORT=8080
          export NOENV_FLAKE_PATH=/tmp/dummy-flake

          result-backend/bin/steadystate-backend &
          echo "BACKEND_PID=$!" >> $GITHUB_ENV
          sleep 2

      - name: Test fake login
        run: |
          export STEADYSTATE_BACKEND=http://localhost:8080

          result-cli/bin/steadystate login --provider fake

          # Human-readable output
          result-cli/bin/steadystate whoami | grep "Logged in as: ci-test-user"

          # JSON output
          result-cli/bin/steadystate whoami --json | jq -e '.logged_in == true'
          result-cli/bin/steadystate whoami --json | jq -e '.login == "ci-test-user"'

      - name: Cleanup Backend
        if: always()
        run: |
          echo "Stopping backend process (PID: $BACKEND_PID)..."
          kill "$BACKEND_PID" || true

########## ./.git/config

[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
[remote "origin"]
	url = git@github.com:The-Exact-Computing-Company/steadystate.git
	fetch = +refs/heads/*:refs/remotes/origin/*
[branch "main"]
	remote = origin
	merge = refs/heads/main
	vscode-merge-base = origin/main
[branch "test_up"]
	vscode-merge-base = origin/main
[branch "pull_push_git"]
	vscode-merge-base = origin/main
[branch "fix_unwrap"]
	vscode-merge-base = origin/main
[branch "modes"]
	vscode-merge-base = origin/main
[branch "collab"]
	vscode-merge-base = origin/main
[branch "fix_shared_codebase"]
	vscode-merge-base = origin/main
[branch "major_refactor"]
	vscode-merge-base = origin/main
[branch "fix_sync"]
	vscode-merge-base = origin/main

########## ./.git/hooks/post-update.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to "post-update".

exec git update-server-info

########## ./.git/hooks/prepare-commit-msg.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to prepare the commit log message.
# Called by "git commit" with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to "prepare-commit-msg".

# This hook includes three examples. The first one removes the
# "# Please enter the commit message..." help message.
#
# The second includes the output of "git diff --name-status -r"
# into the message, just before the "git status" output.  It is
# commented because it doesn't cope with --amend or with squashed
# commits.
#
# The third example adds a Signed-off-by line to the message, that can
# still be edited.  This is rarely a good idea.

COMMIT_MSG_FILE=$1
COMMIT_SOURCE=$2
SHA1=$3

/nix/store/k9lfh0l007sp61rag7d9bf3nn5bg3qy7-perl-5.40.0/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"

# case "$COMMIT_SOURCE,$SHA1" in
#  ,|template,)
#    /nix/store/k9lfh0l007sp61rag7d9bf3nn5bg3qy7-perl-5.40.0/bin/perl -i.bak -pe '
#       print "\n" . `git diff --cached --name-status -r`
# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
#  *) ;;
# esac

# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
# if test -z "$COMMIT_SOURCE"
# then
#   /nix/store/k9lfh0l007sp61rag7d9bf3nn5bg3qy7-perl-5.40.0/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
# fi

########## ./.git/hooks/pre-merge-commit.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to verify what is about to be committed.
# Called by "git merge" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message to
# stderr if it wants to stop the merge commit.
#
# To enable this hook, rename this file to "pre-merge-commit".

. git-sh-setup
test -x "$GIT_DIR/hooks/pre-commit" &&
        exec "$GIT_DIR/hooks/pre-commit"
:

########## ./.git/hooks/applypatch-msg.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to "applypatch-msg".

. git-sh-setup
commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
:

########## ./.git/hooks/commit-msg.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to check the commit log message.
# Called by "git commit" with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to "commit-msg".

# Uncomment the below to add a Signed-off-by line to the message.
# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
# hook is more suited to it.
#
# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"

# This example catches duplicate Signed-off-by lines.

test "" = "$(grep '^Signed-off-by: ' "$1" |
	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
	echo >&2 Duplicate Signed-off-by lines.
	exit 1
}

########## ./.git/hooks/push-to-checkout.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash

# An example hook script to update a checked-out tree on a git push.
#
# This hook is invoked by git-receive-pack(1) when it reacts to git
# push and updates reference(s) in its repository, and when the push
# tries to update the branch that is currently checked out and the
# receive.denyCurrentBranch configuration variable is set to
# updateInstead.
#
# By default, such a push is refused if the working tree and the index
# of the remote repository has any difference from the currently
# checked out commit; when both the working tree and the index match
# the current commit, they are updated to match the newly pushed tip
# of the branch. This hook is to be used to override the default
# behaviour; however the code below reimplements the default behaviour
# as a starting point for convenient modification.
#
# The hook receives the commit with which the tip of the current
# branch is going to be updated:
commit=$1

# It can exit with a non-zero status to refuse the push (when it does
# so, it must not modify the index or the working tree).
die () {
	echo >&2 "$*"
	exit 1
}

# Or it can make any necessary changes to the working tree and to the
# index to bring them to the desired state when the tip of the current
# branch is updated to the new commit, and exit with a zero status.
#
# For example, the hook can simply run git read-tree -u -m HEAD "$1"
# in order to emulate git fetch that is run in the reverse direction
# with git push, as the two-tree form of git read-tree -u -m is
# essentially the same as git switch or git checkout that switches
# branches while keeping the local changes in the working tree that do
# not interfere with the difference between the branches.

# The below is a more-or-less exact translation to shell of the C code
# for the default behaviour for git's push-to-checkout hook defined in
# the push_to_deploy() function in builtin/receive-pack.c.
#
# Note that the hook will be executed from the repository directory,
# not from the working tree, so if you want to perform operations on
# the working tree, you will have to adapt your code accordingly, e.g.
# by adding "cd .." or using relative paths.

if ! git update-index -q --ignore-submodules --refresh
then
	die "Up-to-date check failed"
fi

if ! git diff-files --quiet --ignore-submodules --
then
	die "Working directory has unstaged changes"
fi

# This is a rough translation of:
#
#   head_has_history() ? "HEAD" : EMPTY_TREE_SHA1_HEX
if git cat-file -e HEAD 2>/dev/null
then
	head=HEAD
else
	head=$(git hash-object -t tree --stdin </dev/null)
fi

if ! git diff-index --quiet --cached --ignore-submodules $head --
then
	die "Working directory has staged changes"
fi

if ! git read-tree -u -m "$commit"
then
	die "Could not update working tree to new HEAD"
fi

########## ./.git/hooks/pre-receive.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to make use of push options.
# The example simply echoes all push options that start with 'echoback='
# and rejects all pushes when the "reject" push option is used.
#
# To enable this hook, rename this file to "pre-receive".

if test -n "$GIT_PUSH_OPTION_COUNT"
then
	i=0
	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
	do
		eval "value=\$GIT_PUSH_OPTION_$i"
		case "$value" in
		echoback=*)
			echo "echo from the pre-receive-hook: ${value#*=}" >&2
			;;
		reject)
			exit 1
		esac
		i=$((i + 1))
	done
fi

########## ./.git/hooks/fsmonitor-watchman.sample

#!/nix/store/k9lfh0l007sp61rag7d9bf3nn5bg3qy7-perl-5.40.0/bin/perl

use strict;
use warnings;
use IPC::Open2;

# An example hook script to integrate Watchman
# (https://facebook.github.io/watchman/) with git to speed up detecting
# new and modified files.
#
# The hook is passed a version (currently 2) and last update token
# formatted as a string and outputs to stdout a new update token and
# all files that have been modified since the update token. Paths must
# be relative to the root of the working tree and separated by a single NUL.
#
# To enable this hook, rename this file to "query-watchman" and set
# 'git config core.fsmonitor .git/hooks/query-watchman'
#
my ($version, $last_update_token) = @ARGV;

# Uncomment for debugging
# print STDERR "$0 $version $last_update_token\n";

# Check the hook interface version
if ($version ne 2) {
	die "Unsupported query-fsmonitor hook version '$version'.\n" .
	    "Falling back to scanning...\n";
}

my $git_work_tree = get_working_dir();

my $retry = 1;

my $json_pkg;
eval {
	require JSON::XS;
	$json_pkg = "JSON::XS";
	1;
} or do {
	require JSON::PP;
	$json_pkg = "JSON::PP";
};

launch_watchman();

sub launch_watchman {
	my $o = watchman_query();
	if (is_work_tree_watched($o)) {
		output_result($o->{clock}, @{$o->{files}});
	}
}

sub output_result {
	my ($clockid, @files) = @_;

	# Uncomment for debugging watchman output
	# open (my $fh, ">", ".git/watchman-output.out");
	# binmode $fh, ":utf8";
	# print $fh "$clockid\n@files\n";
	# close $fh;

	binmode STDOUT, ":utf8";
	print $clockid;
	print "\0";
	local $, = "\0";
	print @files;
}

sub watchman_clock {
	my $response = qx/watchman clock "$git_work_tree"/;
	die "Failed to get clock id on '$git_work_tree'.\n" .
		"Falling back to scanning...\n" if $? != 0;

	return $json_pkg->new->utf8->decode($response);
}

sub watchman_query {
	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
	or die "open2() failed: $!\n" .
	"Falling back to scanning...\n";

	# In the query expression below we're asking for names of files that
	# changed since $last_update_token but not from the .git folder.
	#
	# To accomplish this, we're using the "since" generator to use the
	# recency index to select candidate nodes and "fields" to limit the
	# output to file names only. Then we're using the "expression" term to
	# further constrain the results.
	my $last_update_line = "";
	if (substr($last_update_token, 0, 1) eq "c") {
		$last_update_token = "\"$last_update_token\"";
		$last_update_line = qq[\n"since": $last_update_token,];
	}
	my $query = <<"	END";
		["query", "$git_work_tree", {$last_update_line
			"fields": ["name"],
			"expression": ["not", ["dirname", ".git"]]
		}]
	END

	# Uncomment for debugging the watchman query
	# open (my $fh, ">", ".git/watchman-query.json");
	# print $fh $query;
	# close $fh;

	print CHLD_IN $query;
	close CHLD_IN;
	my $response = do {local $/; <CHLD_OUT>};

	# Uncomment for debugging the watch response
	# open ($fh, ">", ".git/watchman-response.json");
	# print $fh $response;
	# close $fh;

	die "Watchman: command returned no output.\n" .
	"Falling back to scanning...\n" if $response eq "";
	die "Watchman: command returned invalid output: $response\n" .
	"Falling back to scanning...\n" unless $response =~ /^\{/;

	return $json_pkg->new->utf8->decode($response);
}

sub is_work_tree_watched {
	my ($output) = @_;
	my $error = $output->{error};
	if ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {
		$retry--;
		my $response = qx/watchman watch "$git_work_tree"/;
		die "Failed to make watchman watch '$git_work_tree'.\n" .
		    "Falling back to scanning...\n" if $? != 0;
		$output = $json_pkg->new->utf8->decode($response);
		$error = $output->{error};
		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		# Uncomment for debugging watchman output
		# open (my $fh, ">", ".git/watchman-output.out");
		# close $fh;

		# Watchman will always return all files on the first query so
		# return the fast "everything is dirty" flag to git and do the
		# Watchman query just to get it over with now so we won't pay
		# the cost in git to look up each individual file.
		my $o = watchman_clock();
		$error = $output->{error};

		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		output_result($o->{clock}, ("/"));
		$last_update_token = $o->{clock};

		eval { launch_watchman() };
		return 0;
	}

	die "Watchman: $error.\n" .
	"Falling back to scanning...\n" if $error;

	return 1;
}

sub get_working_dir {
	my $working_dir;
	if ($^O =~ 'msys' || $^O =~ 'cygwin') {
		$working_dir = Win32::GetCwd();
		$working_dir =~ tr/\\/\//;
	} else {
		require Cwd;
		$working_dir = Cwd::cwd();
	}

	return $working_dir;
}

########## ./.git/hooks/pre-rebase.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The "pre-rebase" hook is run just before "git rebase" starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#
# $1 -- the upstream the series was forked from.
# $2 -- the branch being rebased (or empty when rebasing the current branch).
#
# This sample shows how to prevent topic branches that are already
# merged to 'next' branch from getting rebased, because allowing it
# would result in rebasing already published history.

publish=next
basebranch="$1"
if test "$#" = 2
then
	topic="refs/heads/$2"
else
	topic=`git symbolic-ref HEAD` ||
	exit 0 ;# we do not interrupt rebasing detached HEAD
fi

case "$topic" in
refs/heads/??/*)
	;;
*)
	exit 0 ;# we do not interrupt others.
	;;
esac

# Now we are dealing with a topic branch being rebased
# on top of master.  Is it OK to rebase it?

# Does the topic really exist?
git show-ref -q "$topic" || {
	echo >&2 "No such branch $topic"
	exit 1
}

# Is topic fully merged to master?
not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
if test -z "$not_in_master"
then
	echo >&2 "$topic is fully merged to master; better remove it."
	exit 1 ;# we could allow it, but there is no point.
fi

# Is topic ever merged to next?  If so you should not be rebasing it.
only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
only_next_2=`git rev-list ^master           ${publish} | sort`
if test "$only_next_1" = "$only_next_2"
then
	not_in_topic=`git rev-list "^$topic" master`
	if test -z "$not_in_topic"
	then
		echo >&2 "$topic is already up to date with master"
		exit 1 ;# we could allow it, but there is no point.
	else
		exit 0
	fi
else
	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
	/nix/store/k9lfh0l007sp61rag7d9bf3nn5bg3qy7-perl-5.40.0/bin/perl -e '
		my $topic = $ARGV[0];
		my $msg = "* $topic has commits already merged to public branch:\n";
		my (%not_in_next) = map {
			/^([0-9a-f]+) /;
			($1 => 1);
		} split(/\n/, $ARGV[1]);
		for my $elem (map {
				/^([0-9a-f]+) (.*)$/;
				[$1 => $2];
			} split(/\n/, $ARGV[2])) {
			if (!exists $not_in_next{$elem->[0]}) {
				if ($msg) {
					print STDERR $msg;
					undef $msg;
				}
				print STDERR " $elem->[1]\n";
			}
		}
	' "$topic" "$not_in_next" "$not_in_master"
	exit 1
fi

<<\DOC_END

This sample hook safeguards topic branches that have been
published from being rewound.

The workflow assumed here is:

 * Once a topic branch forks from "master", "master" is never
   merged into it again (either directly or indirectly).

 * Once a topic branch is fully cooked and merged into "master",
   it is deleted.  If you need to build on top of it to correct
   earlier mistakes, a new topic branch is created by forking at
   the tip of the "master".  This is not strictly necessary, but
   it makes it easier to keep your history simple.

 * Whenever you need to test or publish your changes to topic
   branches, merge them into "next" branch.

The script, being an example, hardcodes the publish branch name
to be "next", but it is trivial to make it configurable via
$GIT_DIR/config mechanism.

With this workflow, you would want to know:

(1) ... if a topic branch has ever been merged to "next".  Young
    topic branches can have stupid mistakes you would rather
    clean up before publishing, and things that have not been
    merged into other branches can be easily rebased without
    affecting other people.  But once it is published, you would
    not want to rewind it.

(2) ... if a topic branch has been fully merged to "master".
    Then you can delete it.  More importantly, you should not
    build on top of it -- other people may already want to
    change things related to the topic as patches against your
    "master", so if you need further changes, it is better to
    fork the topic (perhaps with the same name) afresh from the
    tip of "master".

Let's look at this example:

		   o---o---o---o---o---o---o---o---o---o "next"
		  /       /           /           /
		 /   a---a---b A     /           /
		/   /               /           /
	       /   /   c---c---c---c B         /
	      /   /   /             \         /
	     /   /   /   b---b C     \       /
	    /   /   /   /             \     /
    ---o---o---o---o---o---o---o---o---o---o---o "master"


A, B and C are topic branches.

 * A has one fix since it was merged up to "next".

 * B has finished.  It has been fully merged up to "master" and "next",
   and is ready to be deleted.

 * C has not merged to "next" at all.

We would want to allow C to be rebased, refuse A, and encourage
B to be deleted.

To compute (1):

	git rev-list ^master ^topic next
	git rev-list ^master        next

	if these match, topic has not merged in next at all.

To compute (2):

	git rev-list master..topic

	if this is empty, it is fully merged to "master".

DOC_END

########## ./.git/hooks/pre-push.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash

# An example hook script to verify what is about to be pushed.  Called by "git
# push" after it has checked the remote status, but before anything has been
# pushed.  If this script exits with a non-zero status nothing will be pushed.
#
# This hook is called with the following parameters:
#
# $1 -- Name of the remote to which the push is being done
# $2 -- URL to which the push is being done
#
# If pushing without using a named remote those arguments will be equal.
#
# Information about the commits which are being pushed is supplied as lines to
# the standard input in the form:
#
#   <local ref> <local oid> <remote ref> <remote oid>
#
# This sample shows how to prevent push of commits where the log message starts
# with "WIP" (work in progress).

remote="$1"
url="$2"

zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')

while read local_ref local_oid remote_ref remote_oid
do
	if test "$local_oid" = "$zero"
	then
		# Handle delete
		:
	else
		if test "$remote_oid" = "$zero"
		then
			# New branch, examine all commits
			range="$local_oid"
		else
			# Update to existing branch, examine new commits
			range="$remote_oid..$local_oid"
		fi

		# Check for WIP commit
		commit=$(git rev-list -n 1 --grep '^WIP' "$range")
		if test -n "$commit"
		then
			echo >&2 "Found WIP commit in $local_ref, not pushing"
			exit 1
		fi
	fi
done

exit 0

########## ./.git/hooks/pre-commit.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to verify what is about to be committed.
# Called by "git commit" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-commit".

if git rev-parse --verify HEAD >/dev/null 2>&1
then
	against=HEAD
else
	# Initial commit: diff against an empty tree object
	against=$(git hash-object -t tree /dev/null)
fi

# If you want to allow non-ASCII filenames set this variable to true.
allownonascii=$(git config --type=bool hooks.allownonascii)

# Redirect output to stderr.
exec 1>&2

# Cross platform projects tend to avoid non-ASCII filenames; prevent
# them from being added to the repository. We exploit the fact that the
# printable range starts at the space character and ends with tilde.
if [ "$allownonascii" != "true" ] &&
	# Note that the use of brackets around a tr range is ok here, (it's
	# even required, for portability to Solaris 10's /usr/bin/tr), since
	# the square bracket bytes happen to fall in the designated range.
	test $(git diff-index --cached --name-only --diff-filter=A -z $against |
	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
then
	cat <<\EOF
Error: Attempt to add a non-ASCII file name.

This can cause problems if you want to work with people on other platforms.

To be portable it is advisable to rename the file.

If you know what you are doing you can disable this check using:

  git config hooks.allownonascii true
EOF
	exit 1
fi

# If there are whitespace errors, print the offending file names and fail.
exec git diff-index --check --cached $against --

########## ./.git/hooks/sendemail-validate.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash

# An example hook script to validate a patch (and/or patch series) before
# sending it via email.
#
# The hook should exit with non-zero status after issuing an appropriate
# message if it wants to prevent the email(s) from being sent.
#
# To enable this hook, rename this file to "sendemail-validate".
#
# By default, it will only check that the patch(es) can be applied on top of
# the default upstream branch without conflicts in a secondary worktree. After
# validation (successful or not) of the last patch of a series, the worktree
# will be deleted.
#
# The following config variables can be set to change the default remote and
# remote ref that are used to apply the patches against:
#
#   sendemail.validateRemote (default: origin)
#   sendemail.validateRemoteRef (default: HEAD)
#
# Replace the TODO placeholders with appropriate checks according to your
# needs.

validate_cover_letter () {
	file="$1"
	# TODO: Replace with appropriate checks (e.g. spell checking).
	true
}

validate_patch () {
	file="$1"
	# Ensure that the patch applies without conflicts.
	git am -3 "$file" || return
	# TODO: Replace with appropriate checks for this patch
	# (e.g. checkpatch.pl).
	true
}

validate_series () {
	# TODO: Replace with appropriate checks for the whole series
	# (e.g. quick build, coding style checks, etc.).
	true
}

# main -------------------------------------------------------------------------

if test "$GIT_SENDEMAIL_FILE_COUNTER" = 1
then
	remote=$(git config --default origin --get sendemail.validateRemote) &&
	ref=$(git config --default HEAD --get sendemail.validateRemoteRef) &&
	worktree=$(mktemp --tmpdir -d sendemail-validate.XXXXXXX) &&
	git worktree add -fd --checkout "$worktree" "refs/remotes/$remote/$ref" &&
	git config --replace-all sendemail.validateWorktree "$worktree"
else
	worktree=$(git config --get sendemail.validateWorktree)
fi || {
	echo "sendemail-validate: error: failed to prepare worktree" >&2
	exit 1
}

unset GIT_DIR GIT_WORK_TREE
cd "$worktree" &&

if grep -q "^diff --git " "$1"
then
	validate_patch "$1"
else
	validate_cover_letter "$1"
fi &&

if test "$GIT_SENDEMAIL_FILE_COUNTER" = "$GIT_SENDEMAIL_FILE_TOTAL"
then
	git config --unset-all sendemail.validateWorktree &&
	trap 'git worktree remove -ff "$worktree"' EXIT &&
	validate_series
fi

########## ./.git/hooks/update.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to block unannotated tags from entering.
# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to "update".
#
# Config
# ------
# hooks.allowunannotated
#   This boolean sets whether unannotated tags will be allowed into the
#   repository.  By default they won't be.
# hooks.allowdeletetag
#   This boolean sets whether deleting tags will be allowed in the
#   repository.  By default they won't be.
# hooks.allowmodifytag
#   This boolean sets whether a tag may be modified after creation. By default
#   it won't be.
# hooks.allowdeletebranch
#   This boolean sets whether deleting branches will be allowed in the
#   repository.  By default they won't be.
# hooks.denycreatebranch
#   This boolean sets whether remotely creating branches will be denied
#   in the repository.  By default this is allowed.
#

# --- Command line
refname="$1"
oldrev="$2"
newrev="$3"

# --- Safety check
if [ -z "$GIT_DIR" ]; then
	echo "Don't run this script from the command line." >&2
	echo " (if you want, you could supply GIT_DIR then run" >&2
	echo "  $0 <ref> <oldrev> <newrev>)" >&2
	exit 1
fi

if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
	exit 1
fi

# --- Config
allowunannotated=$(git config --type=bool hooks.allowunannotated)
allowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)
denycreatebranch=$(git config --type=bool hooks.denycreatebranch)
allowdeletetag=$(git config --type=bool hooks.allowdeletetag)
allowmodifytag=$(git config --type=bool hooks.allowmodifytag)

# check for no description
projectdesc=$(sed -e '1q' "$GIT_DIR/description")
case "$projectdesc" in
"Unnamed repository"* | "")
	echo "*** Project description file hasn't been set" >&2
	exit 1
	;;
esac

# --- Check types
# if $newrev is 0000...0000, it's a commit to delete a ref.
zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
if [ "$newrev" = "$zero" ]; then
	newrev_type=delete
else
	newrev_type=$(git cat-file -t $newrev)
fi

case "$refname","$newrev_type" in
	refs/tags/*,commit)
		# un-annotated tag
		short_refname=${refname##refs/tags/}
		if [ "$allowunannotated" != "true" ]; then
			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
			exit 1
		fi
		;;
	refs/tags/*,delete)
		# delete tag
		if [ "$allowdeletetag" != "true" ]; then
			echo "*** Deleting a tag is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/tags/*,tag)
		# annotated tag
		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
		then
			echo "*** Tag '$refname' already exists." >&2
			echo "*** Modifying a tag is not allowed in this repository." >&2
			exit 1
		fi
		;;
	refs/heads/*,commit)
		# branch
		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
			echo "*** Creating a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/heads/*,delete)
		# delete branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/remotes/*,commit)
		# tracking branch
		;;
	refs/remotes/*,delete)
		# delete tracking branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	*)
		# Anything else (is there anything else?)
		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
		exit 1
		;;
esac

# --- Finished
exit 0

########## ./.git/hooks/pre-applypatch.sample

#!/nix/store/l9k32vj2aczxw62134j1x0dsh569jz2l-bash-5.2p37/bin/bash
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-applypatch".

. git-sh-setup
precommit="$(git rev-parse --git-path hooks/pre-commit)"
test -x "$precommit" && exec "$precommit" ${1+"$@"}
:

########## ./.git/HEAD

ref: refs/heads/main

########## ./.git/COMMIT_EDITMSG

fixed welcome message

########## ./.git/info/exclude

# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~

########## ./.git/logs/HEAD

0000000000000000000000000000000000000000 223372f5543ac5281c11af1a5872000722f1af56 Bruno Rodrigues <bruno@brodrigues.co> 1762527849 +0100	clone: from github.com:The-Exact-Computing-Company/steadystate-cli.git
223372f5543ac5281c11af1a5872000722f1af56 09a82bc67e69f7690866226ed594a8ca92e7038f Bruno Rodrigues <bruno@brodrigues.co> 1762528493 +0100	commit: first commit
09a82bc67e69f7690866226ed594a8ca92e7038f d89e69360f3cb1029063bc16ef54314da5131589 Bruno Rodrigues <bruno@brodrigues.co> 1762531376 +0100	commit: compiles
d89e69360f3cb1029063bc16ef54314da5131589 69bdb5ebd2103e7788c048cc2593e146afca0759 Bruno Rodrigues <bruno@brodrigues.co> 1762701963 +0100	pull origin main: Fast-forward
69bdb5ebd2103e7788c048cc2593e146afca0759 db7333e1de953ba1c763bf5dd5806f108ebe27de Bruno Rodrigues <bruno@brodrigues.co> 1762703013 +0100	commit: updated dep
db7333e1de953ba1c763bf5dd5806f108ebe27de 936dd51a02cd50a5ee674bcf583ef97d9f304de0 Bruno Rodrigues <bruno@brodrigues.co> 1762703429 +0100	commit: CI: unit tests
936dd51a02cd50a5ee674bcf583ef97d9f304de0 73ee36fe9f01f4d5e6505d6a2f63a2839666668e Bruno Rodrigues <bruno@brodrigues.co> 1762706141 +0100	commit: more tests
73ee36fe9f01f4d5e6505d6a2f63a2839666668e 74a6e45d075e23958b6fe9c1bee3d6b16dfe6fa1 Bruno Rodrigues <bruno@brodrigues.co> 1762790438 +0100	checkout: moving from main to b-rodrigues-patch-1
74a6e45d075e23958b6fe9c1bee3d6b16dfe6fa1 3e82efb5c54be22b25d8d521f3307da45414fb81 Bruno Rodrigues <bruno@brodrigues.co> 1762790549 +0100	commit: implementing mock server using rouille
3e82efb5c54be22b25d8d521f3307da45414fb81 73ee36fe9f01f4d5e6505d6a2f63a2839666668e Bruno Rodrigues <bruno@brodrigues.co> 1762848146 +0100	checkout: moving from b-rodrigues-patch-1 to main
73ee36fe9f01f4d5e6505d6a2f63a2839666668e 3440dec281dbabebf18cd2db75a107d9ab523d4b Bruno Rodrigues <bruno@brodrigues.co> 1762848232 +0100	pull origin main: Fast-forward
3440dec281dbabebf18cd2db75a107d9ab523d4b 7c59b72ae422fd06a4f2bc7215563d33b6512f11 Bruno Rodrigues <bruno@brodrigues.co> 1762848238 +0100	checkout: moving from main to b-rodrigues-patch-2
7c59b72ae422fd06a4f2bc7215563d33b6512f11 7570585ac33ae161a2edeb4c03e64213c0004c4e Bruno Rodrigues <bruno@brodrigues.co> 1762848475 +0100	commit: creaty keychain for macOS runner
7570585ac33ae161a2edeb4c03e64213c0004c4e a21f12c87e395669e78036dbfd32f8ee56930eea Bruno Rodrigues <bruno@brodrigues.co> 1762848938 +0100	commit: close connections to avoid race conditions
a21f12c87e395669e78036dbfd32f8ee56930eea 9ed3fc29cbb48f129ec057d0aca878317d59e357 Bruno Rodrigues <bruno@brodrigues.co> 1762849255 +0100	commit: better test harness
9ed3fc29cbb48f129ec057d0aca878317d59e357 1f39e81bfbb796424887836be3d36643ce6bf9e9 Bruno Rodrigues <bruno@brodrigues.co> 1762849554 +0100	commit: fix harness ownership
1f39e81bfbb796424887836be3d36643ce6bf9e9 276a92e610467d2746ad93a36397f49740612f66 Bruno Rodrigues <bruno@brodrigues.co> 1762850250 +0100	commit: addresses deadlock except: 100-continue
276a92e610467d2746ad93a36397f49740612f66 22547fba67b7e78bad3e214141a534918d5f3a2c Bruno Rodrigues <bruno@brodrigues.co> 1762850691 +0100	commit: simpler headers
22547fba67b7e78bad3e214141a534918d5f3a2c b7dbb5f7688cd5cdb0bde012cdb28b2de095231f Bruno Rodrigues <bruno@brodrigues.co> 1762851589 +0100	commit: remove flakyness
b7dbb5f7688cd5cdb0bde012cdb28b2de095231f 345daa90c92f3aba3dcafd311f931bd153a7ff99 Bruno Rodrigues <bruno@brodrigues.co> 1762852148 +0100	commit: more complexity to remove flakyness
345daa90c92f3aba3dcafd311f931bd153a7ff99 54d59b33fb733e6660d0353415c6c3f9ce05bb52 Bruno Rodrigues <bruno@brodrigues.co> 1762852944 +0100	commit: fixed client
54d59b33fb733e6660d0353415c6c3f9ce05bb52 e2877dcb7f72f73fc7f8fb75efe00555e2586ba9 Bruno Rodrigues <bruno@brodrigues.co> 1762866598 +0100	commit: remove problematic test that doesn't really match real behaviour
e2877dcb7f72f73fc7f8fb75efe00555e2586ba9 78bbdd54be021c4575d91ed2297c10bc3504baa7 Bruno Rodrigues <bruno@brodrigues.co> 1762889989 +0100	pull origin b-rodrigues-patch-2: Fast-forward
78bbdd54be021c4575d91ed2297c10bc3504baa7 3440dec281dbabebf18cd2db75a107d9ab523d4b Bruno Rodrigues <bruno@brodrigues.co> 1763026357 +0100	checkout: moving from b-rodrigues-patch-2 to main
3440dec281dbabebf18cd2db75a107d9ab523d4b a024e486499bd3a013381aec430f94472488af6f Bruno Rodrigues <bruno@brodrigues.co> 1763026361 +0100	pull origin main: Fast-forward
a024e486499bd3a013381aec430f94472488af6f 015587558f53820b270a45b0f91bf34d1d890971 Bruno Rodrigues <bruno@brodrigues.co> 1763027147 +0100	commit: readmes
015587558f53820b270a45b0f91bf34d1d890971 a6e5bdd05f95ee56d097edf29ce1f193f20e2191 Bruno Rodrigues <bruno@brodrigues.co> 1763194458 +0100	pull origin main: Fast-forward
a6e5bdd05f95ee56d097edf29ce1f193f20e2191 05b8b8b37c39eb2ce9bd53a510a128ec4a21cb0c Bruno Rodrigues <bruno@brodrigues.co> 1763194684 +0100	commit: licenses
05b8b8b37c39eb2ce9bd53a510a128ec4a21cb0c 18ae3af6cb568fd98e29a37b9447f21818c2dd89 Bruno Rodrigues <bruno@brodrigues.co> 1763199507 +0100	commit: device flow woring, but polling doesn't return jwt
18ae3af6cb568fd98e29a37b9447f21818c2dd89 b981ed1eddcb189816d23387c77bca78c7dfc2bd Bruno Rodrigues <bruno@brodrigues.co> 1763200672 +0100	commit: polling now works
b981ed1eddcb189816d23387c77bca78c7dfc2bd b981ed1eddcb189816d23387c77bca78c7dfc2bd Bruno Rodrigues <bruno@brodrigues.co> 1763201077 +0100	checkout: moving from main to update_cli
b981ed1eddcb189816d23387c77bca78c7dfc2bd f03fa859252f66ca01942003435647de1857fbbf Bruno Rodrigues <bruno@brodrigues.co> 1763201966 +0100	commit: starting to adapt cli to backend
f03fa859252f66ca01942003435647de1857fbbf cda55629dee71c105810b3b034e5da3e4f8156aa Bruno Rodrigues <bruno@brodrigues.co> 1763216183 +0100	commit: single flake to build both tools
cda55629dee71c105810b3b034e5da3e4f8156aa 748f58e7cf188e2e7115eb9a6634fb7db8e1d046 Bruno Rodrigues <bruno@brodrigues.co> 1763216760 +0100	commit: set backend url in flake for local dev
748f58e7cf188e2e7115eb9a6634fb7db8e1d046 9f9407feb6369cc0f6a6673a055699f94d85a4f1 Bruno Rodrigues <bruno@brodrigues.co> 1763216767 +0100	commit: added contributing section
9f9407feb6369cc0f6a6673a055699f94d85a4f1 5ae6f37f656d1eca8d1b0961e3f77bb080eb40fc Bruno Rodrigues <bruno@brodrigues.co> 1763217136 +0100	commit: pointing to the right xterm binary
5ae6f37f656d1eca8d1b0961e3f77bb080eb40fc 9084bf403618aa6f9704b1e4d1721cd6fb4ea530 Bruno Rodrigues <bruno@brodrigues.co> 1763217377 +0100	commit: tests passing
9084bf403618aa6f9704b1e4d1721cd6fb4ea530 b981ed1eddcb189816d23387c77bca78c7dfc2bd Bruno Rodrigues <bruno@brodrigues.co> 1763217411 +0100	checkout: moving from update_cli to main
b981ed1eddcb189816d23387c77bca78c7dfc2bd 1761805aefde4b3a1f59c7e79634533438681dd4 Bruno Rodrigues <bruno@brodrigues.co> 1763217429 +0100	pull origin main: Fast-forward
1761805aefde4b3a1f59c7e79634533438681dd4 e927c84251f915ff853b4be76c66185029fe838d Bruno Rodrigues <bruno@brodrigues.co> 1763217593 +0100	commit: build env in a separate step
e927c84251f915ff853b4be76c66185029fe838d e927c84251f915ff853b4be76c66185029fe838d Bruno Rodrigues <bruno@brodrigues.co> 1763218960 +0100	checkout: moving from main to fake_provider
e927c84251f915ff853b4be76c66185029fe838d e927c84251f915ff853b4be76c66185029fe838d Bruno Rodrigues <bruno@brodrigues.co> 1763316523 +0100	reset: moving to HEAD
e927c84251f915ff853b4be76c66185029fe838d e3c59b074abd6b69e68cf71c81b4e12b324788a9 Bruno Rodrigues <bruno@brodrigues.co> 1763316537 +0100	commit: experiment but certainly for trash
e3c59b074abd6b69e68cf71c81b4e12b324788a9 e927c84251f915ff853b4be76c66185029fe838d Bruno Rodrigues <bruno@brodrigues.co> 1763316539 +0100	checkout: moving from fake_provider to main
e927c84251f915ff853b4be76c66185029fe838d 6c7b61fb9b4b788009a9f3b95b0143ed79426074 Bruno Rodrigues <bruno@brodrigues.co> 1763316545 +0100	pull origin main: Fast-forward
6c7b61fb9b4b788009a9f3b95b0143ed79426074 4db52a1c1ef801d84b775ff49edb945bc07577bf Bruno Rodrigues <bruno@brodrigues.co> 1763317065 +0100	commit: treemerge is now locked
4db52a1c1ef801d84b775ff49edb945bc07577bf ed6405e59ec04d7f830c66cdccf36ffed404f0c3 Bruno Rodrigues <bruno@brodrigues.co> 1763451950 +0100	pull origin main: Fast-forward
ed6405e59ec04d7f830c66cdccf36ffed404f0c3 0916f0e7a3938ccabfb0d4e299219059cce29cbe Bruno Rodrigues <bruno@brodrigues.co> 1763452871 +0100	commit: treemerge from cache
0916f0e7a3938ccabfb0d4e299219059cce29cbe c7d1a2274872b3d8844be09d54a03cb93999a7e4 Bruno Rodrigues <bruno@brodrigues.co> 1763454441 +0100	commit: github login flow working again
c7d1a2274872b3d8844be09d54a03cb93999a7e4 c7d1a2274872b3d8844be09d54a03cb93999a7e4 Bruno Rodrigues <bruno@brodrigues.co> 1763458108 +0100	checkout: moving from main to add_up
c7d1a2274872b3d8844be09d54a03cb93999a7e4 6ff7aaeb391d953205a6e40b99fde45fcd65d3c7 Bruno Rodrigues <bruno@brodrigues.co> 1763458617 +0100	commit: added --noenv minimal flake
6ff7aaeb391d953205a6e40b99fde45fcd65d3c7 8593c07bacbf6f74faf7ee236091f8e7a92647b9 Bruno Rodrigues <bruno@brodrigues.co> 1763458646 +0100	commit: added lock file
8593c07bacbf6f74faf7ee236091f8e7a92647b9 95342c02bf19cddaa0866c637ca608d28a769bb8 Bruno Rodrigues <bruno@brodrigues.co> 1763459385 +0100	commit: removing cachix conf
95342c02bf19cddaa0866c637ca608d28a769bb8 0764afffbe5305487f400fc4bea544b1fc484fc2 Bruno Rodrigues <bruno@brodrigues.co> 1763461382 +0100	commit: first draft implementation of local provider
0764afffbe5305487f400fc4bea544b1fc484fc2 d3d05f4083ca1ff804964e2df4d69002573d7a25 Bruno Rodrigues <bruno@brodrigues.co> 1763464982 +0100	commit: updated lock files
d3d05f4083ca1ff804964e2df4d69002573d7a25 887263e1e2e863ecb68a113879ca668a6fd6086f Bruno Rodrigues <bruno@brodrigues.co> 1763467055 +0100	commit: jwtwebtoken to jwt-simple
887263e1e2e863ecb68a113879ca668a6fd6086f 98ba9ffc6b3a4fa03ee381a6f8bb367c6f96ee6a Bruno Rodrigues <bruno@brodrigues.co> 1763473237 +0100	pull origin add_up: Fast-forward
98ba9ffc6b3a4fa03ee381a6f8bb367c6f96ee6a 93355e87ecc9ff277070fc451dfc42fe39e96173 Bruno Rodrigues <bruno@brodrigues.co> 1763484505 +0100	pull origin add_up: Fast-forward
93355e87ecc9ff277070fc451dfc42fe39e96173 c7d1a2274872b3d8844be09d54a03cb93999a7e4 Bruno Rodrigues <bruno@brodrigues.co> 1763484507 +0100	checkout: moving from add_up to main
c7d1a2274872b3d8844be09d54a03cb93999a7e4 93355e87ecc9ff277070fc451dfc42fe39e96173 Bruno Rodrigues <bruno@brodrigues.co> 1763484810 +0100	checkout: moving from main to add_up
93355e87ecc9ff277070fc451dfc42fe39e96173 903a56c82459d53cbb3030a2f986bb35d30b7b73 Bruno Rodrigues <bruno@brodrigues.co> 1763484826 +0100	pull origin add_up: Fast-forward
903a56c82459d53cbb3030a2f986bb35d30b7b73 f34f027de07741d6427b8fd3d666c62b97dfbf5a Bruno Rodrigues <bruno@brodrigues.co> 1763488459 +0100	commit: added missing deps
f34f027de07741d6427b8fd3d666c62b97dfbf5a dfd5004e600c7a7e19742f2b38e219ab36d47278 Bruno Rodrigues <bruno@brodrigues.co> 1763488474 +0100	pull --rebase origin add_up (start): checkout dfd5004e600c7a7e19742f2b38e219ab36d47278
dfd5004e600c7a7e19742f2b38e219ab36d47278 fc16c2a65152d5613a3c3cd319c8fa07916047f9 Bruno Rodrigues <bruno@brodrigues.co> 1763488474 +0100	pull --rebase origin add_up (pick): added missing deps
fc16c2a65152d5613a3c3cd319c8fa07916047f9 fc16c2a65152d5613a3c3cd319c8fa07916047f9 Bruno Rodrigues <bruno@brodrigues.co> 1763488474 +0100	pull --rebase origin add_up (finish): returning to refs/heads/add_up
fc16c2a65152d5613a3c3cd319c8fa07916047f9 2242e50e7dd32f6d5ff5c9f3dc31d7144e84a665 Bruno Rodrigues <bruno@brodrigues.co> 1763489136 +0100	commit: correct imports
2242e50e7dd32f6d5ff5c9f3dc31d7144e84a665 898a18f37effc11ed52d9e8a4f38b81dc40072f3 Bruno Rodrigues <bruno@brodrigues.co> 1763490421 +0100	pull origin add_up: Fast-forward
898a18f37effc11ed52d9e8a4f38b81dc40072f3 3d2ef2db0f3d466af3fea410be1022f5b6afd3d8 Bruno Rodrigues <bruno@brodrigues.co> 1763490719 +0100	commit: fix
3d2ef2db0f3d466af3fea410be1022f5b6afd3d8 0cf386dee7bb5fd0aa29129d33ac13c6f0f714fc Bruno Rodrigues <bruno@brodrigues.co> 1763537255 +0100	pull origin add_up: Fast-forward
0cf386dee7bb5fd0aa29129d33ac13c6f0f714fc 81f3e2dc6728d17c6070abd21615e20de1aa6878 Bruno Rodrigues <bruno@brodrigues.co> 1763537982 +0100	commit: added antigravity
81f3e2dc6728d17c6070abd21615e20de1aa6878 eca59425a8e26485c8e5453867e05d69cd7e6ad1 Bruno Rodrigues <bruno@brodrigues.co> 1763538458 +0100	commit: added necessary env var for running authentication test
eca59425a8e26485c8e5453867e05d69cd7e6ad1 e51cda6098bfa913099bb7ebe7a077ddea47cb59 Bruno Rodrigues <bruno@brodrigues.co> 1763538660 +0100	commit: don't build antigravity in CI
e51cda6098bfa913099bb7ebe7a077ddea47cb59 8bd5b29e67884367c451ef9a4c9e06dee27cf2a0 Bruno Rodrigues <bruno@brodrigues.co> 1763539036 +0100	commit: fix backend
8bd5b29e67884367c451ef9a4c9e06dee27cf2a0 20dc8882105c9bf332464857a098fafd44111dec Bruno Rodrigues <bruno@brodrigues.co> 1763539437 +0100	commit: adding debug to auth
20dc8882105c9bf332464857a098fafd44111dec 9529562a1d12c5e4e93d904bfaee67ab4c00b722 Bruno Rodrigues <bruno@brodrigues.co> 1763539794 +0100	commit: shared across clones
9529562a1d12c5e4e93d904bfaee67ab4c00b722 0a7dde7b56198df6b14997af113a7c925daff7ea Bruno Rodrigues <bruno@brodrigues.co> 1763540555 +0100	commit: removed unneeded debug logs
0a7dde7b56198df6b14997af113a7c925daff7ea a0aebc616e3819819b99a769dba5dc4123086698 Bruno Rodrigues <bruno@brodrigues.co> 1763540568 +0100	commit: added noenv_flake_path env var
a0aebc616e3819819b99a769dba5dc4123086698 c7d1a2274872b3d8844be09d54a03cb93999a7e4 Bruno Rodrigues <bruno@brodrigues.co> 1763541196 +0100	checkout: moving from add_up to main
c7d1a2274872b3d8844be09d54a03cb93999a7e4 ce2bbff1c7eeb6e811182cf7259493783f6ee082 Bruno Rodrigues <bruno@brodrigues.co> 1763541201 +0100	pull origin main: Fast-forward
ce2bbff1c7eeb6e811182cf7259493783f6ee082 ce2bbff1c7eeb6e811182cf7259493783f6ee082 Bruno Rodrigues <bruno@brodrigues.co> 1763541214 +0100	checkout: moving from main to test_up
ce2bbff1c7eeb6e811182cf7259493783f6ee082 68e873ffee37a10e5dfde17281b6d427c59831f1 Bruno Rodrigues <bruno@brodrigues.co> 1763541533 +0100	commit: tests for compute providers
68e873ffee37a10e5dfde17281b6d427c59831f1 cad49e5d463b9aa58a678b5ea7393e3c90317532 Bruno Rodrigues <bruno@brodrigues.co> 1763545703 +0100	commit: unit tests
cad49e5d463b9aa58a678b5ea7393e3c90317532 b9ce90da3a79f50a138f6f1eaabcf201a6805ba0 Bruno Rodrigues <bruno@brodrigues.co> 1763546262 +0100	commit: dealing with warnings
b9ce90da3a79f50a138f6f1eaabcf201a6805ba0 599cfa905574db0298811b4befccb313b51c1460 Bruno Rodrigues <bruno@brodrigues.co> 1763546696 +0100	commit: refactor
599cfa905574db0298811b4befccb313b51c1460 16f1aed7dfcb81f8697296174a05e1e768953411 Bruno Rodrigues <bruno@brodrigues.co> 1763547413 +0100	commit: fixed tests
16f1aed7dfcb81f8697296174a05e1e768953411 04dacf1795e10936bb38c0b7d4a5dcf9d5861149 Bruno Rodrigues <bruno@brodrigues.co> 1763548007 +0100	commit: moar tests
04dacf1795e10936bb38c0b7d4a5dcf9d5861149 ce2bbff1c7eeb6e811182cf7259493783f6ee082 Bruno Rodrigues <bruno@brodrigues.co> 1763577919 +0100	checkout: moving from test_up to main
ce2bbff1c7eeb6e811182cf7259493783f6ee082 ee7dc0b690e38343c231cacd92e4df83efd0bbdd Bruno Rodrigues <bruno@brodrigues.co> 1763577923 +0100	pull origin main: Fast-forward
ee7dc0b690e38343c231cacd92e4df83efd0bbdd ee7dc0b690e38343c231cacd92e4df83efd0bbdd Bruno Rodrigues <bruno@brodrigues.co> 1763578173 +0100	checkout: moving from main to pull_push_git
ee7dc0b690e38343c231cacd92e4df83efd0bbdd a598c72fbb6a57f49a1a39558177c7286688e2e4 Bruno Rodrigues <bruno@brodrigues.co> 1763580815 +0100	commit: trying to get authenticated push pull
a598c72fbb6a57f49a1a39558177c7286688e2e4 614599d7e366889a5e7c4d56e81a9404662d5a1c Bruno Rodrigues <bruno@brodrigues.co> 1763624794 +0100	commit: fixed tests
614599d7e366889a5e7c4d56e81a9404662d5a1c 696af594191efddfb3e3cd88bb5c55bba545af8e Bruno Rodrigues <bruno@brodrigues.co> 1763650425 +0100	commit: up almost working
696af594191efddfb3e3cd88bb5c55bba545af8e 710c2fd38161c433c431982a4c27712294f8025b Bruno Rodrigues <bruno@brodrigues.co> 1763651198 +0100	commit: almost there
710c2fd38161c433c431982a4c27712294f8025b 6c3781173f761a36d5c763aeb620c5d70acb14ad Bruno Rodrigues <bruno@brodrigues.co> 1763713977 +0100	commit: updated to latest upterm
6c3781173f761a36d5c763aeb620c5d70acb14ad b76fe6fb5be8562ac52d99a5cd23933086599905 Bruno Rodrigues <bruno@brodrigues.co> 1763718061 +0100	commit: ssh connection working!
b76fe6fb5be8562ac52d99a5cd23933086599905 b76fe6fb5be8562ac52d99a5cd23933086599905 Bruno Rodrigues <bruno@brodrigues.co> 1763724565 +0100	reset: moving to HEAD
b76fe6fb5be8562ac52d99a5cd23933086599905 b76fe6fb5be8562ac52d99a5cd23933086599905 Bruno Rodrigues <bruno@brodrigues.co> 1763746958 +0100	reset: moving to HEAD
b76fe6fb5be8562ac52d99a5cd23933086599905 5a9096a6dd928bfa6a2e0ae63c8d1e04f56b9ec1 Bruno Rodrigues <bruno@brodrigues.co> 1763755672 +0100	commit: --env=noenv working!
5a9096a6dd928bfa6a2e0ae63c8d1e04f56b9ec1 480f44b0d039c2c4e575185ee1386a04d84c703a Bruno Rodrigues <bruno@brodrigues.co> 1763760180 +0100	commit: local flake being used working
480f44b0d039c2c4e575185ee1386a04d84c703a 27a603a68b084414f15b19c2684198a47e7d0520 Bruno Rodrigues <bruno@brodrigues.co> 1763761532 +0100	commit: --env flag is now more comprehensive
27a603a68b084414f15b19c2684198a47e7d0520 dfc455e18edfc4eb8e2f8c2e98e60bcbf8aac042 Bruno Rodrigues <bruno@brodrigues.co> 1763809621 +0100	commit: race condition keys
dfc455e18edfc4eb8e2f8c2e98e60bcbf8aac042 90f724a379c91f09fcbfba8dc62cb9d7d86ef3b7 Bruno Rodrigues <bruno@brodrigues.co> 1763810013 +0100	commit: fix test on mac
90f724a379c91f09fcbfba8dc62cb9d7d86ef3b7 ee7dc0b690e38343c231cacd92e4df83efd0bbdd Bruno Rodrigues <bruno@brodrigues.co> 1763811319 +0100	checkout: moving from pull_push_git to main
ee7dc0b690e38343c231cacd92e4df83efd0bbdd 0a3d7c33ca1599a7b2332dffebf5634dad9fd152 Bruno Rodrigues <bruno@brodrigues.co> 1763811323 +0100	pull origin main: Fast-forward
0a3d7c33ca1599a7b2332dffebf5634dad9fd152 00402d6f3b0f76d9c1d082a992c24a2e65d11742 Bruno Rodrigues <bruno@brodrigues.co> 1763811718 +0100	commit: updated antigravity
00402d6f3b0f76d9c1d082a992c24a2e65d11742 00402d6f3b0f76d9c1d082a992c24a2e65d11742 Bruno Rodrigues <bruno@brodrigues.co> 1763812518 +0100	checkout: moving from main to fix_unwrap
00402d6f3b0f76d9c1d082a992c24a2e65d11742 e07171e0a3b50ef52746aac1ffbd9e444b9cb1fb Bruno Rodrigues <bruno@brodrigues.co> 1763812992 +0100	commit: unit tests pass, now for some smoke testing
e07171e0a3b50ef52746aac1ffbd9e444b9cb1fb 00402d6f3b0f76d9c1d082a992c24a2e65d11742 Bruno Rodrigues <bruno@brodrigues.co> 1763823967 +0100	checkout: moving from fix_unwrap to main
00402d6f3b0f76d9c1d082a992c24a2e65d11742 3f7acb6f9edc50d7a5d1bc8d8a062e844684cb94 Bruno Rodrigues <bruno@brodrigues.co> 1763823970 +0100	pull origin main: Fast-forward
3f7acb6f9edc50d7a5d1bc8d8a062e844684cb94 550e848ac1da87da30084069a5b9952f01ff5893 Bruno Rodrigues <bruno@brodrigues.co> 1763824474 +0100	commit: action to build steadystate
550e848ac1da87da30084069a5b9952f01ff5893 ea427b91f742685a699bf5e1da3893f19c1b50d5 Bruno Rodrigues <bruno@brodrigues.co> 1763825406 +0100	commit: actions now depend on build
ea427b91f742685a699bf5e1da3893f19c1b50d5 1e65b50f8658998207b5379cc2328eaa3eececcd Bruno Rodrigues <bruno@brodrigues.co> 1763825676 +0100	commit: disable unit tests during build in CI; they're executed separaetly afterwards
1e65b50f8658998207b5379cc2328eaa3eececcd e07557ad39eac59e21d449aa15decda32652be12 Bruno Rodrigues <bruno@brodrigues.co> 1763826549 +0100	commit: better flake
e07557ad39eac59e21d449aa15decda32652be12 e07557ad39eac59e21d449aa15decda32652be12 Bruno Rodrigues <bruno@brodrigues.co> 1763826569 +0100	checkout: moving from main to modes
e07557ad39eac59e21d449aa15decda32652be12 672c63c0702aa0cc0f6428f5fe49814a769ef34c Bruno Rodrigues <bruno@brodrigues.co> 1763829148 +0100	commit: --mode flag
672c63c0702aa0cc0f6428f5fe49814a769ef34c b5c5751900717a5dada0251740e55b00a0c37a18 Bruno Rodrigues <bruno@brodrigues.co> 1763829396 +0100	commit: darwin fixes for the flake
b5c5751900717a5dada0251740e55b00a0c37a18 e07557ad39eac59e21d449aa15decda32652be12 Bruno Rodrigues <bruno@brodrigues.co> 1763830006 +0100	checkout: moving from modes to main
e07557ad39eac59e21d449aa15decda32652be12 7178ce21c83e6c696d6c2ed593e8917d232d99de Bruno Rodrigues <bruno@brodrigues.co> 1763830010 +0100	pull origin main: Fast-forward
7178ce21c83e6c696d6c2ed593e8917d232d99de 7178ce21c83e6c696d6c2ed593e8917d232d99de Bruno Rodrigues <bruno@brodrigues.co> 1763830691 +0100	checkout: moving from main to collab
7178ce21c83e6c696d6c2ed593e8917d232d99de 8f182beaeb33052a7aa10e0839db336048362256 Bruno Rodrigues <bruno@brodrigues.co> 1763832416 +0100	commit: started collab mode
8f182beaeb33052a7aa10e0839db336048362256 df5c6f8693c06f813e80a8b4cf49151f303398b6 Bruno Rodrigues <bruno@brodrigues.co> 1763846787 +0100	commit: sessions get spawned
df5c6f8693c06f813e80a8b4cf49151f303398b6 69b342316da84a4febe7dfc54e991d5f268d08ae Bruno Rodrigues <bruno@brodrigues.co> 1763910040 +0100	commit: project with setting up collab mode
69b342316da84a4febe7dfc54e991d5f268d08ae 9aaf637a5c6ce98547b45fd6458520775094e935 Bruno Rodrigues <bruno@brodrigues.co> 1763911312 +0100	commit: dashboard starting
9aaf637a5c6ce98547b45fd6458520775094e935 dfdd500af6b6b8c1c795bd924ba26086e4173300 Bruno Rodrigues <bruno@brodrigues.co> 1763911895 +0100	commit: magic link under session in dashboard
dfdd500af6b6b8c1c795bd924ba26086e4173300 4211695ea6f55925c78517b897027fec0929d04d Bruno Rodrigues <bruno@brodrigues.co> 1763912420 +0100	commit: steadystate join with magic link collab mode working
4211695ea6f55925c78517b897027fec0929d04d c4a809a8c70dccc219e33a58c4c09e312b74c665 Bruno Rodrigues <bruno@brodrigues.co> 1763914281 +0100	commit: trying to debug empty workspace
c4a809a8c70dccc219e33a58c4c09e312b74c665 c4a809a8c70dccc219e33a58c4c09e312b74c665 Bruno Rodrigues <bruno@brodrigues.co> 1764164220 +0100	reset: moving to HEAD
c4a809a8c70dccc219e33a58c4c09e312b74c665 eb52d0f72508104b3f7b46ab919b70ac5eca58eb Bruno Rodrigues <bruno@brodrigues.co> 1764166977 +0100	commit: pijul -> yjs
eb52d0f72508104b3f7b46ab919b70ac5eca58eb 85049d9c3b47b38f62a774d1dbc5f4d6561bd85a Bruno Rodrigues <bruno@brodrigues.co> 1764167898 +0100	commit: listen to 0.0.0.0
85049d9c3b47b38f62a774d1dbc5f4d6561bd85a 8a02cad7aee9b86f112cb9e388c612061146ce99 Bruno Rodrigues <bruno@brodrigues.co> 1764168245 +0100	commit: fixed magic link
8a02cad7aee9b86f112cb9e388c612061146ce99 5218589e96fd963f2972ba7f4ff23b7876b6f916 Bruno Rodrigues <bruno@brodrigues.co> 1764168608 +0100	commit: iproute2 to flake
5218589e96fd963f2972ba7f4ff23b7876b6f916 35c49324a4358cbec68d75677b80a0fe4e08debf Bruno Rodrigues <bruno@brodrigues.co> 1764169583 +0100	commit: magic link fixed
35c49324a4358cbec68d75677b80a0fe4e08debf ec94d2475d721cc88ad52108abd09647955c857b Bruno Rodrigues <bruno@brodrigues.co> 1764173504 +0100	commit: allow collaborators by default
ec94d2475d721cc88ad52108abd09647955c857b 01bcb701f9b7a6d61aad9875e64495f1bb865c38 Bruno Rodrigues <bruno@brodrigues.co> 1764180900 +0100	commit: debug
01bcb701f9b7a6d61aad9875e64495f1bb865c38 82aace0abbf967db376ff4c1549367afc2a411aa Bruno Rodrigues <bruno@brodrigues.co> 1764185307 +0100	commit: locally stored jwt
82aace0abbf967db376ff4c1549367afc2a411aa 6637b1a70268fc699358ef02387555e3d9d095e7 Bruno Rodrigues <bruno@brodrigues.co> 1764186807 +0100	commit: trying
6637b1a70268fc699358ef02387555e3d9d095e7 ac7af8278a8e9886a1d5648be29ef1476da78c41 Bruno Rodrigues <bruno@brodrigues.co> 1764188299 +0100	commit: working!
ac7af8278a8e9886a1d5648be29ef1476da78c41 7178ce21c83e6c696d6c2ed593e8917d232d99de Bruno Rodrigues <bruno@brodrigues.co> 1764188310 +0100	checkout: moving from collab to main
7178ce21c83e6c696d6c2ed593e8917d232d99de 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764188348 +0100	pull origin main: Fast-forward
563310eb9cb48816332855e8465b0a7b2597ef3c 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764188445 +0100	checkout: moving from main to fix_shared_codebase
563310eb9cb48816332855e8465b0a7b2597ef3c 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764189662 +0100	reset: moving to HEAD
563310eb9cb48816332855e8465b0a7b2597ef3c b7b934b79d6bf7813b25a50e83663a208a5a2646 Bruno Rodrigues <bruno@brodrigues.co> 1764190603 +0100	commit: documentation
b7b934b79d6bf7813b25a50e83663a208a5a2646 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764191342 +0100	checkout: moving from fix_shared_codebase to main
563310eb9cb48816332855e8465b0a7b2597ef3c 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764191347 +0100	checkout: moving from main to major_refactor
563310eb9cb48816332855e8465b0a7b2597ef3c 325dbdf0e62671d581b74c266613fb5afade0772 Bruno Rodrigues <bruno@brodrigues.co> 1764230729 +0100	commit: tests
325dbdf0e62671d581b74c266613fb5afade0772 a37f2a37585603a9953702437128470f37db05b5 Bruno Rodrigues <bruno@brodrigues.co> 1764234314 +0100	commit: major refactor
a37f2a37585603a9953702437128470f37db05b5 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764235013 +0100	checkout: moving from major_refactor to main
563310eb9cb48816332855e8465b0a7b2597ef3c 811d61f04fb7c1f8fbe4075366503c36b1fa9aac Bruno Rodrigues <bruno@brodrigues.co> 1764235026 +0100	commit: only include iproute2 on linux
811d61f04fb7c1f8fbe4075366503c36b1fa9aac f30012ab9198720e57f3b55e9a25f78d0c4b58f2 Bruno Rodrigues <bruno@brodrigues.co> 1764235045 +0100	rebase (start): checkout refs/remotes/origin/main
f30012ab9198720e57f3b55e9a25f78d0c4b58f2 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764235045 +0100	rebase (pick): only include iproute2 on linux
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764235045 +0100	rebase (finish): returning to refs/heads/main
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764240513 +0100	checkout: moving from main to fix_sync
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa 1bf2fe8a743e68948cf4eb91182da5d4de6e39fb Bruno Rodrigues <bruno@brodrigues.co> 1764240678 +0100	commit: fix first commit issue
1bf2fe8a743e68948cf4eb91182da5d4de6e39fb 8c536063bfdc4e40bb2c9dd05583ea434a130619 Bruno Rodrigues <bruno@brodrigues.co> 1764241452 +0100	commit: not bare
8c536063bfdc4e40bb2c9dd05583ea434a130619 8c536063bfdc4e40bb2c9dd05583ea434a130619 Bruno Rodrigues <bruno@brodrigues.co> 1764314475 +0100	reset: moving to HEAD
8c536063bfdc4e40bb2c9dd05583ea434a130619 8c536063bfdc4e40bb2c9dd05583ea434a130619 Bruno Rodrigues <bruno@brodrigues.co> 1764315904 +0100	reset: moving to HEAD
8c536063bfdc4e40bb2c9dd05583ea434a130619 c675da5bd94b9e6ea17a32da59258ae7ff2f710a Bruno Rodrigues <bruno@brodrigues.co> 1764320240 +0100	commit: keyring ifxes
c675da5bd94b9e6ea17a32da59258ae7ff2f710a 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764320242 +0100	checkout: moving from fix_sync to main
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa c675da5bd94b9e6ea17a32da59258ae7ff2f710a Bruno Rodrigues <bruno@brodrigues.co> 1764320318 +0100	checkout: moving from main to fix_sync
c675da5bd94b9e6ea17a32da59258ae7ff2f710a ea110492d575826155feb41832d7611ba262101c Bruno Rodrigues <bruno@brodrigues.co> 1764334102 +0100	commit: testing syncing
ea110492d575826155feb41832d7611ba262101c a894b02b370bfe49adcadd5ef86411efac6df7e2 Bruno Rodrigues <bruno@brodrigues.co> 1764334837 +0100	commit: better logs
a894b02b370bfe49adcadd5ef86411efac6df7e2 02f31382a662042e752f872c5d53e89480689c67 Bruno Rodrigues <bruno@brodrigues.co> 1764335562 +0100	commit: better detection of binary files
02f31382a662042e752f872c5d53e89480689c67 5c70ba38386c391979e5b0f47e7f457783ba37c5 Bruno Rodrigues <bruno@brodrigues.co> 1764335937 +0100	commit: fixing sync
5c70ba38386c391979e5b0f47e7f457783ba37c5 461d4d383c917accadd315d7fa7107f8c0b94f6c Bruno Rodrigues <bruno@brodrigues.co> 1764336234 +0100	commit: fixing compilation
461d4d383c917accadd315d7fa7107f8c0b94f6c 17824c872f5637cd3a04513b54b620ae6a578199 Bruno Rodrigues <bruno@brodrigues.co> 1764337913 +0100	commit: fix welcome messages
17824c872f5637cd3a04513b54b620ae6a578199 b035d1ee534872f41d0075d17a81cce331050ab1 Bruno Rodrigues <bruno@brodrigues.co> 1764338286 +0100	commit: fixed welcome message
b035d1ee534872f41d0075d17a81cce331050ab1 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764338893 +0100	checkout: moving from fix_sync to main
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa 8479b1a57cd47dd360353a249c6a49de54b56c99 Bruno Rodrigues <bruno@brodrigues.co> 1764338897 +0100	pull origin main: Fast-forward

########## ./.git/logs/refs/heads/main

0000000000000000000000000000000000000000 223372f5543ac5281c11af1a5872000722f1af56 Bruno Rodrigues <bruno@brodrigues.co> 1762527849 +0100	clone: from github.com:The-Exact-Computing-Company/steadystate-cli.git
223372f5543ac5281c11af1a5872000722f1af56 09a82bc67e69f7690866226ed594a8ca92e7038f Bruno Rodrigues <bruno@brodrigues.co> 1762528493 +0100	commit: first commit
09a82bc67e69f7690866226ed594a8ca92e7038f d89e69360f3cb1029063bc16ef54314da5131589 Bruno Rodrigues <bruno@brodrigues.co> 1762531376 +0100	commit: compiles
d89e69360f3cb1029063bc16ef54314da5131589 69bdb5ebd2103e7788c048cc2593e146afca0759 Bruno Rodrigues <bruno@brodrigues.co> 1762701963 +0100	pull origin main: Fast-forward
69bdb5ebd2103e7788c048cc2593e146afca0759 db7333e1de953ba1c763bf5dd5806f108ebe27de Bruno Rodrigues <bruno@brodrigues.co> 1762703013 +0100	commit: updated dep
db7333e1de953ba1c763bf5dd5806f108ebe27de 936dd51a02cd50a5ee674bcf583ef97d9f304de0 Bruno Rodrigues <bruno@brodrigues.co> 1762703429 +0100	commit: CI: unit tests
936dd51a02cd50a5ee674bcf583ef97d9f304de0 73ee36fe9f01f4d5e6505d6a2f63a2839666668e Bruno Rodrigues <bruno@brodrigues.co> 1762706141 +0100	commit: more tests
73ee36fe9f01f4d5e6505d6a2f63a2839666668e 3440dec281dbabebf18cd2db75a107d9ab523d4b Bruno Rodrigues <bruno@brodrigues.co> 1762848232 +0100	pull origin main: Fast-forward
3440dec281dbabebf18cd2db75a107d9ab523d4b a024e486499bd3a013381aec430f94472488af6f Bruno Rodrigues <bruno@brodrigues.co> 1763026361 +0100	pull origin main: Fast-forward
a024e486499bd3a013381aec430f94472488af6f 015587558f53820b270a45b0f91bf34d1d890971 Bruno Rodrigues <bruno@brodrigues.co> 1763027147 +0100	commit: readmes
015587558f53820b270a45b0f91bf34d1d890971 a6e5bdd05f95ee56d097edf29ce1f193f20e2191 Bruno Rodrigues <bruno@brodrigues.co> 1763194458 +0100	pull origin main: Fast-forward
a6e5bdd05f95ee56d097edf29ce1f193f20e2191 05b8b8b37c39eb2ce9bd53a510a128ec4a21cb0c Bruno Rodrigues <bruno@brodrigues.co> 1763194684 +0100	commit: licenses
05b8b8b37c39eb2ce9bd53a510a128ec4a21cb0c 18ae3af6cb568fd98e29a37b9447f21818c2dd89 Bruno Rodrigues <bruno@brodrigues.co> 1763199507 +0100	commit: device flow woring, but polling doesn't return jwt
18ae3af6cb568fd98e29a37b9447f21818c2dd89 b981ed1eddcb189816d23387c77bca78c7dfc2bd Bruno Rodrigues <bruno@brodrigues.co> 1763200672 +0100	commit: polling now works
b981ed1eddcb189816d23387c77bca78c7dfc2bd 1761805aefde4b3a1f59c7e79634533438681dd4 Bruno Rodrigues <bruno@brodrigues.co> 1763217429 +0100	pull origin main: Fast-forward
1761805aefde4b3a1f59c7e79634533438681dd4 e927c84251f915ff853b4be76c66185029fe838d Bruno Rodrigues <bruno@brodrigues.co> 1763217593 +0100	commit: build env in a separate step
e927c84251f915ff853b4be76c66185029fe838d 6c7b61fb9b4b788009a9f3b95b0143ed79426074 Bruno Rodrigues <bruno@brodrigues.co> 1763316545 +0100	pull origin main: Fast-forward
6c7b61fb9b4b788009a9f3b95b0143ed79426074 4db52a1c1ef801d84b775ff49edb945bc07577bf Bruno Rodrigues <bruno@brodrigues.co> 1763317065 +0100	commit: treemerge is now locked
4db52a1c1ef801d84b775ff49edb945bc07577bf ed6405e59ec04d7f830c66cdccf36ffed404f0c3 Bruno Rodrigues <bruno@brodrigues.co> 1763451950 +0100	pull origin main: Fast-forward
ed6405e59ec04d7f830c66cdccf36ffed404f0c3 0916f0e7a3938ccabfb0d4e299219059cce29cbe Bruno Rodrigues <bruno@brodrigues.co> 1763452871 +0100	commit: treemerge from cache
0916f0e7a3938ccabfb0d4e299219059cce29cbe c7d1a2274872b3d8844be09d54a03cb93999a7e4 Bruno Rodrigues <bruno@brodrigues.co> 1763454441 +0100	commit: github login flow working again
c7d1a2274872b3d8844be09d54a03cb93999a7e4 ce2bbff1c7eeb6e811182cf7259493783f6ee082 Bruno Rodrigues <bruno@brodrigues.co> 1763541201 +0100	pull origin main: Fast-forward
ce2bbff1c7eeb6e811182cf7259493783f6ee082 ee7dc0b690e38343c231cacd92e4df83efd0bbdd Bruno Rodrigues <bruno@brodrigues.co> 1763577923 +0100	pull origin main: Fast-forward
ee7dc0b690e38343c231cacd92e4df83efd0bbdd 0a3d7c33ca1599a7b2332dffebf5634dad9fd152 Bruno Rodrigues <bruno@brodrigues.co> 1763811323 +0100	pull origin main: Fast-forward
0a3d7c33ca1599a7b2332dffebf5634dad9fd152 00402d6f3b0f76d9c1d082a992c24a2e65d11742 Bruno Rodrigues <bruno@brodrigues.co> 1763811718 +0100	commit: updated antigravity
00402d6f3b0f76d9c1d082a992c24a2e65d11742 3f7acb6f9edc50d7a5d1bc8d8a062e844684cb94 Bruno Rodrigues <bruno@brodrigues.co> 1763823970 +0100	pull origin main: Fast-forward
3f7acb6f9edc50d7a5d1bc8d8a062e844684cb94 550e848ac1da87da30084069a5b9952f01ff5893 Bruno Rodrigues <bruno@brodrigues.co> 1763824474 +0100	commit: action to build steadystate
550e848ac1da87da30084069a5b9952f01ff5893 ea427b91f742685a699bf5e1da3893f19c1b50d5 Bruno Rodrigues <bruno@brodrigues.co> 1763825406 +0100	commit: actions now depend on build
ea427b91f742685a699bf5e1da3893f19c1b50d5 1e65b50f8658998207b5379cc2328eaa3eececcd Bruno Rodrigues <bruno@brodrigues.co> 1763825676 +0100	commit: disable unit tests during build in CI; they're executed separaetly afterwards
1e65b50f8658998207b5379cc2328eaa3eececcd e07557ad39eac59e21d449aa15decda32652be12 Bruno Rodrigues <bruno@brodrigues.co> 1763826549 +0100	commit: better flake
e07557ad39eac59e21d449aa15decda32652be12 7178ce21c83e6c696d6c2ed593e8917d232d99de Bruno Rodrigues <bruno@brodrigues.co> 1763830010 +0100	pull origin main: Fast-forward
7178ce21c83e6c696d6c2ed593e8917d232d99de 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764188348 +0100	pull origin main: Fast-forward
563310eb9cb48816332855e8465b0a7b2597ef3c 811d61f04fb7c1f8fbe4075366503c36b1fa9aac Bruno Rodrigues <bruno@brodrigues.co> 1764235026 +0100	commit: only include iproute2 on linux
811d61f04fb7c1f8fbe4075366503c36b1fa9aac 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764235045 +0100	rebase (finish): refs/heads/main onto f30012ab9198720e57f3b55e9a25f78d0c4b58f2
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa 8479b1a57cd47dd360353a249c6a49de54b56c99 Bruno Rodrigues <bruno@brodrigues.co> 1764338897 +0100	pull origin main: Fast-forward

########## ./.git/logs/refs/heads/pull_push_git

0000000000000000000000000000000000000000 ee7dc0b690e38343c231cacd92e4df83efd0bbdd Bruno Rodrigues <bruno@brodrigues.co> 1763578173 +0100	branch: Created from HEAD
ee7dc0b690e38343c231cacd92e4df83efd0bbdd a598c72fbb6a57f49a1a39558177c7286688e2e4 Bruno Rodrigues <bruno@brodrigues.co> 1763580815 +0100	commit: trying to get authenticated push pull
a598c72fbb6a57f49a1a39558177c7286688e2e4 614599d7e366889a5e7c4d56e81a9404662d5a1c Bruno Rodrigues <bruno@brodrigues.co> 1763624794 +0100	commit: fixed tests
614599d7e366889a5e7c4d56e81a9404662d5a1c 696af594191efddfb3e3cd88bb5c55bba545af8e Bruno Rodrigues <bruno@brodrigues.co> 1763650425 +0100	commit: up almost working
696af594191efddfb3e3cd88bb5c55bba545af8e 710c2fd38161c433c431982a4c27712294f8025b Bruno Rodrigues <bruno@brodrigues.co> 1763651198 +0100	commit: almost there
710c2fd38161c433c431982a4c27712294f8025b 6c3781173f761a36d5c763aeb620c5d70acb14ad Bruno Rodrigues <bruno@brodrigues.co> 1763713977 +0100	commit: updated to latest upterm
6c3781173f761a36d5c763aeb620c5d70acb14ad b76fe6fb5be8562ac52d99a5cd23933086599905 Bruno Rodrigues <bruno@brodrigues.co> 1763718061 +0100	commit: ssh connection working!
b76fe6fb5be8562ac52d99a5cd23933086599905 5a9096a6dd928bfa6a2e0ae63c8d1e04f56b9ec1 Bruno Rodrigues <bruno@brodrigues.co> 1763755672 +0100	commit: --env=noenv working!
5a9096a6dd928bfa6a2e0ae63c8d1e04f56b9ec1 480f44b0d039c2c4e575185ee1386a04d84c703a Bruno Rodrigues <bruno@brodrigues.co> 1763760180 +0100	commit: local flake being used working
480f44b0d039c2c4e575185ee1386a04d84c703a 27a603a68b084414f15b19c2684198a47e7d0520 Bruno Rodrigues <bruno@brodrigues.co> 1763761532 +0100	commit: --env flag is now more comprehensive
27a603a68b084414f15b19c2684198a47e7d0520 dfc455e18edfc4eb8e2f8c2e98e60bcbf8aac042 Bruno Rodrigues <bruno@brodrigues.co> 1763809621 +0100	commit: race condition keys
dfc455e18edfc4eb8e2f8c2e98e60bcbf8aac042 90f724a379c91f09fcbfba8dc62cb9d7d86ef3b7 Bruno Rodrigues <bruno@brodrigues.co> 1763810013 +0100	commit: fix test on mac

########## ./.git/logs/refs/heads/fix_sync

0000000000000000000000000000000000000000 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764240513 +0100	branch: Created from HEAD
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa 1bf2fe8a743e68948cf4eb91182da5d4de6e39fb Bruno Rodrigues <bruno@brodrigues.co> 1764240678 +0100	commit: fix first commit issue
1bf2fe8a743e68948cf4eb91182da5d4de6e39fb 8c536063bfdc4e40bb2c9dd05583ea434a130619 Bruno Rodrigues <bruno@brodrigues.co> 1764241452 +0100	commit: not bare
8c536063bfdc4e40bb2c9dd05583ea434a130619 c675da5bd94b9e6ea17a32da59258ae7ff2f710a Bruno Rodrigues <bruno@brodrigues.co> 1764320240 +0100	commit: keyring ifxes
c675da5bd94b9e6ea17a32da59258ae7ff2f710a ea110492d575826155feb41832d7611ba262101c Bruno Rodrigues <bruno@brodrigues.co> 1764334102 +0100	commit: testing syncing
ea110492d575826155feb41832d7611ba262101c a894b02b370bfe49adcadd5ef86411efac6df7e2 Bruno Rodrigues <bruno@brodrigues.co> 1764334837 +0100	commit: better logs
a894b02b370bfe49adcadd5ef86411efac6df7e2 02f31382a662042e752f872c5d53e89480689c67 Bruno Rodrigues <bruno@brodrigues.co> 1764335562 +0100	commit: better detection of binary files
02f31382a662042e752f872c5d53e89480689c67 5c70ba38386c391979e5b0f47e7f457783ba37c5 Bruno Rodrigues <bruno@brodrigues.co> 1764335937 +0100	commit: fixing sync
5c70ba38386c391979e5b0f47e7f457783ba37c5 461d4d383c917accadd315d7fa7107f8c0b94f6c Bruno Rodrigues <bruno@brodrigues.co> 1764336234 +0100	commit: fixing compilation
461d4d383c917accadd315d7fa7107f8c0b94f6c 17824c872f5637cd3a04513b54b620ae6a578199 Bruno Rodrigues <bruno@brodrigues.co> 1764337913 +0100	commit: fix welcome messages
17824c872f5637cd3a04513b54b620ae6a578199 b035d1ee534872f41d0075d17a81cce331050ab1 Bruno Rodrigues <bruno@brodrigues.co> 1764338286 +0100	commit: fixed welcome message

########## ./.git/logs/refs/heads/major_refactor

0000000000000000000000000000000000000000 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764191347 +0100	branch: Created from HEAD
563310eb9cb48816332855e8465b0a7b2597ef3c 325dbdf0e62671d581b74c266613fb5afade0772 Bruno Rodrigues <bruno@brodrigues.co> 1764230729 +0100	commit: tests
325dbdf0e62671d581b74c266613fb5afade0772 a37f2a37585603a9953702437128470f37db05b5 Bruno Rodrigues <bruno@brodrigues.co> 1764234314 +0100	commit: major refactor

########## ./.git/logs/refs/heads/fix_shared_codebase

0000000000000000000000000000000000000000 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764188445 +0100	branch: Created from HEAD
563310eb9cb48816332855e8465b0a7b2597ef3c b7b934b79d6bf7813b25a50e83663a208a5a2646 Bruno Rodrigues <bruno@brodrigues.co> 1764190603 +0100	commit: documentation

########## ./.git/logs/refs/heads/modes

0000000000000000000000000000000000000000 e07557ad39eac59e21d449aa15decda32652be12 Bruno Rodrigues <bruno@brodrigues.co> 1763826569 +0100	branch: Created from HEAD
e07557ad39eac59e21d449aa15decda32652be12 672c63c0702aa0cc0f6428f5fe49814a769ef34c Bruno Rodrigues <bruno@brodrigues.co> 1763829148 +0100	commit: --mode flag
672c63c0702aa0cc0f6428f5fe49814a769ef34c b5c5751900717a5dada0251740e55b00a0c37a18 Bruno Rodrigues <bruno@brodrigues.co> 1763829396 +0100	commit: darwin fixes for the flake

########## ./.git/logs/refs/heads/fix_unwrap

0000000000000000000000000000000000000000 00402d6f3b0f76d9c1d082a992c24a2e65d11742 Bruno Rodrigues <bruno@brodrigues.co> 1763812518 +0100	branch: Created from HEAD
00402d6f3b0f76d9c1d082a992c24a2e65d11742 e07171e0a3b50ef52746aac1ffbd9e444b9cb1fb Bruno Rodrigues <bruno@brodrigues.co> 1763812992 +0100	commit: unit tests pass, now for some smoke testing

########## ./.git/logs/refs/heads/collab

0000000000000000000000000000000000000000 7178ce21c83e6c696d6c2ed593e8917d232d99de Bruno Rodrigues <bruno@brodrigues.co> 1763830691 +0100	branch: Created from HEAD
7178ce21c83e6c696d6c2ed593e8917d232d99de 8f182beaeb33052a7aa10e0839db336048362256 Bruno Rodrigues <bruno@brodrigues.co> 1763832416 +0100	commit: started collab mode
8f182beaeb33052a7aa10e0839db336048362256 df5c6f8693c06f813e80a8b4cf49151f303398b6 Bruno Rodrigues <bruno@brodrigues.co> 1763846787 +0100	commit: sessions get spawned
df5c6f8693c06f813e80a8b4cf49151f303398b6 69b342316da84a4febe7dfc54e991d5f268d08ae Bruno Rodrigues <bruno@brodrigues.co> 1763910040 +0100	commit: project with setting up collab mode
69b342316da84a4febe7dfc54e991d5f268d08ae 9aaf637a5c6ce98547b45fd6458520775094e935 Bruno Rodrigues <bruno@brodrigues.co> 1763911312 +0100	commit: dashboard starting
9aaf637a5c6ce98547b45fd6458520775094e935 dfdd500af6b6b8c1c795bd924ba26086e4173300 Bruno Rodrigues <bruno@brodrigues.co> 1763911895 +0100	commit: magic link under session in dashboard
dfdd500af6b6b8c1c795bd924ba26086e4173300 4211695ea6f55925c78517b897027fec0929d04d Bruno Rodrigues <bruno@brodrigues.co> 1763912420 +0100	commit: steadystate join with magic link collab mode working
4211695ea6f55925c78517b897027fec0929d04d c4a809a8c70dccc219e33a58c4c09e312b74c665 Bruno Rodrigues <bruno@brodrigues.co> 1763914281 +0100	commit: trying to debug empty workspace
c4a809a8c70dccc219e33a58c4c09e312b74c665 eb52d0f72508104b3f7b46ab919b70ac5eca58eb Bruno Rodrigues <bruno@brodrigues.co> 1764166977 +0100	commit: pijul -> yjs
eb52d0f72508104b3f7b46ab919b70ac5eca58eb 85049d9c3b47b38f62a774d1dbc5f4d6561bd85a Bruno Rodrigues <bruno@brodrigues.co> 1764167898 +0100	commit: listen to 0.0.0.0
85049d9c3b47b38f62a774d1dbc5f4d6561bd85a 8a02cad7aee9b86f112cb9e388c612061146ce99 Bruno Rodrigues <bruno@brodrigues.co> 1764168245 +0100	commit: fixed magic link
8a02cad7aee9b86f112cb9e388c612061146ce99 5218589e96fd963f2972ba7f4ff23b7876b6f916 Bruno Rodrigues <bruno@brodrigues.co> 1764168608 +0100	commit: iproute2 to flake
5218589e96fd963f2972ba7f4ff23b7876b6f916 35c49324a4358cbec68d75677b80a0fe4e08debf Bruno Rodrigues <bruno@brodrigues.co> 1764169583 +0100	commit: magic link fixed
35c49324a4358cbec68d75677b80a0fe4e08debf ec94d2475d721cc88ad52108abd09647955c857b Bruno Rodrigues <bruno@brodrigues.co> 1764173504 +0100	commit: allow collaborators by default
ec94d2475d721cc88ad52108abd09647955c857b 01bcb701f9b7a6d61aad9875e64495f1bb865c38 Bruno Rodrigues <bruno@brodrigues.co> 1764180900 +0100	commit: debug
01bcb701f9b7a6d61aad9875e64495f1bb865c38 82aace0abbf967db376ff4c1549367afc2a411aa Bruno Rodrigues <bruno@brodrigues.co> 1764185307 +0100	commit: locally stored jwt
82aace0abbf967db376ff4c1549367afc2a411aa 6637b1a70268fc699358ef02387555e3d9d095e7 Bruno Rodrigues <bruno@brodrigues.co> 1764186807 +0100	commit: trying
6637b1a70268fc699358ef02387555e3d9d095e7 ac7af8278a8e9886a1d5648be29ef1476da78c41 Bruno Rodrigues <bruno@brodrigues.co> 1764188299 +0100	commit: working!

########## ./.git/logs/refs/heads/test_up

0000000000000000000000000000000000000000 ce2bbff1c7eeb6e811182cf7259493783f6ee082 Bruno Rodrigues <bruno@brodrigues.co> 1763541214 +0100	branch: Created from HEAD
ce2bbff1c7eeb6e811182cf7259493783f6ee082 68e873ffee37a10e5dfde17281b6d427c59831f1 Bruno Rodrigues <bruno@brodrigues.co> 1763541533 +0100	commit: tests for compute providers
68e873ffee37a10e5dfde17281b6d427c59831f1 cad49e5d463b9aa58a678b5ea7393e3c90317532 Bruno Rodrigues <bruno@brodrigues.co> 1763545703 +0100	commit: unit tests
cad49e5d463b9aa58a678b5ea7393e3c90317532 b9ce90da3a79f50a138f6f1eaabcf201a6805ba0 Bruno Rodrigues <bruno@brodrigues.co> 1763546262 +0100	commit: dealing with warnings
b9ce90da3a79f50a138f6f1eaabcf201a6805ba0 599cfa905574db0298811b4befccb313b51c1460 Bruno Rodrigues <bruno@brodrigues.co> 1763546696 +0100	commit: refactor
599cfa905574db0298811b4befccb313b51c1460 16f1aed7dfcb81f8697296174a05e1e768953411 Bruno Rodrigues <bruno@brodrigues.co> 1763547413 +0100	commit: fixed tests
16f1aed7dfcb81f8697296174a05e1e768953411 04dacf1795e10936bb38c0b7d4a5dcf9d5861149 Bruno Rodrigues <bruno@brodrigues.co> 1763548007 +0100	commit: moar tests

########## ./.git/logs/refs/heads/fake_provider

0000000000000000000000000000000000000000 e927c84251f915ff853b4be76c66185029fe838d Bruno Rodrigues <bruno@brodrigues.co> 1763218960 +0100	branch: Created from HEAD
e927c84251f915ff853b4be76c66185029fe838d e3c59b074abd6b69e68cf71c81b4e12b324788a9 Bruno Rodrigues <bruno@brodrigues.co> 1763316537 +0100	commit: experiment but certainly for trash

########## ./.git/logs/refs/stash

0000000000000000000000000000000000000000 64e3d9374968c0b9065bf8f0696d18934e3da75c Bruno Rodrigues <bruno@brodrigues.co> 1763724565 +0100	WIP on pull_push_git: b76fe6f ssh connection working!
64e3d9374968c0b9065bf8f0696d18934e3da75c 6e723640450d93232acf984c9521cb0f0f99b0f0 Bruno Rodrigues <bruno@brodrigues.co> 1763746958 +0100	WIP on pull_push_git: b76fe6f ssh connection working!
6e723640450d93232acf984c9521cb0f0f99b0f0 6ab5a5a144e4c45af1137660ee15737c8c50d40d Bruno Rodrigues <bruno@brodrigues.co> 1764164220 +0100	WIP on collab: c4a809a trying to debug empty workspace
6ab5a5a144e4c45af1137660ee15737c8c50d40d 30da6036a7c2437f040b3cd5e4b0209211b3f3c8 Bruno Rodrigues <bruno@brodrigues.co> 1764189662 +0100	WIP on fix_shared_codebase: 563310e Merge pull request #22 from The-Exact-Computing-Company/collab

########## ./.git/logs/refs/remotes/origin/main

223372f5543ac5281c11af1a5872000722f1af56 09a82bc67e69f7690866226ed594a8ca92e7038f Bruno Rodrigues <bruno@brodrigues.co> 1762528497 +0100	update by push
09a82bc67e69f7690866226ed594a8ca92e7038f d89e69360f3cb1029063bc16ef54314da5131589 Bruno Rodrigues <bruno@brodrigues.co> 1762531379 +0100	update by push
d89e69360f3cb1029063bc16ef54314da5131589 69bdb5ebd2103e7788c048cc2593e146afca0759 Bruno Rodrigues <bruno@brodrigues.co> 1762701963 +0100	pull origin main: fast-forward
69bdb5ebd2103e7788c048cc2593e146afca0759 db7333e1de953ba1c763bf5dd5806f108ebe27de Bruno Rodrigues <bruno@brodrigues.co> 1762703016 +0100	update by push
db7333e1de953ba1c763bf5dd5806f108ebe27de 936dd51a02cd50a5ee674bcf583ef97d9f304de0 Bruno Rodrigues <bruno@brodrigues.co> 1762703432 +0100	update by push
936dd51a02cd50a5ee674bcf583ef97d9f304de0 73ee36fe9f01f4d5e6505d6a2f63a2839666668e Bruno Rodrigues <bruno@brodrigues.co> 1762706146 +0100	update by push
73ee36fe9f01f4d5e6505d6a2f63a2839666668e fcb2aeb7f8977dc01cdf6791470a8d01a0b49227 Bruno Rodrigues <bruno@brodrigues.co> 1762790434 +0100	fetch: fast-forward
fcb2aeb7f8977dc01cdf6791470a8d01a0b49227 3440dec281dbabebf18cd2db75a107d9ab523d4b Bruno Rodrigues <bruno@brodrigues.co> 1762848198 +0100	fetch: fast-forward
3440dec281dbabebf18cd2db75a107d9ab523d4b a024e486499bd3a013381aec430f94472488af6f Bruno Rodrigues <bruno@brodrigues.co> 1763026361 +0100	pull origin main: fast-forward
a024e486499bd3a013381aec430f94472488af6f 015587558f53820b270a45b0f91bf34d1d890971 Bruno Rodrigues <bruno@brodrigues.co> 1763027150 +0100	update by push
015587558f53820b270a45b0f91bf34d1d890971 a6e5bdd05f95ee56d097edf29ce1f193f20e2191 Bruno Rodrigues <bruno@brodrigues.co> 1763194458 +0100	pull origin main: fast-forward
a6e5bdd05f95ee56d097edf29ce1f193f20e2191 05b8b8b37c39eb2ce9bd53a510a128ec4a21cb0c Bruno Rodrigues <bruno@brodrigues.co> 1763194691 +0100	update by push
05b8b8b37c39eb2ce9bd53a510a128ec4a21cb0c 18ae3af6cb568fd98e29a37b9447f21818c2dd89 Bruno Rodrigues <bruno@brodrigues.co> 1763199512 +0100	update by push
18ae3af6cb568fd98e29a37b9447f21818c2dd89 b981ed1eddcb189816d23387c77bca78c7dfc2bd Bruno Rodrigues <bruno@brodrigues.co> 1763200675 +0100	update by push
b981ed1eddcb189816d23387c77bca78c7dfc2bd 1761805aefde4b3a1f59c7e79634533438681dd4 Bruno Rodrigues <bruno@brodrigues.co> 1763217429 +0100	pull origin main: fast-forward
1761805aefde4b3a1f59c7e79634533438681dd4 e927c84251f915ff853b4be76c66185029fe838d Bruno Rodrigues <bruno@brodrigues.co> 1763217597 +0100	update by push
e927c84251f915ff853b4be76c66185029fe838d 6c7b61fb9b4b788009a9f3b95b0143ed79426074 Bruno Rodrigues <bruno@brodrigues.co> 1763316545 +0100	pull origin main: fast-forward
6c7b61fb9b4b788009a9f3b95b0143ed79426074 4db52a1c1ef801d84b775ff49edb945bc07577bf Bruno Rodrigues <bruno@brodrigues.co> 1763317069 +0100	update by push
4db52a1c1ef801d84b775ff49edb945bc07577bf ed6405e59ec04d7f830c66cdccf36ffed404f0c3 Bruno Rodrigues <bruno@brodrigues.co> 1763451942 +0100	fetch: fast-forward
ed6405e59ec04d7f830c66cdccf36ffed404f0c3 0916f0e7a3938ccabfb0d4e299219059cce29cbe Bruno Rodrigues <bruno@brodrigues.co> 1763452874 +0100	update by push
0916f0e7a3938ccabfb0d4e299219059cce29cbe c7d1a2274872b3d8844be09d54a03cb93999a7e4 Bruno Rodrigues <bruno@brodrigues.co> 1763454445 +0100	update by push
c7d1a2274872b3d8844be09d54a03cb93999a7e4 ce2bbff1c7eeb6e811182cf7259493783f6ee082 Bruno Rodrigues <bruno@brodrigues.co> 1763541201 +0100	pull origin main: fast-forward
ce2bbff1c7eeb6e811182cf7259493783f6ee082 ee7dc0b690e38343c231cacd92e4df83efd0bbdd Bruno Rodrigues <bruno@brodrigues.co> 1763577923 +0100	pull origin main: fast-forward
ee7dc0b690e38343c231cacd92e4df83efd0bbdd 0a3d7c33ca1599a7b2332dffebf5634dad9fd152 Bruno Rodrigues <bruno@brodrigues.co> 1763811323 +0100	pull origin main: fast-forward
0a3d7c33ca1599a7b2332dffebf5634dad9fd152 00402d6f3b0f76d9c1d082a992c24a2e65d11742 Bruno Rodrigues <bruno@brodrigues.co> 1763811721 +0100	update by push
00402d6f3b0f76d9c1d082a992c24a2e65d11742 3f7acb6f9edc50d7a5d1bc8d8a062e844684cb94 Bruno Rodrigues <bruno@brodrigues.co> 1763823970 +0100	pull origin main: fast-forward
3f7acb6f9edc50d7a5d1bc8d8a062e844684cb94 550e848ac1da87da30084069a5b9952f01ff5893 Bruno Rodrigues <bruno@brodrigues.co> 1763824478 +0100	update by push
550e848ac1da87da30084069a5b9952f01ff5893 ea427b91f742685a699bf5e1da3893f19c1b50d5 Bruno Rodrigues <bruno@brodrigues.co> 1763825410 +0100	update by push
ea427b91f742685a699bf5e1da3893f19c1b50d5 1e65b50f8658998207b5379cc2328eaa3eececcd Bruno Rodrigues <bruno@brodrigues.co> 1763825679 +0100	update by push
1e65b50f8658998207b5379cc2328eaa3eececcd e07557ad39eac59e21d449aa15decda32652be12 Bruno Rodrigues <bruno@brodrigues.co> 1763826552 +0100	update by push
e07557ad39eac59e21d449aa15decda32652be12 7178ce21c83e6c696d6c2ed593e8917d232d99de Bruno Rodrigues <bruno@brodrigues.co> 1763830010 +0100	pull origin main: fast-forward
7178ce21c83e6c696d6c2ed593e8917d232d99de 563310eb9cb48816332855e8465b0a7b2597ef3c Bruno Rodrigues <bruno@brodrigues.co> 1764188348 +0100	pull origin main: fast-forward
563310eb9cb48816332855e8465b0a7b2597ef3c f30012ab9198720e57f3b55e9a25f78d0c4b58f2 Bruno Rodrigues <bruno@brodrigues.co> 1764235042 +0100	fetch: fast-forward
f30012ab9198720e57f3b55e9a25f78d0c4b58f2 0598e20fdf0fa92b5d9a41f815a0abd753ee20fa Bruno Rodrigues <bruno@brodrigues.co> 1764235054 +0100	update by push
0598e20fdf0fa92b5d9a41f815a0abd753ee20fa 8479b1a57cd47dd360353a249c6a49de54b56c99 Bruno Rodrigues <bruno@brodrigues.co> 1764338897 +0100	pull origin main: fast-forward

########## ./.git/logs/refs/remotes/origin/add_up

0000000000000000000000000000000000000000 6ff7aaeb391d953205a6e40b99fde45fcd65d3c7 Bruno Rodrigues <bruno@brodrigues.co> 1763458622 +0100	update by push
6ff7aaeb391d953205a6e40b99fde45fcd65d3c7 8593c07bacbf6f74faf7ee236091f8e7a92647b9 Bruno Rodrigues <bruno@brodrigues.co> 1763458653 +0100	update by push
8593c07bacbf6f74faf7ee236091f8e7a92647b9 95342c02bf19cddaa0866c637ca608d28a769bb8 Bruno Rodrigues <bruno@brodrigues.co> 1763459392 +0100	update by push
95342c02bf19cddaa0866c637ca608d28a769bb8 0764afffbe5305487f400fc4bea544b1fc484fc2 Bruno Rodrigues <bruno@brodrigues.co> 1763461386 +0100	update by push
0764afffbe5305487f400fc4bea544b1fc484fc2 d3d05f4083ca1ff804964e2df4d69002573d7a25 Bruno Rodrigues <bruno@brodrigues.co> 1763464986 +0100	update by push
d3d05f4083ca1ff804964e2df4d69002573d7a25 887263e1e2e863ecb68a113879ca668a6fd6086f Bruno Rodrigues <bruno@brodrigues.co> 1763467059 +0100	update by push
887263e1e2e863ecb68a113879ca668a6fd6086f 98ba9ffc6b3a4fa03ee381a6f8bb367c6f96ee6a Bruno Rodrigues <bruno@brodrigues.co> 1763473237 +0100	pull origin add_up: fast-forward
98ba9ffc6b3a4fa03ee381a6f8bb367c6f96ee6a 93355e87ecc9ff277070fc451dfc42fe39e96173 Bruno Rodrigues <bruno@brodrigues.co> 1763484505 +0100	pull origin add_up: fast-forward
93355e87ecc9ff277070fc451dfc42fe39e96173 903a56c82459d53cbb3030a2f986bb35d30b7b73 Bruno Rodrigues <bruno@brodrigues.co> 1763484826 +0100	pull origin add_up: fast-forward
903a56c82459d53cbb3030a2f986bb35d30b7b73 dfd5004e600c7a7e19742f2b38e219ab36d47278 Bruno Rodrigues <bruno@brodrigues.co> 1763488474 +0100	pull --rebase origin add_up: fast-forward
dfd5004e600c7a7e19742f2b38e219ab36d47278 fc16c2a65152d5613a3c3cd319c8fa07916047f9 Bruno Rodrigues <bruno@brodrigues.co> 1763488477 +0100	update by push
fc16c2a65152d5613a3c3cd319c8fa07916047f9 2242e50e7dd32f6d5ff5c9f3dc31d7144e84a665 Bruno Rodrigues <bruno@brodrigues.co> 1763489142 +0100	update by push
2242e50e7dd32f6d5ff5c9f3dc31d7144e84a665 898a18f37effc11ed52d9e8a4f38b81dc40072f3 Bruno Rodrigues <bruno@brodrigues.co> 1763490421 +0100	pull origin add_up: fast-forward
898a18f37effc11ed52d9e8a4f38b81dc40072f3 3d2ef2db0f3d466af3fea410be1022f5b6afd3d8 Bruno Rodrigues <bruno@brodrigues.co> 1763490723 +0100	update by push
3d2ef2db0f3d466af3fea410be1022f5b6afd3d8 0cf386dee7bb5fd0aa29129d33ac13c6f0f714fc Bruno Rodrigues <bruno@brodrigues.co> 1763537255 +0100	pull origin add_up: fast-forward
0cf386dee7bb5fd0aa29129d33ac13c6f0f714fc 81f3e2dc6728d17c6070abd21615e20de1aa6878 Bruno Rodrigues <bruno@brodrigues.co> 1763537987 +0100	update by push
81f3e2dc6728d17c6070abd21615e20de1aa6878 eca59425a8e26485c8e5453867e05d69cd7e6ad1 Bruno Rodrigues <bruno@brodrigues.co> 1763538462 +0100	update by push
eca59425a8e26485c8e5453867e05d69cd7e6ad1 e51cda6098bfa913099bb7ebe7a077ddea47cb59 Bruno Rodrigues <bruno@brodrigues.co> 1763538663 +0100	update by push
e51cda6098bfa913099bb7ebe7a077ddea47cb59 8bd5b29e67884367c451ef9a4c9e06dee27cf2a0 Bruno Rodrigues <bruno@brodrigues.co> 1763539042 +0100	update by push
8bd5b29e67884367c451ef9a4c9e06dee27cf2a0 20dc8882105c9bf332464857a098fafd44111dec Bruno Rodrigues <bruno@brodrigues.co> 1763539441 +0100	update by push
20dc8882105c9bf332464857a098fafd44111dec 9529562a1d12c5e4e93d904bfaee67ab4c00b722 Bruno Rodrigues <bruno@brodrigues.co> 1763539798 +0100	update by push
9529562a1d12c5e4e93d904bfaee67ab4c00b722 a0aebc616e3819819b99a769dba5dc4123086698 Bruno Rodrigues <bruno@brodrigues.co> 1763540573 +0100	update by push

########## ./.git/logs/refs/remotes/origin/HEAD

0000000000000000000000000000000000000000 223372f5543ac5281c11af1a5872000722f1af56 Bruno Rodrigues <bruno@brodrigues.co> 1762527849 +0100	clone: from github.com:The-Exact-Computing-Company/steadystate-cli.git

########## ./.git/logs/refs/remotes/origin/pull_push_git

0000000000000000000000000000000000000000 a598c72fbb6a57f49a1a39558177c7286688e2e4 Bruno Rodrigues <bruno@brodrigues.co> 1763580821 +0100	update by push
a598c72fbb6a57f49a1a39558177c7286688e2e4 614599d7e366889a5e7c4d56e81a9404662d5a1c Bruno Rodrigues <bruno@brodrigues.co> 1763624800 +0100	update by push
614599d7e366889a5e7c4d56e81a9404662d5a1c 696af594191efddfb3e3cd88bb5c55bba545af8e Bruno Rodrigues <bruno@brodrigues.co> 1763650431 +0100	update by push
696af594191efddfb3e3cd88bb5c55bba545af8e 710c2fd38161c433c431982a4c27712294f8025b Bruno Rodrigues <bruno@brodrigues.co> 1763651203 +0100	update by push
710c2fd38161c433c431982a4c27712294f8025b 6c3781173f761a36d5c763aeb620c5d70acb14ad Bruno Rodrigues <bruno@brodrigues.co> 1763713983 +0100	update by push
6c3781173f761a36d5c763aeb620c5d70acb14ad b76fe6fb5be8562ac52d99a5cd23933086599905 Bruno Rodrigues <bruno@brodrigues.co> 1763718074 +0100	update by push
b76fe6fb5be8562ac52d99a5cd23933086599905 5a9096a6dd928bfa6a2e0ae63c8d1e04f56b9ec1 Bruno Rodrigues <bruno@brodrigues.co> 1763755677 +0100	update by push
5a9096a6dd928bfa6a2e0ae63c8d1e04f56b9ec1 480f44b0d039c2c4e575185ee1386a04d84c703a Bruno Rodrigues <bruno@brodrigues.co> 1763760187 +0100	update by push
480f44b0d039c2c4e575185ee1386a04d84c703a 27a603a68b084414f15b19c2684198a47e7d0520 Bruno Rodrigues <bruno@brodrigues.co> 1763761537 +0100	update by push
27a603a68b084414f15b19c2684198a47e7d0520 dfc455e18edfc4eb8e2f8c2e98e60bcbf8aac042 Bruno Rodrigues <bruno@brodrigues.co> 1763809630 +0100	update by push
dfc455e18edfc4eb8e2f8c2e98e60bcbf8aac042 90f724a379c91f09fcbfba8dc62cb9d7d86ef3b7 Bruno Rodrigues <bruno@brodrigues.co> 1763810021 +0100	update by push

########## ./.git/logs/refs/remotes/origin/b-rodrigues-patch-1

0000000000000000000000000000000000000000 74a6e45d075e23958b6fe9c1bee3d6b16dfe6fa1 Bruno Rodrigues <bruno@brodrigues.co> 1762790434 +0100	fetch: storing head
74a6e45d075e23958b6fe9c1bee3d6b16dfe6fa1 3e82efb5c54be22b25d8d521f3307da45414fb81 Bruno Rodrigues <bruno@brodrigues.co> 1762790553 +0100	update by push
3e82efb5c54be22b25d8d521f3307da45414fb81 c61e575e033d33edafb3fa9cef6ac154950e5c0b Bruno Rodrigues <bruno@brodrigues.co> 1762848198 +0100	fetch: fast-forward

########## ./.git/logs/refs/remotes/origin/more_tests

0000000000000000000000000000000000000000 69bddfac3c1020cec09f3ae37464aa49a2e62f13 Bruno Rodrigues <bruno@brodrigues.co> 1763451942 +0100	fetch: storing head

########## ./.git/logs/refs/remotes/origin/fix_sync

0000000000000000000000000000000000000000 1bf2fe8a743e68948cf4eb91182da5d4de6e39fb Bruno Rodrigues <bruno@brodrigues.co> 1764240684 +0100	update by push
1bf2fe8a743e68948cf4eb91182da5d4de6e39fb 8c536063bfdc4e40bb2c9dd05583ea434a130619 Bruno Rodrigues <bruno@brodrigues.co> 1764241458 +0100	update by push
8c536063bfdc4e40bb2c9dd05583ea434a130619 ea110492d575826155feb41832d7611ba262101c Bruno Rodrigues <bruno@brodrigues.co> 1764334106 +0100	update by push
ea110492d575826155feb41832d7611ba262101c a894b02b370bfe49adcadd5ef86411efac6df7e2 Bruno Rodrigues <bruno@brodrigues.co> 1764334843 +0100	update by push
a894b02b370bfe49adcadd5ef86411efac6df7e2 02f31382a662042e752f872c5d53e89480689c67 Bruno Rodrigues <bruno@brodrigues.co> 1764335566 +0100	update by push
02f31382a662042e752f872c5d53e89480689c67 5c70ba38386c391979e5b0f47e7f457783ba37c5 Bruno Rodrigues <bruno@brodrigues.co> 1764335942 +0100	update by push
5c70ba38386c391979e5b0f47e7f457783ba37c5 461d4d383c917accadd315d7fa7107f8c0b94f6c Bruno Rodrigues <bruno@brodrigues.co> 1764336238 +0100	update by push
461d4d383c917accadd315d7fa7107f8c0b94f6c 17824c872f5637cd3a04513b54b620ae6a578199 Bruno Rodrigues <bruno@brodrigues.co> 1764337920 +0100	update by push
17824c872f5637cd3a04513b54b620ae6a578199 b035d1ee534872f41d0075d17a81cce331050ab1 Bruno Rodrigues <bruno@brodrigues.co> 1764338293 +0100	update by push

########## ./.git/logs/refs/remotes/origin/major_refactor

0000000000000000000000000000000000000000 325dbdf0e62671d581b74c266613fb5afade0772 Bruno Rodrigues <bruno@brodrigues.co> 1764230735 +0100	update by push
325dbdf0e62671d581b74c266613fb5afade0772 a37f2a37585603a9953702437128470f37db05b5 Bruno Rodrigues <bruno@brodrigues.co> 1764234318 +0100	update by push

########## ./.git/logs/refs/remotes/origin/fix_shared_codebase

0000000000000000000000000000000000000000 b7b934b79d6bf7813b25a50e83663a208a5a2646 Bruno Rodrigues <bruno@brodrigues.co> 1764190611 +0100	update by push

########## ./.git/logs/refs/remotes/origin/modes

0000000000000000000000000000000000000000 672c63c0702aa0cc0f6428f5fe49814a769ef34c Bruno Rodrigues <bruno@brodrigues.co> 1763829153 +0100	update by push
672c63c0702aa0cc0f6428f5fe49814a769ef34c b5c5751900717a5dada0251740e55b00a0c37a18 Bruno Rodrigues <bruno@brodrigues.co> 1763829400 +0100	update by push

########## ./.git/logs/refs/remotes/origin/fix_unwrap

0000000000000000000000000000000000000000 e07171e0a3b50ef52746aac1ffbd9e444b9cb1fb Bruno Rodrigues <bruno@brodrigues.co> 1763813142 +0100	update by push

########## ./.git/logs/refs/remotes/origin/collab

0000000000000000000000000000000000000000 df5c6f8693c06f813e80a8b4cf49151f303398b6 Bruno Rodrigues <bruno@brodrigues.co> 1763846793 +0100	update by push
df5c6f8693c06f813e80a8b4cf49151f303398b6 69b342316da84a4febe7dfc54e991d5f268d08ae Bruno Rodrigues <bruno@brodrigues.co> 1763910045 +0100	update by push
69b342316da84a4febe7dfc54e991d5f268d08ae 4211695ea6f55925c78517b897027fec0929d04d Bruno Rodrigues <bruno@brodrigues.co> 1763912425 +0100	update by push
4211695ea6f55925c78517b897027fec0929d04d c4a809a8c70dccc219e33a58c4c09e312b74c665 Bruno Rodrigues <bruno@brodrigues.co> 1763914284 +0100	update by push
c4a809a8c70dccc219e33a58c4c09e312b74c665 eb52d0f72508104b3f7b46ab919b70ac5eca58eb Bruno Rodrigues <bruno@brodrigues.co> 1764167184 +0100	update by push
eb52d0f72508104b3f7b46ab919b70ac5eca58eb 85049d9c3b47b38f62a774d1dbc5f4d6561bd85a Bruno Rodrigues <bruno@brodrigues.co> 1764167904 +0100	update by push
85049d9c3b47b38f62a774d1dbc5f4d6561bd85a 8a02cad7aee9b86f112cb9e388c612061146ce99 Bruno Rodrigues <bruno@brodrigues.co> 1764168249 +0100	update by push
8a02cad7aee9b86f112cb9e388c612061146ce99 5218589e96fd963f2972ba7f4ff23b7876b6f916 Bruno Rodrigues <bruno@brodrigues.co> 1764168614 +0100	update by push
5218589e96fd963f2972ba7f4ff23b7876b6f916 35c49324a4358cbec68d75677b80a0fe4e08debf Bruno Rodrigues <bruno@brodrigues.co> 1764169587 +0100	update by push
35c49324a4358cbec68d75677b80a0fe4e08debf ec94d2475d721cc88ad52108abd09647955c857b Bruno Rodrigues <bruno@brodrigues.co> 1764173509 +0100	update by push
ec94d2475d721cc88ad52108abd09647955c857b 01bcb701f9b7a6d61aad9875e64495f1bb865c38 Bruno Rodrigues <bruno@brodrigues.co> 1764180904 +0100	update by push
01bcb701f9b7a6d61aad9875e64495f1bb865c38 82aace0abbf967db376ff4c1549367afc2a411aa Bruno Rodrigues <bruno@brodrigues.co> 1764185311 +0100	update by push
82aace0abbf967db376ff4c1549367afc2a411aa 6637b1a70268fc699358ef02387555e3d9d095e7 Bruno Rodrigues <bruno@brodrigues.co> 1764186813 +0100	update by push
6637b1a70268fc699358ef02387555e3d9d095e7 ac7af8278a8e9886a1d5648be29ef1476da78c41 Bruno Rodrigues <bruno@brodrigues.co> 1764188303 +0100	update by push

########## ./.git/logs/refs/remotes/origin/test_up

0000000000000000000000000000000000000000 68e873ffee37a10e5dfde17281b6d427c59831f1 Bruno Rodrigues <bruno@brodrigues.co> 1763541537 +0100	update by push
68e873ffee37a10e5dfde17281b6d427c59831f1 cad49e5d463b9aa58a678b5ea7393e3c90317532 Bruno Rodrigues <bruno@brodrigues.co> 1763545707 +0100	update by push
cad49e5d463b9aa58a678b5ea7393e3c90317532 b9ce90da3a79f50a138f6f1eaabcf201a6805ba0 Bruno Rodrigues <bruno@brodrigues.co> 1763546266 +0100	update by push
b9ce90da3a79f50a138f6f1eaabcf201a6805ba0 599cfa905574db0298811b4befccb313b51c1460 Bruno Rodrigues <bruno@brodrigues.co> 1763546700 +0100	update by push
599cfa905574db0298811b4befccb313b51c1460 16f1aed7dfcb81f8697296174a05e1e768953411 Bruno Rodrigues <bruno@brodrigues.co> 1763547417 +0100	update by push
16f1aed7dfcb81f8697296174a05e1e768953411 04dacf1795e10936bb38c0b7d4a5dcf9d5861149 Bruno Rodrigues <bruno@brodrigues.co> 1763548012 +0100	update by push

########## ./.git/logs/refs/remotes/origin/b-rodrigues-patch-2

0000000000000000000000000000000000000000 7c59b72ae422fd06a4f2bc7215563d33b6512f11 Bruno Rodrigues <bruno@brodrigues.co> 1762848198 +0100	fetch: storing head
7c59b72ae422fd06a4f2bc7215563d33b6512f11 7570585ac33ae161a2edeb4c03e64213c0004c4e Bruno Rodrigues <bruno@brodrigues.co> 1762848482 +0100	update by push
7570585ac33ae161a2edeb4c03e64213c0004c4e a21f12c87e395669e78036dbfd32f8ee56930eea Bruno Rodrigues <bruno@brodrigues.co> 1762848943 +0100	update by push
a21f12c87e395669e78036dbfd32f8ee56930eea 9ed3fc29cbb48f129ec057d0aca878317d59e357 Bruno Rodrigues <bruno@brodrigues.co> 1762849262 +0100	update by push
9ed3fc29cbb48f129ec057d0aca878317d59e357 1f39e81bfbb796424887836be3d36643ce6bf9e9 Bruno Rodrigues <bruno@brodrigues.co> 1762849557 +0100	update by push
1f39e81bfbb796424887836be3d36643ce6bf9e9 276a92e610467d2746ad93a36397f49740612f66 Bruno Rodrigues <bruno@brodrigues.co> 1762850256 +0100	update by push
276a92e610467d2746ad93a36397f49740612f66 22547fba67b7e78bad3e214141a534918d5f3a2c Bruno Rodrigues <bruno@brodrigues.co> 1762850702 +0100	update by push
22547fba67b7e78bad3e214141a534918d5f3a2c b7dbb5f7688cd5cdb0bde012cdb28b2de095231f Bruno Rodrigues <bruno@brodrigues.co> 1762851599 +0100	update by push
b7dbb5f7688cd5cdb0bde012cdb28b2de095231f 345daa90c92f3aba3dcafd311f931bd153a7ff99 Bruno Rodrigues <bruno@brodrigues.co> 1762852152 +0100	update by push
345daa90c92f3aba3dcafd311f931bd153a7ff99 54d59b33fb733e6660d0353415c6c3f9ce05bb52 Bruno Rodrigues <bruno@brodrigues.co> 1762852952 +0100	update by push
54d59b33fb733e6660d0353415c6c3f9ce05bb52 e2877dcb7f72f73fc7f8fb75efe00555e2586ba9 Bruno Rodrigues <bruno@brodrigues.co> 1762866603 +0100	update by push
e2877dcb7f72f73fc7f8fb75efe00555e2586ba9 78bbdd54be021c4575d91ed2297c10bc3504baa7 Bruno Rodrigues <bruno@brodrigues.co> 1762889989 +0100	pull origin b-rodrigues-patch-2: fast-forward

########## ./.git/logs/refs/remotes/origin/update_cli

0000000000000000000000000000000000000000 f03fa859252f66ca01942003435647de1857fbbf Bruno Rodrigues <bruno@brodrigues.co> 1763201971 +0100	update by push
f03fa859252f66ca01942003435647de1857fbbf cda55629dee71c105810b3b034e5da3e4f8156aa Bruno Rodrigues <bruno@brodrigues.co> 1763216190 +0100	update by push
cda55629dee71c105810b3b034e5da3e4f8156aa 9f9407feb6369cc0f6a6673a055699f94d85a4f1 Bruno Rodrigues <bruno@brodrigues.co> 1763216772 +0100	update by push
9f9407feb6369cc0f6a6673a055699f94d85a4f1 5ae6f37f656d1eca8d1b0961e3f77bb080eb40fc Bruno Rodrigues <bruno@brodrigues.co> 1763217140 +0100	update by push
5ae6f37f656d1eca8d1b0961e3f77bb080eb40fc 9084bf403618aa6f9704b1e4d1721cd6fb4ea530 Bruno Rodrigues <bruno@brodrigues.co> 1763217382 +0100	update by push

########## ./.git/ORIG_HEAD

0598e20fdf0fa92b5d9a41f815a0abd753ee20fa

########## ./.git/packed-refs

# pack-refs with: peeled fully-peeled sorted 
223372f5543ac5281c11af1a5872000722f1af56 refs/remotes/origin/main

########## ./.git/refs/heads/main

8479b1a57cd47dd360353a249c6a49de54b56c99

########## ./.git/refs/heads/pull_push_git

90f724a379c91f09fcbfba8dc62cb9d7d86ef3b7

########## ./.git/refs/heads/fix_sync

b035d1ee534872f41d0075d17a81cce331050ab1

########## ./.git/refs/heads/major_refactor

a37f2a37585603a9953702437128470f37db05b5

########## ./.git/refs/heads/fix_shared_codebase

b7b934b79d6bf7813b25a50e83663a208a5a2646

########## ./.git/refs/heads/modes

b5c5751900717a5dada0251740e55b00a0c37a18

########## ./.git/refs/heads/fix_unwrap

e07171e0a3b50ef52746aac1ffbd9e444b9cb1fb

########## ./.git/refs/heads/collab

ac7af8278a8e9886a1d5648be29ef1476da78c41

########## ./.git/refs/heads/test_up

04dacf1795e10936bb38c0b7d4a5dcf9d5861149

########## ./.git/refs/heads/fake_provider

e3c59b074abd6b69e68cf71c81b4e12b324788a9

########## ./.git/refs/stash

30da6036a7c2437f040b3cd5e4b0209211b3f3c8

########## ./.git/refs/remotes/origin/main

8479b1a57cd47dd360353a249c6a49de54b56c99

########## ./.git/refs/remotes/origin/add_up

a0aebc616e3819819b99a769dba5dc4123086698

########## ./.git/refs/remotes/origin/HEAD

ref: refs/remotes/origin/main

########## ./.git/refs/remotes/origin/pull_push_git

90f724a379c91f09fcbfba8dc62cb9d7d86ef3b7

########## ./.git/refs/remotes/origin/b-rodrigues-patch-1

c61e575e033d33edafb3fa9cef6ac154950e5c0b

########## ./.git/refs/remotes/origin/more_tests

69bddfac3c1020cec09f3ae37464aa49a2e62f13

########## ./.git/refs/remotes/origin/fix_sync

b035d1ee534872f41d0075d17a81cce331050ab1

########## ./.git/refs/remotes/origin/major_refactor

a37f2a37585603a9953702437128470f37db05b5

########## ./.git/refs/remotes/origin/fix_shared_codebase

b7b934b79d6bf7813b25a50e83663a208a5a2646

########## ./.git/refs/remotes/origin/modes

b5c5751900717a5dada0251740e55b00a0c37a18

########## ./.git/refs/remotes/origin/fix_unwrap

e07171e0a3b50ef52746aac1ffbd9e444b9cb1fb

########## ./.git/refs/remotes/origin/collab

ac7af8278a8e9886a1d5648be29ef1476da78c41

########## ./.git/refs/remotes/origin/test_up

04dacf1795e10936bb38c0b7d4a5dcf9d5861149

########## ./.git/refs/remotes/origin/b-rodrigues-patch-2

78bbdd54be021c4575d91ed2297c10bc3504baa7

########## ./.git/refs/remotes/origin/update_cli

9084bf403618aa6f9704b1e4d1721cd6fb4ea530

########## ./.git/description

Unnamed repository; edit this file 'description' to name the repository.

########## ./.git/FETCH_HEAD

8479b1a57cd47dd360353a249c6a49de54b56c99		branch 'main' of github.com:The-Exact-Computing-Company/steadystate

########## ./architecture.md

# Architecture

This document provides a high-level overview of the SteadyState monorepo architecture.

## Monorepo Structure

The repository is organized as a monorepo containing three primary components:

- **`backend/`**: A Rust-based backend service responsible for authentication, session management, and orchestration of development environments.
- **`cli/`**: A command-line interface (CLI) application, also written in Rust, that provides a user-facing interface for interacting with the backend.
- **`packages/`**: A directory containing shared crates and libraries used by other components in the monorepo.
  - **`common/`**: A shared crate with common data structures and utilities used by both the `backend` and `cli`.

## Backend Architecture

The `backend` is an Axum-based REST API service written in Rust. It handles user authentication, session orchestration, and compute resource management.

### Key Modules and Functions

#### `main.rs`
The entry point of the application. It sets up the Axum router, initializes the application state, applies middleware (CORS, tracing), and starts the HTTP server.

- **`main()`**: The asynchronous main function that initializes the `AppState` and starts the web server.

#### `state.rs`
Defines the `AppState`, which manages shared resources like the authentication provider registry and token storage. This state is shared across all request handlers.

- **`Config::from_env()`**: Creates a `Config` struct by reading settings from environment variables.
- **`AppState::try_new()`**: Asynchronously initializes the application state, including the HTTP client, JWT keys, compute providers, and auth provider factories.
- **`AppState::register_provider_factory()`**: Registers a new authentication provider factory, making it available for use.
- **`AppState::get_or_create_provider()`**: Retrieves a cached authentication provider or creates a new one using a registered factory. This allows for lazy initialization of providers.
- **`AppState::issue_refresh_token()`**: Generates, stores, and returns a new refresh token for a user.
- **`now()`**: A private helper function that returns the current Unix timestamp in seconds.

#### `jwt.rs`
Handles the creation (encoding) and validation of JSON Web Tokens (JWTs), which are used to secure the API endpoints.

- **`JwtKeys::new()`**: Constructs a new `JwtKeys` instance with a signing key, issuer, and token time-to-live (TTL).
- **`JwtKeys::sign()`**: Creates and signs a new JWT for a given user and provider.
- **`JwtKeys::verify()`**: Validates a JWT's signature and standard claims (like expiry and issuer).
- **`CustomClaims::from_request_parts()`**: An Axum extractor that validates the `Authorization: Bearer` token from request headers and extracts the custom claims.

#### `routes/auth.rs`
Defines the authentication-related API endpoints.

- **`router()`**: Creates and returns the Axum `Router` for all authentication routes (`/device`, `/poll`, `/refresh`, `/revoke`, `/me`).
- **`device_start()`**: The handler for `POST /auth/device`. It initiates the OAuth device flow for a specified provider.
- **`poll()`**: The handler for `POST /auth/poll`. It polls the backend to check if the user has completed the device flow authorization.
- **`refresh()`**: The handler for `POST /auth/refresh`. It exchanges a valid refresh token for a new JWT.
- **`revoke()`**: The handler for `POST /auth/revoke`. It revokes a refresh token, invalidating it for future use.
- **`me()`**: The handler for `GET /auth/me`. It returns the identity of the currently authenticated user based on their JWT.
- **`internal()`**: A private helper function that converts a displayable error into a `(StatusCode, String)` tuple for an HTTP 500 Internal Server Error response.

#### `routes/sessions.rs`
Defines endpoints for managing development sessions.

- **`router()`**: Creates and returns the Axum `Router` for all session-related routes (`/`, `/{id}`).
- **`create_session()`**: The handler for `POST /sessions`. It creates a new session record, returns an `ACCEPTED` status (including a `magic_link` for easy connection), and spawns a background task to provision the session.
- **`get_session_status()`**: The handler for `GET /sessions/{id}`. It returns the current status of a specific session.
- **`terminate_session()`**: The handler for `DELETE /sessions/{id}`. It initiates the termination of a session and returns an `ACCEPTED` status.
- **`run_provisioning()`**: A private background task that handles the logic for provisioning a new compute session using the appropriate `ComputeProvider`. It updates the session state based on the outcome (e.g., to `Running` or `Failed`).

#### `auth/provider.rs`
Defines the core traits for the modular authentication system.

- **`AuthProvider` (trait)**: An async trait defining the interface for an authentication provider, including methods for starting and polling the device flow.
- **`AuthProviderFactory` (trait)**: An async trait for factories that can build instances of `AuthProvider`.

#### `auth/github.rs`
An implementation of the `AuthProvider` trait for GitHub.

- **`GitHubAuth::new()`**: Creates a new instance of the GitHub authentication provider.
- **`GitHubAuth::start_device_flow()`**: Implements the device flow initiation logic by making a request to GitHub's device code endpoint.
- **`GitHubAuth::poll_device_flow()`**: Implements the polling logic by exchanging the device code for an access token and then fetching the user's identity.
- **`GitHubFactory::build()`**: Implements the factory pattern to build a `GitHubAuth` provider, reading the necessary client ID and secret from the application config.

#### `compute/local_provider.rs`
An implementation of the `ComputeProvider` trait that runs development sessions locally.

- **`LocalComputeProvider::new()`**: Creates a new instance of the local compute provider.
- **`LocalComputeProvider::start_session()`**: Implements the session startup logic, which involves creating a workspace, cloning the user's repository, and launching an `upterm` session within a Nix environment.
- **`LocalComputeProvider::terminate_session()`**: Implements the session termination logic by killing the `upterm` process and cleaning up the workspace directory.
- **`create_workspace()`**: A private method to create a temporary directory for the session's workspace.
- **`nix_shell_command()`**: A private helper to construct a `tokio::process::Command` that runs a given command inside a sourced Nix shell environment.
- **`ensure_nix_installed()`**: A private helper that checks if Nix is installed and, if not, installs it.
- **`clone_repo()`**: A private async function to clone a Git repository into a specified destination.
- **`launch_upterm_in_noenv()`**: A private async function that launches the `upterm` host process inside a `nix develop` shell.
- **`capture_upterm_invite()`**: A private async helper that reads the stdout of the `upterm` process to find and return the SSH invite link.
- **`kill_pid()`**: A private async function to terminate a process by its process ID (PID).

## Collaboration Architecture
 
 SteadyState introduces a "Shared Workspace" model (`--mode=collab`) to enable seamless asynchronous collaboration on the same compute instance.
 
 ### Shared Workspace Structure
 
 When a session is started in `collab` mode, the backend provisions a secure directory structure:
 
 ```
 ~/.steadystate/sessions/<session_id>/
 â”œâ”€â”€ canonical/          # Bare Git repository (synchronization point)
 â”œâ”€â”€ worktrees/          # Directory containing per-user worktrees
 â”‚   â”œâ”€â”€ user1/
 â”‚   â””â”€â”€ user2/
 â”œâ”€â”€ sshd/               # Dedicated SSH daemon configuration and keys
 â”œâ”€â”€ bin/                # Session-specific binaries (steadystate-cli, sync scripts)
 â”œâ”€â”€ activity-log        # Log of user actions (syncs, commits)
 â””â”€â”€ active-users        # List of currently connected users
 ```
 
 ### Session Initialization
 
 1.  **Canonical Repo**: The backend initializes a bare clone of the target repository in `canonical/`.
 2.  **Session Branch**: A dedicated branch `steadystate/collab/<session_id>` is created to isolate session work.
 3.  **SSHD Launch**: A custom `sshd` instance is launched on a random high port, configured to:
     *   Use a generated host key.
     *   Authenticate users via their GitHub public keys (fetched by the backend).
     *   Force all connections to execute a `wrapper.sh` script.
 
 ### Magic Links
 
 The backend generates a **Magic Link** (`steadystate://<mode>/<session_id>?ssh=...`) for every session. This link encodes:
 
 *   **Mode**: `pair` or `collab`.
 *   **Session ID**: The unique identifier for the session.
 *   **Connection Details**: The full SSH connection string (user, host, port) needed to join.
 
 The CLI's `steadystate join <url>` command parses this link to automatically configure the SSH connection.
 
 ### Connection & Isolation
 
 When a user connects via SSH (`ssh steady@host -p <port>`):
 
 1.  **Authentication**: `sshd` authenticates the user using their public key.
 2.  **Wrapper Script**: The `wrapper.sh` script is executed:
     *   Identifies the user based on the key used.
     *   Creates a private **Git Worktree** for the user in `worktrees/<user>/` if it doesn't exist.
     *   Configures Git identity (user.name, user.email) for that worktree.
     *   Sets `HOME` and `USER_WORKSPACE` environment variables to the worktree path.
     *   Drops the user into a shell (or executes the requested command) *inside* their worktree.
 
 This ensures that while users share the same compute resources (CPU, RAM, Nix store), their file system changes are isolated until they choose to sync.
 
 ### Synchronization Workflow
 
 Users synchronize their work using the `steadystate sync` command (injected into the session path):
 
 1.  **Commit**: Local changes in the user's worktree are automatically committed.
 2.  **Pull (Rebase)**: The user's branch is rebased on top of the `canonical` repository's HEAD. This pulls in changes from other collaborators.
 3.  **Push**: The user's updated branch is pushed back to the `canonical` repository.
 
 This "Commit-Rebase-Push" loop ensures a linear history and allows users to resolve conflicts locally if they arise during the rebase step.
 
 ## CLI Architecture

The `cli` is a command-line application built with `clap` that serves as the primary user interface for SteadyState. It communicates with the `backend` via a REST API.

### Key Modules and Functions

#### `main.rs`
The entry point for the CLI. It defines the command structure, parses arguments, and dispatches to the appropriate handler.

- **`main()`**: The asynchronous main function that sets up logging, parses CLI commands, creates an HTTP client, and executes the matched subcommand.
- **`whoami()`**: The handler for the `steadystate whoami` command. It reads the local session and prints the current user's login status.
- **`logout()`**: The handler for the `steadystate logout` command. It revokes the refresh token via an API call and deletes local session data.
- **`up()`**: The handler for the `steadystate up` command. It makes an authenticated request to the backend to create a new development session.

#### `auth.rs`
Contains the logic for handling user authentication and making authenticated API calls.

- **`device_login()`**: Orchestrates the entire OAuth device flow from the CLI side, including initiating the flow, polling for completion, and storing the resulting tokens.
- **`perform_refresh()`**: Proactively refreshes the JWT using the stored refresh token. It is called automatically when the JWT is near expiry.
- **`request_with_auth()`**: A generic helper function for making authenticated API requests. It handles reading the session, checking for JWT expiry, performing a refresh if needed, and then sending the request with the `Authorization: Bearer` header.
- **`extract_exp_from_jwt()`**: A utility function to parse a JWT (without verifying its signature) to extract its expiry timestamp.
- **`store_refresh_token()`**: Securely stores a refresh token in the operating system's keychain/keyring.
- **`get_refresh_token()`**: Retrieves a stored refresh token from the keychain.
- **`delete_refresh_token()`**: Deletes a stored refresh token from the keychain.
- **`send_with_retries()`**: A private helper function that wraps an HTTP request, providing a retry mechanism for transient network failures (like timeouts or connection errors).

#### `session.rs`
Manages the local user session file.

- **`Session::new()`**: Creates a new `Session` struct, automatically extracting the expiry timestamp from the provided JWT.
- **`Session::is_near_expiry()`**: Checks if the session's JWT has expired or will expire within a specified time buffer.
- **`get_cfg_dir()`**: Determines the appropriate configuration directory for storing session files, respecting the `STEADYSTATE_CONFIG_DIR` environment variable for overrides.
- **`session_file()`**: Constructs the full path to the `session.json` file.
- **`write_session()`**: Serializes a `Session` struct to JSON and writes it to the session file with secure file permissions (0600 on Unix).
- **`read_session()`**: Reads and deserializes the `Session` struct from the session file.
- **`remove_session()`**: Deletes the session file from the disk.

## Shared Code (`packages/common`)

The `packages/common` crate is intended to hold shared data structures, utilities, and business logic that is common to both the `backend` and `cli` applications. This helps to reduce code duplication and ensure consistency between the two components.

Currently, this crate is a placeholder and does not contain any shared code.

## CLI-Backend Interaction

The `cli` and `backend` communicate over a REST API. The `cli` acts as the client, making HTTP requests to the `backend` service to perform actions.

### Authentication Flow (`steadystate login`)

1.  **Device Flow Request**: The `cli` sends a `POST` request to the `backend`'s `/auth/device` endpoint.
2.  **User Authorization**: The `backend` responds with a `verification_uri` and `user_code`, which the `cli` displays to the user. The user authorizes the application in their browser.
3.  **Polling**: The `cli` polls the `backend`'s `/auth/poll` endpoint until the user completes authorization.
4.  **Token Issuance**: The `backend` returns a JWT and a refresh token.
5.  **Secure Storage**: The `cli` stores the refresh token in the system keychain and saves the JWT in a local session file.

### Authenticated API Calls (`steadystate up`)

1.  The `cli` reads the JWT from the session file.
2.  If the JWT is expired, the `cli` uses the refresh token to request a new one from the `/auth/refresh` endpoint.
3.  The `cli` sends the API request (e.g., to `/sessions`) with the valid JWT in the `Authorization: Bearer <token>` header.
4.  The `backend` validates the JWT and processes the request.

########## ./flake.nix

{
  description = "SteadyState dev environment";
  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-25.05";
    flake-utils.url = "github:numtide/flake-utils";
    treemerge.url = "github:b-rodrigues/treemerge";
    upterm-pkgs.url = "github:b-rodrigues/nixpkgs/update_upterm";
    antigravity-pkgs.url = "github:NixOS/nixpkgs/master";
  };
  outputs = { self, nixpkgs, flake-utils, treemerge, antigravity-pkgs, upterm-pkgs }:
    flake-utils.lib.eachDefaultSystem (system:
    let
      pkgs = import nixpkgs { inherit system; };
      isCI = builtins.getEnv "CI" == "true" || builtins.getEnv "CI" == "1";
      isDarwin = pkgs.stdenv.isDarwin;
      
      # Only import antigravity packages when NOT in CI
      agpkgs = if isCI then null else import antigravity-pkgs {
        inherit system;
        config.allowUnfree = true;
      };
      
      upkgs = import upterm-pkgs {inherit system;};
      antigravity = if isCI then null else agpkgs.antigravity;
      upterm = upkgs.upterm;
      workspaceSrc = pkgs.lib.cleanSource ./.;
      # Build entire Cargo workspace once
      workspaceDrv = pkgs.rustPlatform.buildRustPackage {
        pname = "steadystate-workspace";
        version = "0.0.1";
        src = workspaceSrc;
        cargoLock = {
          lockFile = ./Cargo.lock;
        };
        nativeBuildInputs = [ pkgs.pkg-config ];
        buildInputs = [ pkgs.openssl ];
        # Skip tests in CI builds - tests run in separate CI action
        # Skip tests on Darwin - they may have platform-specific issues
        # Run tests locally on Linux for fast feedback during development
        doCheck = !isCI && !isDarwin;
        OPENSSL_NO_VENDOR = 1;
      };
      # Extract individual binaries
      backend = pkgs.stdenv.mkDerivation {
        name = "steadystate-backend";
        buildCommand = ''
          mkdir -p $out/bin
          cp ${workspaceDrv}/bin/steadystate-backend $out/bin/
        '';
      };
      cli = pkgs.stdenv.mkDerivation {
        name = "steadystate";
        buildCommand = ''
          mkdir -p $out/bin
          cp ${workspaceDrv}/bin/steadystate $out/bin/
        '';
      };
      # Use kitty on Darwin, ghostty on Linux
      terminal = if isDarwin 
        then pkgs.lib.getExe' pkgs.kitty "kitty"
        else pkgs.lib.getExe' pkgs.ghostty "ghostty";
    in {
      packages.default = cli;
      packages.backend = backend;
      packages.cli = cli;
      apps.backend = flake-utils.lib.mkApp { drv = backend; };
      apps.cli = flake-utils.lib.mkApp { drv = cli; };
      devShells.default = pkgs.mkShell {
        name = "steadystate-dev";
        buildInputs = [
          pkgs.cargo
          pkgs.rustc
          pkgs.rustfmt
          pkgs.clippy
          pkgs.openssl.dev
          pkgs.pkg-config

          backend
          cli
          treemerge.packages.${system}.default
          upterm
        ] ++ pkgs.lib.optionals pkgs.stdenv.isLinux [ pkgs.iproute2 ]
          ++ pkgs.lib.optionals (!isCI) [ antigravity pkgs.gemini-cli ];
        shellHook = ''
          echo "ðŸ”§ Entering SteadyState dev shell"
          if [ -f backend/.env ]; then
            echo "ðŸ“¦ Loading backend/.env..."
            export $(grep -v '^#' backend/.env | xargs)
          else
            echo "âš ï¸ No backend/.env found"
          fi
          # IMPORTANT: CLI must talk HTTP, not HTTPS
          export STEADYSTATE_BACKEND=http://localhost:8080
          export NOENV_FLAKE_PATH=/tmp/dummy-flake
          echo "ðŸš€ Launching backend in new terminal"
          ${terminal} -e sh -c "${backend}/bin/steadystate-backend; exec bash" &
          echo ""
          echo "Use: steadystate login / steadystate up / steadystate whoami"
        '';
      };
    });
}

########## ./docs/ycrdt-merge-design.md

# Y-CRDT Merge Engine for SteadyState (Phase 2)

**Goal:**
SteadyState should perform merges using CRDT semantics (Yjs/Yrs), generating a conflict-free merged tree for the canonical Git repoâ€”no Pijul, no Git conflicts, no textual merging.

This is the core of the new architecture.

## 2.1 Core model: three trees

For each `steadystate sync`, you have:

### 1. canonical_tree

The Git tree at **HEAD** in the canonical repo.

### 2. local_tree

The userâ€™s ephemeral worktree as it currently exists on disk.

### 3. base_tree

The canonical tree at the moment the userâ€™s worktree was last synchronized.

`base_tree` is stored in:

```json
// .worktree/steadystate.json
{
  "last_synced_commit": "<hash>"
}
```

This gives us the required structure for a three-way merge:

```
merge(base_tree, local_tree, canonical_tree) â†’ merged_tree
```

We do *not* use a multi-file delta; instead we do per-file Yjs merges.

## 2.2 Representing each file as a Yjs document

For each file (text files only; skip binaries for now):

* Read:
  * `base_text` from `base_tree/file`
  * `local_text` from `local_tree/file`
  * `canonical_text` from `canonical_tree/file`

* Create a new Yjs/Yrs `YDoc`.

* Insert `base_text` into a `YText` inside the doc.

* Compute operations:

```
ops_local     = diff(base_text â†’ local_text)
ops_canonical = diff(base_text â†’ canonical_text)
```

Use any standard line-based or char-based diff algorithm (you already have one in the codebase; alternatively use `diffy`, `similar`, or `xi-unicode`).

* Apply ops to the YText sequentially:

```rust
let ydoc = yrs::Doc::new();
let ytext = ydoc.get_or_insert_text("content");

ytext.insert(0, &base_text);

// Apply local deltas
for op in ops_local { ytext.apply_delta(op); }

// Apply remote deltas
for op in ops_canonical { ytext.apply_delta(op); }
```

Yjs/Yrs guarantees:
* convergence,
* no conflicts,
* deterministic final state no matter the order of ops.

* Serialize merged text:

```rust
let merged_text = ytext.to_string();
```

* Write to `merged_tree/file`.

## 2.3 Managing file sets

You need a unified file list. Define:

```
files = union of:
    files(base_tree)
    âˆª files(local_tree)
    âˆª files(canonical_tree)
```

For each file:
* if exists in all three â†’ 3-way Yjs merge
* if deleted in local but present in canonical â†’ treat empty local as `""`
* if deleted in canonical but present in local â†’ treat empty canonical as `""`
* if created locally â†’ base = "", canonical = current
* if created in canonical â†’ base = "", local = current

Yjs handles these without conflict.

## 2.4 Materializing the three trees

### In canonical repo:

```bash
git show <base_commit>:<path>  # for base_tree
git show HEAD:<path>           # for canonical_tree
```

### In user worktree:

Just read from disk, skipping `.git/` and `.worktree/`.

### Temporary dirs:

```
/tmp/steadystate/base
/tmp/steadystate/local
/tmp/steadystate/remote
/tmp/steadystate/merged
```

## 2.5 Writing merged_tree back into canonical Git

After the Yjs/Yrs merge loop finishes:

1. Copy `merged_tree/*` into canonical worktree (overwriting).
2. In canonical repo:

```bash
git add -A
git commit -m "sync: SteadyState session $SESSION_ID by $USER"
git push origin HEAD
```

If `git diff --cached --quiet` is true â†’ skip commit.

## 2.6 Refresh the userâ€™s worktree

After canonical is updated:

```bash
git fetch origin
git reset --hard origin/HEAD
```

or simply remove and re-clone for simplicity and correctness.

## 2.7 Update metadata

Write the new canonical commit hash:

```
last_synced_commit = git rev-parse HEAD
```

and update `.worktree/steadystate.json`.

## 2.8 Modify CLI sync.rs

Replace the old Pijul logic with:

1. Identify paths (you already do this).
2. Materialize `base_tree`.
3. Materialize `local_tree`.
4. Materialize `canonical_tree`.
5. Do Yrs per-file merges.
6. Produce merged_tree.
7. Apply merged_tree to canonical Git repo.
8. Commit & push.
9. Reset user worktree.
10. Log to sync-log (existing behavior kept).

No Pijul commands remain.

## 2.9 Detailed Rust pseudocode for engineers

```rust
fn sync() -> Result<()> {
    // 1. Determine paths
    let root = find_repo_root()?;
    let canonical = root.join("canonical");
    let worktree = std::env::current_dir()?;
    let meta_path = worktree.join(".worktree/steadystate.json");

    // 2. Load base commit
    let base_commit = read_base_commit(&meta_path)
        .unwrap_or_else(|| current_canonical_head(&canonical));

    // 3. Materialize tree snapshots
    let base_tree     = materialize_tree(&canonical, base_commit)?;
    let canonical_tree = materialize_tree(&canonical, "HEAD")?;
    let local_tree     = materialize_local_tree(&worktree)?;

    // 4. Compute file union
    let files = union_files(&base_tree, &local_tree, &canonical_tree);

    // 5. Merge each file via Yrs
    let mut merged_tree = HashMap::new();
    for file in files {
        let base_text     = base_tree.get(&file).unwrap_or("").to_owned();
        let local_text    = local_tree.get(&file).unwrap_or("").to_owned();
        let canonical_text = canonical_tree.get(&file).unwrap_or("").to_owned();

        let merged = merge_file_yjs(&base_text, &local_text, &canonical_text)?;
        merged_tree.insert(file, merged);
    }

    // 6. Write merged result back into canonical repo
    write_merged_into_canonical(&canonical, &merged_tree)?;

    // 7. Commit + push
    git_commit_and_push(&canonical)?;

    // 8. Reset user worktree to canonical HEAD
    refresh_user_worktree(&worktree, &canonical)?;

    // 9. Update metadata
    write_base_commit(&meta_path, canonical_head(&canonical));

    // 10. Log
    append_sync_log(&worktree)?;

    Ok(())
}
```

## 2.10 CRDT invariants for engineers

### Yjs guarantees:
* no conflicts
* associativity
* commutativity
* idempotence
* deterministic convergence

Therefore:
* Diff quality does *not* affect correctness, only efficiency.
* Order of applying local and canonical ops doesnâ€™t matter.
* The system is robust to partial writes and replays.

## 2.11 No running Yjs server required

You are **not** building a real-time collaborative editor server.
Yjs/Yrs is used purely as a **merge engine**.

Meaning:
* No awareness of clients.
* No websockets.
* No provider.
* Just two deltas â†’ one merged document.

This keeps complexity low and makes sync deterministic.

########## ./Cargo.toml

[workspace]
members = [
    "cli",
    "backend",
    "packages/common"
]
resolver = "2"

########## ./backend/README.md

# SteadyState Backend

> Authentication and session orchestration service for reproducible cloud
> development environments.

---

## Overview

The **SteadyState Backend** powers the [SteadyState
CLI](https://github.com/steadystate-dev/steadystate) â€” a tool that lets
developers spin up **reproducible, on-demand development environments** defined
by Nix.

It provides secure authentication via OAuth (starting with **GitHub**, more
coming later) and manages **ephemeral SSH-accessible sessions** in the cloud.

---

## Features

* **GitHub OAuth (Device Flow)** authentication
* **Modular authentication provider architecture** (future: GitLab, Orchid, etc.)
* **JWT + Refresh token** issuing and verification
* **Axum-based REST API**, written in Rust
* **Nix-based dev environment** for reproducible builds
* **Session management skeleton** ready for Hetzner or other cloud backends

---

## Architecture

| Component       | Purpose                                                           |
| --------------- | ----------------------------------------------------------------- |
| `/auth/device`  | Start the OAuth device flow (returns verification URL + code)     |
| `/auth/poll`    | Poll until the user authorizes the device                         |
| `/auth/refresh` | Exchange a refresh token for a new JWT                            |
| `/auth/me`      | Return current user identity from JWT                             |
| `/sessions`     | (WIP) Create reproducible cloud dev environments                  |
| `providers/`    | Modular auth providers (`github.rs`, `gitlab.rs`, `orchid.rs`, â€¦) |
| `storage.rs`    | In-memory token registry (can later use Postgres or Redis)        |
| `jwt.rs`        | JWT encoding and validation                                       |
| `main.rs`       | Axum router and startup logic                                     |

---

## Quick Start

### 1. Clone and enter

```bash
git clone https://github.com/steadystate-dev/backend.git
cd backend
```

### 2. Run in Nix shell

```bash
nix develop
```

### 3. Start the server

```bash
cargo run
```

By default, the API listens on `http://localhost:8080`.

---

## Testing GitHub OAuth (manual)

Start the server, then run:

```bash
curl -X POST http://localhost:8080/auth/device?provider=github
```

Youâ€™ll receive a response like:

```json
{
  "device_code": "abcd123",
  "user_code": "ABCD-EFGH",
  "verification_uri": "https://github.com/login/device",
  "interval": 5
}
```

Open the `verification_uri`, enter the `user_code`, and authorize the app.
Then poll:

```bash
curl -X POST http://localhost:8080/auth/poll \
  -H "Content-Type: application/json" \
  -d '{"device_code": "abcd123"}'
```

Once authorized:

```json
{
  "jwt": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "refresh_token": "f2ec8b73-9dd8-4d23-bb3a-6dc0fa7a6c1a",
  "login": "your-github-username"
}
```

You can validate your token:

```bash
curl -H "Authorization: Bearer <jwt>" http://localhost:8080/auth/me
```

---

## Development

### Requirements

* Nix

### Environment variables

| Name                   | Description                         |
| ---------------------- | ----------------------------------- |
| `GITHUB_CLIENT_ID`     | OAuth client ID for your GitHub app |
| `GITHUB_CLIENT_SECRET` | OAuth client secret                 |
| `JWT_SECRET`           | Symmetric signing key for JWTs      |
| `PORT`                 | Optional, defaults to `8080`        |

### Example `.env` file

```env
GITHUB_CLIENT_ID=gho_xxxxxxx
GITHUB_CLIENT_SECRET=ghs_xxxxxxx
JWT_SECRET=devsecret
PORT=8080
```

Then:

```bash
source .env
cargo run
```

---

## Extending Authentication

The authentication system is **provider-agnostic**.
To add a new provider (e.g., GitLab or Orchid):

1. Create a file in `src/providers/<provider>.rs`
2. Implement the `AuthProvider` trait
3. Register it in `main.rs` under the `/auth/device` dispatcher

Example:

```rust
pub trait AuthProvider {
    fn name(&self) -> &'static str;
    async fn start_device_flow(&self) -> Result<DeviceFlowStart>;
    async fn poll_device_flow(&self, device_code: &str) -> Result<UserIdentity>;
}
```

This allows SteadyState to support new identity providers without changing the CLI.

---

## Roadmap

* [x] GitHub OAuth device flow
* [x] JWT issuance and verification
* [x] `--noenv` and `ne` editor integration for quick pair-programming
* [ ] Persistent refresh token storage (SQLite or Postgres)
* [ ] `/sessions` endpoint (Hetzner VM orchestration)
* [ ] Web dashboard for session management

---

## License

Copyright (C) 2025 The Exact Computing Company

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License version 3,
as published by the Free Software Foundation.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program. If not, see <https://www.gnu.org/licenses/>.



########## ./backend/flakes/noenv/flake.nix

{
  description = "SteadyState --noenv environment";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-25.05";
    flake-utils.url = "github:numtide/flake-utils";
    treemerge.url = "github:b-rodrigues/treemerge";
  };

  outputs = { self, nixpkgs, flake-utils, treemerge }:
    flake-utils.lib.eachDefaultSystem (system:
    let
      pkgs = import nixpkgs { inherit system; };
    in
    {
      devShells.default = pkgs.mkShell {
        name = "steadystate-noenv";

        buildInputs = [
          pkgs.upterm
          pkgs.ne
          pkgs.neovim
          pkgs.git
          treemerge.packages.${system}.default
        ];

        # Optional helper tools
        nativeBuildInputs = [
          pkgs.coreutils
        ];

        shellHook = ''
          echo "SteadyState --noenv environment activated."
          echo "You have access to following tools: upterm, ne, neovim, git, teremerge."
        '';
      };
    });
}

########## ./backend/tests/integration_compute.rs

use std::path::PathBuf;
use steadystate_backend::compute::local_provider::LocalComputeProvider;
use steadystate_backend::compute::ComputeProvider;
use steadystate_backend::models::{Session, SessionRequest, SessionState};

#[tokio::test]
#[ignore]
async fn test_integration_nix_check() {
    // This test runs with the RealCommandExecutor, so it interacts with the system.
    // It checks if `nix` is installed (or tries to install it, which might fail in CI if not privileged/configured).
    // We assume the environment has nix or we can at least run the check.

    // We need a dummy flake path.
    let flake_path = PathBuf::from("/tmp/dummy-flake");
    let provider = LocalComputeProvider::new(flake_path);

    // We can't easily call private methods like ensure_nix_installed directly unless we expose them or use start_session.
    // Using start_session involves cloning and upterm, which is heavy.
    // Ideally, we'd test public methods.
    
    // Let's try to start a session with a repo that definitely exists and is small, 
    // or just check if we can instantiate and run something simple.
    // But start_session does everything.
    
    // For this integration test, let's just verify we can create the provider and it has the real executor.
    // To actually test functionality, we'd need to run start_session.
    
    // Let's try to run a session with a non-existent repo, expecting a git failure from the REAL git command.
    // This verifies that the RealCommandExecutor is working and propagating errors.

    let session = Session {
        id: "integration-test-session".into(),
        _repo_url: "https://github.com/this-repo/does-not-exist-12345.git".into(),
        _branch: None,
        _environment: None,
        compute_provider: "local".into(),
        _creator_login: "integration-user".into(),
        state: SessionState::Provisioning,
        endpoint: None,
        _created_at: std::time::SystemTime::now(),
        updated_at: std::time::SystemTime::now(),
        error_message: None,
        magic_link: None,
    };

    let request = SessionRequest {
        repo_url: "https://github.com/this-repo/does-not-exist-12345.git".into(),
        branch: None,
        environment: None,
        provider_config: None,
        allowed_users: None,
        public: false,
        mode: Some("pair".to_string()),
    };

    let result = provider.start_session(&session.id, &request).await;
    
    // We expect an error because the repo doesn't exist.
    // If RealCommandExecutor is working, it will try to run `git clone ...` and fail.
    assert!(result.is_err());
    
    // Verify the error message contains something about git or not found
    let err = result.unwrap_err();
    println!("Integration test error (expected): {:#}", err);
    // The error from LocalComputeProvider::clone_repo is "git clone failed for ..."
    assert!(err.to_string().contains("git clone failed"));
}

########## ./backend/Cargo.toml

[package]
name = "steadystate-backend"
version = "0.0.1"
edition = "2024"

[dependencies]
axum = { version = "0.8.7", features = ["macros", "json"] }
tokio = { version = "1.39", features = ["rt-multi-thread", "macros", "signal", "process"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
serde_qs = "0.15.0"
anyhow = "1"
thiserror = "2.0.17"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features=["env-filter", "fmt"] }
reqwest = { version = "0.12", features = ["json", "rustls-tls"] }
jwt-simple = { version = "0.12", default-features = false, features = ["pure-rust"] }
time = { version = "0.3", features = ["formatting", "parsing", "macros"] }
uuid = { version = "1", features = ["v4"] }
once_cell = "1"
dashmap = "6"
tower-http = { version = "0.6.6", features = ["cors", "trace"] }
async-trait = "0.1"
shell-escape = "0.1"
futures = "0.3"
tokio-stream = { version = "0.1", features = ["io-util"] }
url = "2.5"
rand = "0.8"
notify = "6.1"
dirs = "5.0"
urlencoding = "2.1.3"
clap = { version = "4.5.53", features = ["derive"] }
chrono = "0.4.42"

########## ./backend/src/bin/steady-eventd.rs

use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::time::{Duration, SystemTime};
use std::fs::OpenOptions;
use std::io::Write;

use anyhow::{Context, Result};
use chrono::Utc;
use clap::Parser;
use notify::{RecommendedWatcher, RecursiveMode, Watcher, Event as NotifyEvent};
use tokio::sync::mpsc;

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// Path to the session root directory
    #[arg(short, long)]
    session_root: PathBuf,
}

#[derive(Debug, Clone)]
struct FileEvent {
    user: String,
    path: PathBuf,
    event_type: FileEventType,
}

#[derive(Debug, Clone)]
enum FileEventType {
    Opened,
    Modified,
    Saved,
}

struct EventDaemon {
    session_root: PathBuf,
    activity_log: PathBuf,
    debounce_map: HashMap<PathBuf, std::time::Instant>,
}

impl EventDaemon {
    fn new(session_root: PathBuf) -> Self {
        let activity_log = session_root.join("activity-log");
        Self {
            session_root,
            activity_log,
            debounce_map: HashMap::new(),
        }
    }

    async fn run(&mut self) -> Result<()> {
        let (tx, mut rx) = mpsc::channel(100);
        
        // Initial discovery of workspaces
        let workspaces = self.discover_workspaces()?;
        let mut _watchers = Vec::new(); // Keep watchers alive
        
        for (user, workspace) in workspaces {
            tracing::info!("Watching workspace for user: {}", user);
            let watcher = self.watch_workspace(&user, &workspace, tx.clone())?;
            _watchers.push(watcher);
        }
        
        // TODO: Watch for new user directories appearing?
        // For now, we assume users are created before daemon starts or we restart daemon?
        // Actually, users are created when they SSH in.
        // So we should watch the session_root for new directories too.
        
        let root_tx = tx.clone();
        let root_session = self.session_root.clone();
        let mut root_watcher = notify::recommended_watcher(move |res: notify::Result<NotifyEvent>| {
            if let Ok(event) = res {
                if let notify::event::EventKind::Create(_) = event.kind {
                    for path in event.paths {
                        if path.is_dir() {
                            // Check if it looks like a user workspace (has .git?)
                            // Or just assume any new dir is a user workspace?
                            // Let's just signal a "NewWorkspace" event and handle it in the loop?
                            // For simplicity, we might just poll or restart.
                            // But let's stick to the plan: watch existing.
                            // If we need dynamic watching, we'd need a control channel to add watchers.
                        }
                    }
                }
            }
        })?;
        root_watcher.watch(&self.session_root, RecursiveMode::NonRecursive)?;
        
        tracing::info!("Event daemon started for session: {}", self.session_root.display());

        while let Some(event) = rx.recv().await {
            self.process_file_event(event).await?;
        }
        
        Ok(())
    }
    
    fn discover_workspaces(&self) -> Result<Vec<(String, PathBuf)>> {
        let mut workspaces = Vec::new();
        
        for entry in std::fs::read_dir(&self.session_root)? {
            let entry = entry?;
            let path = entry.path();
            
            if !path.is_dir() { continue; }
            
            let name = path.file_name().unwrap().to_string_lossy().to_string();
            
            // Skip system dirs
            if name.starts_with('.') || name == "git-repo" || name == "canonical" || name == "bin" {
                continue;
            }
            
            // Check if it has .git (valid workspace)
            if path.join(".git").exists() {
                workspaces.push((name, path));
            }
        }
        
        Ok(workspaces)
    }

    fn watch_workspace(
        &self,
        user: &str,
        workspace: &Path,
        tx: mpsc::Sender<FileEvent>,
    ) -> Result<RecommendedWatcher> {
        let user_id = user.to_string();
        let workspace_path = workspace.to_path_buf();
        
        let mut watcher = notify::recommended_watcher(move |res: notify::Result<NotifyEvent>| {
            if let Ok(event) = res {
                for path in &event.paths {
                    if !should_track(path) {
                        continue;
                    }
                    
                    let rel_path = match path.strip_prefix(&workspace_path) {
                        Ok(p) => p.to_path_buf(),
                        Err(_) => continue,
                    };
                    
                    let event_type = match event.kind {
                        notify::event::EventKind::Access(notify::event::AccessKind::Open(_)) => FileEventType::Opened,
                        notify::event::EventKind::Modify(notify::event::ModifyKind::Data(_)) => FileEventType::Modified,
                        notify::event::EventKind::Access(notify::event::AccessKind::Close(notify::event::AccessMode::Write)) => FileEventType::Saved,
                        _ => continue,
                    };
                    
                    let file_event = FileEvent {
                        user: user_id.clone(),
                        path: rel_path,
                        event_type,
                    };
                    
                    let _ = tx.blocking_send(file_event);
                }
            }
        })?;
        
        watcher.watch(workspace, RecursiveMode::Recursive)?;
        Ok(watcher)
    }
    
    async fn process_file_event(&mut self, event: FileEvent) -> Result<()> {
        // Debounce
        let key = event.path.clone(); // Simple debounce by path
        let now = std::time::Instant::now();
        
        if let Some(last) = self.debounce_map.get(&key) {
            if now.duration_since(*last) < Duration::from_millis(500) {
                return Ok(());
            }
        }
        self.debounce_map.insert(key, now);
        
        let action = match event.event_type {
            FileEventType::Opened => format!("editing:{}", event.path.display()),
            FileEventType::Modified => return Ok(()), // Don't log every modification, wait for save
            FileEventType::Saved => format!("saved:{}", event.path.display()),
        };
        
        self.log_activity(&event.user, &action).await?;
        
        Ok(())
    }
    
    async fn log_activity(&self, user: &str, action: &str) -> Result<()> {
        let mut file = OpenOptions::new()
            .create(true)
            .append(true)
            .open(&self.activity_log)?;
            
        let log_line = format!(
            "{},{},{}\n",
            Utc::now().to_rfc3339(),
            user,
            action
        );
        
        file.write_all(log_line.as_bytes())?;
        // file.sync_all()?; // Async write might be better but std::fs is blocking.
        // Since this is a separate binary, blocking IO is fine for now.
        
        Ok(())
    }
}

fn should_track(path: &Path) -> bool {
    // Basic filtering
    let s = path.to_string_lossy();
    !s.contains("/.git/") && 
    !s.contains("/target/") && 
    !s.contains("/node_modules/") &&
    !s.ends_with('~')
}

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::fmt::init();
    
    let args = Args::parse();
    
    if !args.session_root.exists() {
        return Err(anyhow::anyhow!("Session root does not exist: {}", args.session_root.display()));
    }
    
    let mut daemon = EventDaemon::new(args.session_root);
    daemon.run().await?;
    
    Ok(())
}

########## ./backend/src/routes/sessions.rs

// backend/src/routes/sessions.rs

use std::sync::Arc;
use axum::{
    extract::{Path, State},
    http::StatusCode,
    routing::{get, post, delete},
    Json, Router,
};
use uuid::Uuid;

use crate::{
    jwt::CustomClaims,
    models::{Session, SessionInfo, SessionRequest, SessionState},
    state::AppState,
};

pub fn router() -> Router<Arc<AppState>> {
    Router::new()
        .route("/", post(create_session))
        .route("/{id}", get(get_session_status))
        .route("/{id}", delete(terminate_session))
}

async fn run_provisioning(
    app_state: Arc<AppState>,
    session_id: String,
    request: SessionRequest,
) {
    // 1. Retrieve the provider ID
    let provider_id = if let Some(session) = app_state.sessions.get(&session_id) {
        session.compute_provider.clone()
    } else {
        return; 
    };

    // 2. Get the provider (map is now wrapped in Arc, so access is cheap)
    let provider = if let Some(p) = app_state.compute_providers.get(&provider_id) {
        p.clone()
    } else {
        tracing::error!("Provider '{}' not found", provider_id);
        return;
    };

    // 3. Do the work (release lock first!)
    // We clone request data needed for provisioning if necessary, but here we pass the whole request.
    
    // Release the lock by not holding a reference to session_entry across the await point.
    // We already have provider_id and provider.
    
    let result = provider.start_session(&session_id, &request).await;

    // 4. Handle result
    if let Some(mut session) = app_state.sessions.get_mut(&session_id) {
        match result {
            Ok(start_result) => {
                session.state = SessionState::Running;
                session.endpoint = start_result.endpoint;
                session.magic_link = start_result.magic_link;
                session.updated_at = std::time::SystemTime::now();
                tracing::info!("Session {} provisioned successfully", session_id);
            }
            Err(e) => {
                tracing::error!("Provisioning failed for session {}: {:#}", session_id, e);
                session.state = SessionState::Failed;
                session.error_message = Some(format!("{:#}", e));
                session.updated_at = std::time::SystemTime::now();
            }
        }
    } else {
        tracing::warn!("Session {} disappeared after provisioning", session_id);
    }
}

async fn create_session(
    State(state): State<Arc<AppState>>,
    claims: CustomClaims,
    Json(mut request): Json<SessionRequest>,
) -> (StatusCode, Json<SessionInfo>) {
    let session_id = Uuid::new_v4().to_string();
    let now = std::time::SystemTime::now();

    let session = Session {
        id: session_id.clone(),
        state: SessionState::Provisioning,
        _repo_url: request.repo_url.clone(),
        _branch: request.branch.clone(),
        _environment: request.environment.clone(),
        endpoint: None,
        // FIX IS HERE: Access default_compute_provider via config
        compute_provider: state.config.default_compute_provider.clone(),
        _creator_login: claims.sub.clone(),
        _created_at: now,
        updated_at: now,
        error_message: None,
        magic_link: None,
    };

    let session_info = SessionInfo::from(&session);
    
    state.sessions.insert(session_id.clone(), session);
    tracing::info!("Session {} inserted into map, total sessions: {}", session_id, state.sessions.len());

    // --- Inject GitHub token if available ---
    if claims.provider == "github" {
        if let Some(token) = state
            .provider_tokens
            .get(&("github".to_string(), claims.sub.clone()))
        {
            request.provider_config = Some(serde_json::json!({
                "github": {
                    "login": claims.sub,
                    "access_token": token.value().clone(),
                }
            }));
        }
    }

    // state is cheap to clone now
    tokio::spawn(run_provisioning(state.clone(), session_id, request));

    (StatusCode::ACCEPTED, Json(session_info))
}

async fn get_session_status(
    State(state): State<Arc<AppState>>,
    _claims: CustomClaims,
    Path(id): Path<String>,
) -> Result<Json<SessionInfo>, StatusCode> {
    tracing::info!("GET /sessions/{}, total sessions in map: {}", id, state.sessions.len());
    match state.sessions.get(&id) {
        Some(session) => {
            tracing::info!("Found session {} in state {:?}", id, session.state);
            Ok(Json(SessionInfo::from(&*session)))
        }
        None => {
            tracing::warn!("Session {} not found in map", id);
            Err(StatusCode::NOT_FOUND)
        }
    }
}

async fn terminate_session(
    State(state): State<Arc<AppState>>,
    Path(id): Path<String>,
) -> StatusCode {
    if let Some(mut session) = state.sessions.get_mut(&id) {
        session.state = SessionState::Terminating;
        
        if let Some(provider) = state.compute_providers.get(&session.compute_provider) {
            let provider = provider.clone();
            let session_clone = session.clone();
            
            tokio::spawn(async move {
                if let Err(e) = provider.terminate_session(&session_clone).await {
                    tracing::error!("Failed to terminate session {}: {:#}", id, e);
                }
            });
        }
        StatusCode::ACCEPTED
    } else {
        StatusCode::NOT_FOUND
    }
}

########## ./backend/src/routes/auth.rs

// backend/src/routes/auth.rs

use std::sync::Arc;
use axum::{
    extract::{Query, State},
    http::StatusCode,
    routing::{get, post},
    Json, Router,
};
use serde_json::json;
use tracing::{info, warn};

use crate::jwt::CustomClaims;
use crate::{
    auth::provider::DevicePollOutcome,
    models::*,
    state::AppState,
};

pub fn router() -> Router<Arc<AppState>> {
    Router::new()
        .route("/device", post(device_start))
        .route("/poll", post(poll))
        .route("/refresh", post(refresh))
        .route("/revoke", post(revoke))
        .route("/me", get(me))
}

pub async fn device_start(
    State(state): State<Arc<AppState>>,
    Query(q): Query<DeviceQuery>,
) -> Result<Json<DeviceStartResponse>, (StatusCode, String)> {
    let provider_id = ProviderId::from(q.provider.as_deref().unwrap_or("github"));

    let provider = state.get_or_create_provider(&provider_id).await
        .map_err(|e| (StatusCode::BAD_REQUEST, e.to_string()))?;

    let start = provider.start_device_flow().await
        .map_err(internal)?;

    state.device_pending.insert(start.device_code.clone(), PendingDevice {
        provider: q.provider.unwrap_or("github".into()).into(),
        _device_code: start.device_code.clone(),
        _user_code: start.user_code.clone(),
        _verification_uri: start.verification_uri.clone(),
        _interval: start.interval,
        _created_at: now(),
    });

    Ok(Json(start))
}

pub async fn poll(
    State(state): State<Arc<AppState>>,
    Json(q): Json<PollQuery>,
) -> Result<Json<PollOut>, (StatusCode, String)> {
    let pending = match state.device_pending.get(&q.device_code) {
        Some(e) => e,
        None => {
            return Ok(Json(PollOut {
                status: None,
                jwt: None,
                refresh_token: None,
                login: None,
                error: Some("invalid_device_code".into()),
            }))
        }
    };
    let provider_id = pending.provider.clone();
    drop(pending);

    let provider = match state.get_or_create_provider(&provider_id).await {
        Ok(p) => p,
        Err(e) => return Err((StatusCode::INTERNAL_SERVER_ERROR, e.to_string())),
    };

    match provider.poll_device_flow(&q.device_code).await {
        Ok(DevicePollOutcome::Complete { identity, provider_access_token }) => {
            info!(
                "device flow complete for {} via {}",
                identity.login,
                provider_id.as_str()
            );

            state.device_pending.remove(&q.device_code);

            let jwt = state
                .jwt
                .sign(&identity.login, provider_id.as_str())
                .map_err(internal)?;
            let refresh_token = state.issue_refresh_token(identity.login.clone(), provider_id);

            if let Some(token) = provider_access_token {
                state.provider_tokens.insert(
                    (identity.provider.clone(), identity.login.clone()),
                    token
                );
                if let Err(e) = state.save_tokens() {
                    warn!("Failed to persist tokens: {}", e);
                }
            }

            Ok(Json(PollOut {
                status: Some("complete".into()),
                jwt: Some(jwt),
                refresh_token: Some(refresh_token),
                login: Some(identity.login),
                error: None,
            }))
        }
        Ok(DevicePollOutcome::Pending) => {
            info!("poll pending for provider '{}'", provider_id.as_str());
            Ok(Json(PollOut {
                status: Some("pending".into()),
                jwt: None, refresh_token: None, login: None,
                error: None,
            }))
        },
        Ok(DevicePollOutcome::SlowDown) => {
            info!("poll slow_down for provider '{}'", provider_id.as_str());
            Ok(Json(PollOut {
                status: Some("pending".into()),
                jwt: None, refresh_token: None, login: None,
                error: Some("slow_down".into()),
            }))
        },
        Err(e) => {
            warn!("poll error for provider '{}': {}", provider_id.as_str(), e);
            Ok(Json(PollOut {
                status: None,
                jwt: None,
                refresh_token: None,
                login: None,
                error: Some(e.to_string()),
            }))
        }
    }
}


pub async fn refresh(
    State(state): State<Arc<AppState>>,
    Json(inp): Json<RefreshIn>,
) -> Result<Json<RefreshOut>, (StatusCode, String)> {
    let rec = state
        .refresh_store
        .get(&inp.refresh_token)
        .map(|e| e.clone())
        .ok_or_else(|| (StatusCode::UNAUTHORIZED, "invalid refresh token".into()))?;

    if now() >= rec.expires_at {
        state.refresh_store.remove(&inp.refresh_token);
        return Err((StatusCode::UNAUTHORIZED, "refresh expired".into()));
    }

    let jwt = state
        .jwt
        .sign(&rec.login, rec.provider.as_str())
        .map_err(internal)?;

    Ok(Json(RefreshOut {
        jwt,
        refresh_expires_at: Some(rec.expires_at),
    }))
}


pub async fn revoke(
    State(state): State<Arc<AppState>>,
    Json(inp): Json<RevokeIn>,
) -> Result<Json<serde_json::Value>, (StatusCode, String)> {
    state.refresh_store.remove(&inp.refresh_token);
    Ok(Json(json!({ "revoked": true })))
}

fn now() -> u64 {
    std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).expect("System time is before UNIX EPOCH").as_secs()
}

fn internal<E: std::fmt::Display>(e: E) -> (StatusCode, String) {
    (StatusCode::INTERNAL_SERVER_ERROR, e.to_string())
}

pub async fn me(
    claims: CustomClaims,
) -> Json<CustomClaims> {
    Json(claims)
}

########## ./backend/src/routes/mod.rs

pub mod auth;
pub mod sessions;


########## ./backend/src/lib.rs

pub mod state;
pub mod jwt;
pub mod models;
pub mod routes;
pub mod auth;
pub mod compute;

########## ./backend/src/state.rs

// backend/src/state.rs

use std::{collections::HashMap, sync::Arc, time::Duration};
use anyhow::{anyhow, Context, Result};
use dashmap::DashMap;
use reqwest::Client;
use tracing::info;
use uuid::Uuid;

use crate::auth;
use crate::auth::provider::{AuthProviderDyn, AuthProviderFactoryDyn};
use crate::compute::local_provider::LocalComputeProvider;
use crate::compute::ComputeProvider;
use crate::jwt::JwtKeys;
use crate::models::{PendingDevice, ProviderId, RefreshRecord, Session};

pub type SessionStore = DashMap<String, Session>;

// --- Centralized Configuration ---
#[derive(Clone)]
pub struct Config {
    // Auth Keys
    pub enable_fake_auth: bool,
    pub github_client_id: Option<String>,
    pub github_client_secret: Option<String>,
    pub _gitlab_client_id: Option<String>,
    pub _gitlab_client_secret: Option<String>,
    pub _orchid_client_id: Option<String>,
    pub _orchid_client_secret: Option<String>,
    
    // Timeouts & TTLs
    pub _device_poll_interval: u64,
    pub jwt_ttl_secs: u64,
    pub refresh_ttl_secs: u64,

    // Compute
    pub noenv_flake_path: String,
    pub default_compute_provider: String,
}

impl Config {
    pub fn from_env() -> Result<Self> {
        Ok(Self {
            enable_fake_auth: std::env::var("ENABLE_FAKE_AUTH").is_ok(),
            github_client_id: std::env::var("GITHUB_CLIENT_ID").ok(),
            github_client_secret: std::env::var("GITHUB_CLIENT_SECRET").ok(),
            _gitlab_client_id: std::env::var("GITLAB_CLIENT_ID").ok(),
            _gitlab_client_secret: std::env::var("GITLAB_CLIENT_SECRET").ok(),
            _orchid_client_id: std::env::var("ORCHID_CLIENT_ID").ok(),
            _orchid_client_secret: std::env::var("ORCHID_CLIENT_SECRET").ok(),
            
            _device_poll_interval: std::env::var("DEVICE_POLL_MAX_INTERVAL_SECS")
                .ok().and_then(|s| s.parse().ok()).unwrap_or(15),
            jwt_ttl_secs: std::env::var("JWT_TTL_SECS")
                .ok().and_then(|s| s.parse().ok()).unwrap_or(900),
            refresh_ttl_secs: std::env::var("REFRESH_TTL_SECS")
                .ok().and_then(|s| s.parse().ok()).unwrap_or(14 * 24 * 3600),
            
            noenv_flake_path: std::env::var("NOENV_FLAKE_PATH")
                .context("NOENV_FLAKE_PATH must be set")?,
            default_compute_provider: std::env::var("STEADYSTATE_PROVIDER")
                .unwrap_or_else(|_| "local".to_string()),
        })
    }
}

// --- AppState Definition ---
// Derived Clone is now efficient because complex types are wrapped in Arc/DashMap.
#[derive(Clone)]
pub struct AppState {
    pub http: Client,
    pub jwt: JwtKeys,
    pub config: Config,

    // Auth state
    pub device_pending: Arc<DashMap<String, PendingDevice>>,
    pub refresh_store: Arc<DashMap<String, RefreshRecord>>,
    pub providers: Arc<DashMap<ProviderId, AuthProviderDyn>>,
    pub provider_factories: Arc<DashMap<String, AuthProviderFactoryDyn>>,
    // Key: (provider, login) -> access_token
    pub provider_tokens: Arc<DashMap<(String, String), String>>,

    // Compute & Session state
    pub sessions: SessionStore,
    // Map is wrapped in Arc to allow cheap cloning of AppState
    pub compute_providers: Arc<HashMap<String, Arc<dyn ComputeProvider>>>,
}

impl AppState {
    pub async fn try_new() -> anyhow::Result<Arc<Self>> {
        // 1. Load Config first to fail fast on missing env vars
        let config = Config::from_env()?;

        let http = Client::builder()
            .user_agent("steadystate-backend/0.1")
            .timeout(Duration::from_secs(30))
            .pool_max_idle_per_host(8)
            .build()
            .context("build reqwest client")?;

        let secret = std::env::var("JWT_SECRET").context("JWT_SECRET not set")?;
        let issuer = std::env::var("JWT_ISSUER").unwrap_or("steadystate".into());
        let jwt = JwtKeys::new(&secret, &issuer, config.jwt_ttl_secs);

        // 2. Setup Compute Providers
        let mut compute_providers = HashMap::<String, Arc<dyn ComputeProvider>>::new();

        // Initialize local provider using config path
        let local_provider = Arc::new(LocalComputeProvider::new(config.noenv_flake_path.clone().into()));
        compute_providers.insert(local_provider.id().to_string(), local_provider);

        // 3. Build State
        let state = Arc::new(Self {
            http,
            jwt,
            config,
            device_pending: Arc::new(DashMap::new()),
            refresh_store: Arc::new(DashMap::new()),
            providers: Arc::new(DashMap::new()),
            provider_factories: Arc::new(DashMap::new()),
            provider_tokens: Arc::new(load_tokens()),
            sessions: SessionStore::new(),
            compute_providers: Arc::new(compute_providers),
        });

        // 4. Register Auth Providers
        auth::register_builtin_providers(&state);

        Ok(state)
    }
    
    pub fn register_provider_factory(&self, factory: AuthProviderFactoryDyn) {
        self.provider_factories.insert(factory.id().to_string(), factory);
    }

    pub async fn get_or_create_provider(&self, id: &ProviderId) -> Result<AuthProviderDyn> {
        if let Some(provider) = self.providers.get(id) {
            return Ok(provider.clone());
        }

        info!("Initializing auth provider for the first time: {}", id.as_str());
        
        let key = id.as_str();
        let factory = self.provider_factories
            .get(key)
            .ok_or_else(|| anyhow!("Unknown or unsupported auth provider: '{}'", key))?
            .clone();

        let provider = factory.build(self).await?;
        self.providers.insert(id.clone(), provider.clone());
        Ok(provider)
    }

    pub fn issue_refresh_token(&self, login: String, provider: ProviderId) -> String {
        let token = Uuid::new_v4().to_string();
        
        // Use cached TTL from config
        let expires_at = now() + self.config.refresh_ttl_secs;

        self.refresh_store.insert(token.clone(), RefreshRecord {
            login,
            provider,
            expires_at,
        });

        token
    }
    pub fn save_tokens(&self) -> Result<()> {
        let home = std::env::var("HOME").context("HOME not set")?;
        let dir = std::path::PathBuf::from(home).join(".steadystate");
        std::fs::create_dir_all(&dir)?;
        let file_path = dir.join("tokens.json");

        let mut map = HashMap::new();
        for item in self.provider_tokens.iter() {
            let (key, value) = item.pair();
            // key is (provider, login)
            map.insert(format!("{}:{}", key.0, key.1), value.clone());
        }

        let json = serde_json::to_string_pretty(&map)?;
        std::fs::write(file_path, json)?;
        Ok(())
    }
}

fn load_tokens() -> DashMap<(String, String), String> {
    let dash = DashMap::new();
    if let Ok(home) = std::env::var("HOME") {
        let file_path = std::path::PathBuf::from(home).join(".steadystate").join("tokens.json");
        if file_path.exists() {
            if let Ok(content) = std::fs::read_to_string(file_path) {
                if let Ok(map) = serde_json::from_str::<HashMap<String, String>>(&content) {
                    for (k, v) in map {
                        if let Some((provider, login)) = k.split_once(':') {
                            dash.insert((provider.to_string(), login.to_string()), v);
                        }
                    }
                    info!("Loaded {} tokens from disk", dash.len());
                }
            }
        }
    }
    dash
}

fn now() -> u64 {
    std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .expect("System time is before UNIX EPOCH")
        .as_secs()
} 

########## ./backend/src/models.rs

// backend/src/models.rs

use serde::{Deserialize, Serialize};
use crate::auth::provider::UserIdentity;

// ============================================================================
//  Authentication & Identity Models
// ============================================================================

/// A type-safe, string-based identifier for an authentication provider.
#[derive(Clone, Debug, Eq, PartialEq, Hash, Serialize, Deserialize)]
pub struct ProviderId(String);

impl ProviderId {
    pub fn as_str(&self) -> &str { &self.0 }
}

impl From<String> for ProviderId {
    fn from(s: String) -> Self { ProviderId(s) }
}

impl From<&str> for ProviderId {
    fn from(s: &str) -> Self { ProviderId(s.to_owned()) }
}

#[derive(Clone)]
pub struct PendingDevice {
    pub provider: ProviderId,
    pub _device_code: String,
    pub _user_code: String,
    pub _verification_uri: String,
    pub _interval: u64,
    pub _created_at: u64,
}

#[derive(Clone)]
pub struct RefreshRecord {
    pub login: String,
    pub provider: ProviderId,
    pub expires_at: u64,
}

#[derive(Serialize)]
pub struct DeviceStartResponse {
    pub device_code: String,
    pub user_code: String,
    pub verification_uri: String,
    pub expires_in: u64,
    pub interval: u64,
}

#[derive(Serialize)]
pub struct PollOut {
    pub status: Option<String>,
    pub jwt: Option<String>,
    pub refresh_token: Option<String>,
    pub login: Option<String>,
    pub error: Option<String>,
}

#[derive(Deserialize)]
pub struct PollQuery {
    pub device_code: String,
}

#[derive(Deserialize)]
pub struct DeviceQuery {
    pub provider: Option<String>,
}

#[derive(Deserialize)]
pub struct RefreshIn {
    pub refresh_token: String,
}

#[derive(Serialize)]
pub struct RefreshOut {
    pub jwt: String,
    pub refresh_expires_at: Option<u64>,
}

#[derive(Deserialize)]
pub struct RevokeIn {
    pub refresh_token: String,
}

#[derive(Serialize)]
pub struct WhoamiOut {
    pub login: String,
    pub provider: String,
}

impl From<UserIdentity> for WhoamiOut {
    fn from(u: UserIdentity) -> Self {
        Self { login: u.login, provider: u.provider }
    }
}

#[derive(Serialize)]
pub struct UserInfo {
    pub login: String,
    pub provider: String,
}

// ============================================================================
//  Compute & Session Models
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SessionState {
    Provisioning,
    Running,
    Failed,
    Terminating,
    Terminated,
}

/// The internal representation of a session, stored in the backend.
#[derive(Debug, Clone)]
pub struct Session {
    pub id: String,
    pub state: SessionState,
    pub _repo_url: String,
    pub _branch: Option<String>,
    pub _environment: Option<String>,
    pub endpoint: Option<String>,
    pub compute_provider: String,
    pub _creator_login: String,
    pub _created_at: std::time::SystemTime,
    pub updated_at: std::time::SystemTime,
    pub error_message: Option<String>,
    pub magic_link: Option<String>,
}

/// The request from the CLI to create a new session.
#[derive(Debug, Deserialize)]
pub struct SessionRequest {
    pub repo_url: String,
    pub branch: Option<String>,
    pub environment: Option<String>,
    pub provider_config: Option<serde_json::Value>,
    pub allowed_users: Option<Vec<String>>,
    #[serde(default)]
    pub public: bool,
    pub mode: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MagicLink {
    pub mode: String,
    pub session_id: String,
    pub username: String,
    pub hostname: String,
    pub port: Option<u16>,
    pub token: Option<String>,
    pub fingerprint: Option<String>,
    pub upterm_url: Option<String>,
}

/// The information about a session that is sent back to the CLI.
#[derive(Debug, Serialize)]
pub struct SessionInfo {
    pub id: String,
    pub state: SessionState,
    pub endpoint: Option<String>,
    pub compute_provider: String,
    pub message: Option<String>,
    pub magic_link: Option<String>,
}

impl From<&Session> for SessionInfo {
    fn from(session: &Session) -> Self {
        Self {
            id: session.id.clone(),
            state: session.state.clone(),
            endpoint: session.endpoint.clone(),
            compute_provider: session.compute_provider.clone(),
            message: session.error_message.clone(),
            magic_link: session.magic_link.clone(),
        }
    }
}

/// The result of a successful session start operation.
#[derive(Debug)]
pub struct SessionStartResult {
    pub endpoint: Option<String>,
    pub magic_link: Option<String>,
}

########## ./backend/src/main.rs

// backend/src/main.rs

// ... (same imports)
use std::net::SocketAddr;
use axum::Router;
use tower_http::{cors::CorsLayer, trace::TraceLayer};
use steadystate_backend::state::AppState;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    tracing_subscriber::fmt::init();

    let state = AppState::try_new().await?;

    let app: Router = Router::new()
        .nest("/auth", steadystate_backend::routes::auth::router())
        .nest("/sessions", steadystate_backend::routes::sessions::router())
        // Pass Arc<AppState> directly - do NOT dereference and clone!
        // Each clone of Arc points to the same underlying AppState.
        .with_state(state.clone()) 
        .layer(CorsLayer::permissive())
        .layer(TraceLayer::new_for_http());

    let port: u16 = std::env::var("PORT")
        .ok()
        .and_then(|s| s.parse().ok())
        .unwrap_or(8080);

    let addr = SocketAddr::from(([0, 0, 0, 0], port));

    tracing::info!("SteadyState backend listening on http://{addr}");

    let listener = tokio::net::TcpListener::bind(addr).await?;
    axum::serve(listener, app).await?;

    Ok(())
} 

########## ./backend/src/jwt.rs

// backend/src/jwt.rs

use std::collections::HashSet;
use std::sync::Arc;
use anyhow::{anyhow, Result};
use axum::{
    extract::FromRequestParts,
    http::{header, request::Parts, StatusCode},
};
use jwt_simple::prelude::*;
use serde::{Deserialize, Serialize};
use crate::state::AppState;

#[derive(Clone)]
pub struct JwtKeys {
    key: HS256Key,
    issuer: String,
    ttl_duration: Duration,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
struct InternalCustomClaims {
    provider: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CustomClaims {
    pub sub: String,
    pub provider: String,
}

impl JwtKeys {
    pub fn new(secret: &str, issuer: &str, ttl_secs: u64) -> Self {
        Self {
            key: HS256Key::from_bytes(secret.as_bytes()),
            issuer: issuer.into(),
            ttl_duration: Duration::from_secs(ttl_secs),
        }
    }

    pub fn sign(&self, login: &str, provider: &str) -> Result<String> {
        let internal_claims = InternalCustomClaims {
            provider: provider.to_string(),
        };

        let claims = Claims::with_custom_claims(internal_claims, self.ttl_duration)
            .with_issuer(self.issuer.clone())
            .with_subject(login.to_string());

        self.key.authenticate(claims).map_err(|e| anyhow!("Failed to sign JWT: {}", e))
    }

    pub fn verify(&self, token: &str) -> Result<CustomClaims> {
        let mut allowed_issuers = HashSet::new();
        allowed_issuers.insert(self.issuer.clone());

        let options = VerificationOptions {
            allowed_issuers: Some(allowed_issuers),
            ..Default::default()
        };

        let claims = self.key
            .verify_token::<InternalCustomClaims>(token, Some(options))
            .map_err(|e| anyhow!("Invalid or expired JWT: {}", e))?;
        
        let sub = claims.subject.ok_or_else(|| anyhow!("JWT missing subject claim"))?;

        Ok(CustomClaims {
            sub,
            provider: claims.custom.provider,
        })
    }
}

// --- AXUM 0.8 EXTRACTOR FOR CUSTOM CLAIMS ---
// NOTE: Do NOT use #[async_trait] here. Axum 0.8 uses standard async fn traits.

impl FromRequestParts<Arc<AppState>> for CustomClaims {
    type Rejection = (StatusCode, String);

    async fn from_request_parts(
        parts: &mut Parts,
        state: &Arc<AppState>,
    ) -> Result<Self, Self::Rejection> {
        let auth_header = parts
            .headers
            .get(header::AUTHORIZATION)
            .and_then(|value| value.to_str().ok())
            .ok_or_else(|| {
                (StatusCode::UNAUTHORIZED, "Missing Authorization header".into())
            })?;

        let token = auth_header
            .strip_prefix("Bearer ")
            .ok_or_else(|| {
                (StatusCode::BAD_REQUEST, "Invalid token type; expected Bearer".into())
            })?;

        state
            .jwt
            .verify(token)
            .map_err(|e| (StatusCode::UNAUTHORIZED, e.to_string()))
    }
}

########## ./backend/src/auth/gitlab.rs

// backend/src/auth/gitlab.rs

use std::sync::Arc;
use anyhow::anyhow;
use async_trait::async_trait;

use crate::auth::provider::{
    AuthProvider, AuthProviderDyn, AuthProviderFactory, DevicePollOutcome,
};
use crate::models::{DeviceStartResponse, ProviderId};
use crate::state::AppState;

// --- Provider Stub ---
#[derive(Debug)] // <-- ADDED THIS LINE
pub struct GitLabAuth;

// ... (rest of the file is unchanged) ...
#[async_trait]
impl AuthProvider for GitLabAuth {
    fn id(&self) -> ProviderId {
        ProviderId::from("gitlab")
    }

    async fn start_device_flow(&self) -> anyhow::Result<DeviceStartResponse> {
        Err(anyhow!("GitLab device flow is not implemented yet"))
    }

    async fn poll_device_flow(&self, _device_code: &str) -> anyhow::Result<DevicePollOutcome> {
        Err(anyhow!("GitLab device flow is not implemented yet"))
    }
}

// --- Factory Stub ---
pub struct GitLabFactory;

#[async_trait]
impl AuthProviderFactory for GitLabFactory {
    fn id(&self) -> &'static str {
        "gitlab"
    }

    async fn build(self: Arc<Self>, _state: &AppState) -> anyhow::Result<AuthProviderDyn> {
        Err(anyhow!("The 'gitlab' provider is not configured on the server"))
    }
}  

########## ./backend/src/auth/github.rs

// backend/src/auth/github.rs

use std::sync::Arc;
use anyhow::{anyhow, Context};
use async_trait::async_trait;
use reqwest::Client;
use serde::Deserialize;

use crate::auth::provider::{
    AuthProvider, AuthProviderDyn, AuthProviderFactory, DevicePollOutcome, UserIdentity,
};
use crate::models::{DeviceStartResponse, ProviderId};
use crate::state::AppState;

// --- Provider Implementation ---

#[derive(Debug)]
pub struct GitHubAuth {
    pub client_id: String,
    pub client_secret: String,
    pub http: Client,
}

impl GitHubAuth {
    pub fn new(http: Client, client_id: String, client_secret: String) -> Arc<Self> {
        Arc::new(Self {
            client_id,
            client_secret,
            http,
        })
    }
}

#[async_trait]
impl AuthProvider for GitHubAuth {
    fn id(&self) -> ProviderId {
        ProviderId::from("github")
    }

    async fn start_device_flow(&self) -> anyhow::Result<DeviceStartResponse> {
        let params = [
            ("client_id", self.client_id.as_str()),
            ("scope", "read:user repo"),
        ];

        let resp = self.http
            .post("https://github.com/login/device/code")
            .header("Accept", "application/json")
            .header("User-Agent", "steadystate-backend/0.1")
            .form(&params)
            .send()
            .await?
            .error_for_status()?;

        let out: DeviceStartOut = resp.json().await
            .context("Failed to decode GitHub's device code response")?;

        Ok(DeviceStartResponse {
            device_code: out.device_code,
            user_code: out.user_code,
            verification_uri: out.verification_uri,
            expires_in: out.expires_in,
            interval: out.interval.unwrap_or(5),
        })
    }

    async fn poll_device_flow(&self, device_code: &str) -> anyhow::Result<DevicePollOutcome> {
        let params = [
            ("client_id", self.client_id.as_str()),
            ("device_code", device_code),
            ("grant_type", "urn:ietf:params:oauth:grant-type:device_code"),
            ("client_secret", self.client_secret.as_str()),
        ];

        let resp = self.http
            .post("https://github.com/login/oauth/access_token")
            .header("Accept", "application/json")
            .header("User-Agent", "steadystate-backend/0.1")
            .form(&params)
            .send()
            .await?
            .error_for_status()?;

        let token: DeviceTokenOut = resp.json().await
            .context("Failed to decode GitHub's access token response")?;

        match token {
            DeviceTokenOut::Ok { access_token, .. } => {
                let user = self.http
                    .get("https://api.github.com/user")
                    .bearer_auth(&access_token)
                    .header("User-Agent", "steadystate-backend/0.1")
                    .send()
                    .await?
                    .error_for_status()?
                    .json::<GhUser>()
                    .await?;

                Ok(DevicePollOutcome::Complete {
                    identity: UserIdentity {
                        id: user.id.to_string(),
                        login: user.login,
                        email: user.email,
                        provider: "github".into(),
                    },
                    provider_access_token: Some(access_token),
                })
            }
            DeviceTokenOut::Err(err) => match err.error {
                GitHubError::AuthorizationPending => Ok(DevicePollOutcome::Pending),
                GitHubError::SlowDown => Ok(DevicePollOutcome::SlowDown),
                other_error => Err(anyhow!("GitHub device flow error: {:?}", other_error)),
            },
        }
    }
}

// --- Factory Implementation ---

pub struct GitHubFactory;

#[async_trait]
impl AuthProviderFactory for GitHubFactory {
    fn id(&self) -> &'static str { "github" }

    async fn build(self: Arc<Self>, state: &AppState) -> anyhow::Result<AuthProviderDyn> {
        let client_id = state.config.github_client_id.clone()
            .context("GITHUB_CLIENT_ID is not configured on the server")?;
        let client_secret = state.config.github_client_secret.clone()
            .context("GITHUB_CLIENT_SECRET is not configured on the server")?;

        Ok(GitHubAuth::new(
            state.http.clone(),
            client_id,
            client_secret,
        ))
    }
}


// --- DTOs for GitHub API ---

#[derive(Deserialize)]
struct DeviceStartOut {
    device_code: String,
    user_code: String,
    verification_uri: String,
    expires_in: u64,
    interval: Option<u64>,
}

#[derive(Deserialize, Debug)]
#[serde(rename_all = "snake_case")]
enum GitHubError {
    AuthorizationPending,
    SlowDown,
    ExpiredToken,
    UnsupportedGrantType,
    IncorrectClientCredentials,
    IncorrectDeviceCode,
    AccessDenied,
    // Catch-all for any unknown errors GitHub might add in the future.
    #[serde(other)]
    Unknown,
}

#[derive(Deserialize)]
struct DeviceTokenError {
    error: GitHubError,
    #[serde(rename = "error_description")]
    _error_description: Option<String>,
}

#[derive(Deserialize)]
#[serde(untagged)]
enum DeviceTokenOut {
    Ok {
        access_token: String,
        #[serde(rename = "token_type")]
        _token_type: String,
        #[serde(rename = "scope")]
        _scope: String,
    },
    Err(DeviceTokenError),
}

#[derive(Deserialize)]
struct GhUser {
    login: String,
    id: u64,
    email: Option<String>,
}

########## ./backend/src/auth/provider.rs

// backend/src/auth/provider.rs

use std::sync::Arc;
use async_trait::async_trait;
use serde::Serialize;
use crate::{
    models::{DeviceStartResponse, ProviderId},
    state::AppState,
};

#[derive(Clone, Debug, Serialize)]
pub struct UserIdentity {
    pub id: String,
    pub login: String,
    pub email: Option<String>,
    pub provider: String, // "github" | "gitlab" | ...
}

pub enum DevicePollOutcome {
    Pending,
    SlowDown,
    Complete {
        identity: UserIdentity,
        provider_access_token: Option<String>,
    },
}

// Any type implementing AuthProvider must also implement Debug, Send, and Sync.
#[async_trait]
pub trait AuthProvider: std::fmt::Debug + Send + Sync {
    #[allow(dead_code)]
    fn id(&self) -> ProviderId;
    async fn start_device_flow(&self) -> anyhow::Result<DeviceStartResponse>;
    async fn poll_device_flow(
        &self,
        device_code: &str,
    ) -> anyhow::Result<DevicePollOutcome>;
}

pub type AuthProviderDyn = Arc<dyn AuthProvider>;

#[async_trait]
pub trait AuthProviderFactory: Send + Sync {
    fn id(&self) -> &'static str;
    async fn build(self: Arc<Self>, state: &AppState) -> anyhow::Result<AuthProviderDyn>;
}

pub type AuthProviderFactoryDyn = Arc<dyn AuthProviderFactory>;  

########## ./backend/src/auth/fake.rs

// backend/src/auth/fake.rs

use std::sync::Arc;
use anyhow::anyhow;
use async_trait::async_trait;

use crate::auth::provider::{
    AuthProvider, AuthProviderDyn, AuthProviderFactory, DevicePollOutcome, UserIdentity,
};
use crate::models::{DeviceStartResponse, ProviderId};
use crate::state::AppState;

// --- Provider Implementation ---

#[derive(Debug)] // <-- ADDED THIS LINE
pub struct FakeAuth;

// ... (rest of the file is unchanged) ...
impl FakeAuth {
    pub fn new() -> Arc<Self> {
        Arc::new(Self)
    }
}

#[async_trait]
impl AuthProvider for FakeAuth {
    fn id(&self) -> ProviderId {
        ProviderId::from("fake")
    }

    async fn start_device_flow(&self) -> anyhow::Result<DeviceStartResponse> {
        Ok(DeviceStartResponse {
            device_code: "fake-device-code-123".into(),
            user_code: "FAKE-CODE".into(),
            verification_uri: "http://localhost/fake-verify".into(),
            expires_in: 300,
            interval: 1, // Fast polling for CI
        })
    }

    async fn poll_device_flow(&self, device_code: &str) -> anyhow::Result<DevicePollOutcome> {
        if device_code == "fake-device-code-123" {
            Ok(DevicePollOutcome::Complete {
                identity: UserIdentity {
                    id: "fake-user-id-456".into(),
                    login: "ci-test-user".into(),
                    email: Some("ci@test.local".into()),
                    provider: "fake".into(),
                },
                provider_access_token: None,
            })
        } else {
            Ok(DevicePollOutcome::Pending)
        }
    }
}

// --- Factory Implementation ---

pub struct FakeFactory;

#[async_trait]
impl AuthProviderFactory for FakeFactory {
    fn id(&self) -> &'static str {
        "fake"
    }

    async fn build(self: Arc<Self>, state: &AppState) -> anyhow::Result<AuthProviderDyn> {
        if !state.config.enable_fake_auth {
            return Err(anyhow!("The 'fake' provider is not enabled on this server"));
        }
        Ok(FakeAuth::new())
    }
} 

########## ./backend/src/auth/orchid.rs

// backend/src/auth/orchid.rs

use std::sync::Arc;
use anyhow::anyhow;
use async_trait::async_trait;

use crate::auth::provider::{
    AuthProvider, AuthProviderDyn, AuthProviderFactory, DevicePollOutcome,
};
use crate::models::{DeviceStartResponse, ProviderId};
use crate::state::AppState;

// --- Provider Stub ---
#[derive(Debug)] // <-- ADDED THIS LINE
pub struct OrchidAuth;

// ... (rest of the file is unchanged) ...
#[async_trait]
impl AuthProvider for OrchidAuth {
    fn id(&self) -> ProviderId {
        ProviderId::from("orchid")
    }

    async fn start_device_flow(&self) -> anyhow::Result<DeviceStartResponse> {
        Err(anyhow!("Orchid device flow is not implemented yet"))
    }

    async fn poll_device_flow(&self, _device_code: &str) -> anyhow::Result<DevicePollOutcome> {
        Err(anyhow!("Orchid device flow is not implemented yet"))
    }
}

// --- Factory Stub ---
pub struct OrchidFactory;

#[async_trait]
impl AuthProviderFactory for OrchidFactory {
    fn id(&self) -> &'static str {
        "orchid"
    }

    async fn build(self: Arc<Self>, _state: &AppState) -> anyhow::Result<AuthProviderDyn> {
        Err(anyhow!("The 'orchid' provider is not configured on the server"))
    }
}

########## ./backend/src/auth/mod.rs

// backend/src/auth/mod.rs

pub mod provider;
pub mod github;
pub mod gitlab;
pub mod orchid;
pub mod fake;

use std::sync::Arc;
use crate::state::AppState;

/// Registers all the built-in authentication provider factories.
/// This is the *only* place that needs to be modified to add a new provider.
pub fn register_builtin_providers(state: &AppState) {
    state.register_provider_factory(Arc::new(github::GitHubFactory));
    state.register_provider_factory(Arc::new(fake::FakeFactory));
    state.register_provider_factory(Arc::new(gitlab::GitLabFactory));
    state.register_provider_factory(Arc::new(orchid::OrchidFactory));
}

########## ./backend/src/compute/tests.rs

use std::sync::{Arc, Mutex};
use std::path::PathBuf;
use std::process::ExitStatus;
use std::os::unix::process::ExitStatusExt;
use anyhow::Result;
use async_trait::async_trait;
use tokio::io::AsyncRead;

use crate::compute::local_provider::{CommandExecutor, LocalComputeProvider};
use crate::compute::ComputeProvider;
use crate::models::{Session, SessionRequest, SessionState};

#[derive(Debug, Clone)]
struct MockCommandCall {
    cmd: String,
    args: Vec<String>,
}

#[derive(Debug, Clone)]
struct MockCommandExecutor {
    calls: Arc<Mutex<Vec<MockCommandCall>>>,
    // Map of command -> (exit_code, stdout)
    responses: Arc<Mutex<Vec<(String, i32, String)>>>,
}

impl MockCommandExecutor {
    fn new() -> Self {
        Self {
            calls: Arc::new(Mutex::new(Vec::new())),
            responses: Arc::new(Mutex::new(Vec::new())),
        }
    }

    fn add_response(&self, cmd_contains: &str, exit_code: i32, stdout: &str) {
        self.responses.lock().unwrap().push((cmd_contains.to_string(), exit_code, stdout.to_string()));
    }

    fn get_calls(&self) -> Vec<MockCommandCall> {
        self.calls.lock().unwrap().clone()
    }
}

#[async_trait]
impl CommandExecutor for MockCommandExecutor {
    async fn run_status(&self, cmd: &str, args: &[&str]) -> Result<ExitStatus> {
        self.calls.lock().unwrap().push(MockCommandCall {
            cmd: cmd.to_string(),
            args: args.iter().map(|s| s.to_string()).collect(),
        });

        // Simple matching logic for now
        let responses = self.responses.lock().unwrap();
        for (contains, code, _) in responses.iter() {
            if cmd.contains(contains) || args.iter().any(|a| a.contains(contains)) {
                return Ok(ExitStatus::from_raw(*code));
            }
        }

        // Default success
        Ok(ExitStatus::from_raw(0))
    }

    async fn run_capture(&self, cmd: &str, args: &[&str]) -> Result<(u32, Box<dyn AsyncRead + Unpin + Send>, Box<dyn AsyncRead + Unpin + Send>)> {
        self.calls.lock().unwrap().push(MockCommandCall {
            cmd: cmd.to_string(),
            args: args.iter().map(|s| s.to_string()).collect(),
        });

        let responses = self.responses.lock().unwrap();
        let mut stdout_content = String::new();
        
        for (contains, _, stdout) in responses.iter() {
            if cmd.contains(contains) || args.iter().any(|a| a.contains(contains)) {
                stdout_content = stdout.clone();
                break;
            }
        }

        Ok((1234, Box::new(std::io::Cursor::new(stdout_content)), Box::new(std::io::Cursor::new(""))))
    }

    async fn run_shell(&self, script: &str) -> Result<ExitStatus> {
        self.calls.lock().unwrap().push(MockCommandCall {
            cmd: "sh".to_string(),
            args: vec!["-c".to_string(), script.to_string()],
        });

        let responses = self.responses.lock().unwrap();
        for (contains, code, _) in responses.iter() {
            if script.contains(contains) {
                return Ok(ExitStatus::from_raw(*code));
            }
        }

        Ok(ExitStatus::from_raw(0))
    }
}

#[tokio::test]
async fn test_start_session_success() {
    // Set temp session root for test environment
    let temp_root = std::env::temp_dir().join("steady-test-sessions");
    unsafe {
        std::env::set_var("STEADYSTATE_SESSION_ROOT", &temp_root);
    }

    let executor = Box::new(MockCommandExecutor::new());
    // Mock upterm output
    executor.add_response("upterm", 0, "Invite: ssh://user@host:22\n");

    let provider = LocalComputeProvider::new_with_executor(
        PathBuf::from("/tmp/flake"),
        executor.clone()
    );

    let mut session = Session {
        id: "test-session".into(),
        _repo_url: "https://github.com/user/repo".into(),
        _branch: None,
        _environment: None,
        compute_provider: "local".into(),
        _creator_login: "user1".into(),
        state: SessionState::Provisioning,
        endpoint: None,
        _created_at: std::time::SystemTime::now(),
        updated_at: std::time::SystemTime::now(),
        error_message: None,
        magic_link: None,
    };

    let request = SessionRequest {
        repo_url: "https://github.com/user/repo".into(),
        branch: None,
        environment: None,
        provider_config: None,
        allowed_users: None,
        public: false,
        mode: Some("pair".to_string()),
    };

    let result = provider.start_session(&session.id, &request).await.unwrap();
    session.state = SessionState::Running;
    session.endpoint = result.endpoint;
    session.magic_link = result.magic_link;

    assert!(matches!(session.state, SessionState::Running));
    assert_eq!(session.endpoint, Some("ssh://user@host:22".to_string()));

    let calls = executor.get_calls();
    // Verify sequence: check nix, (maybe install lix), clone, upterm
    // 1. check nix (sh -c command -v nix)
    // 2. clone repo (git clone ...)
    // 3. upterm (sh -c ... upterm ...)
    
    // Note: ensure_nix_installed checks for nix. If it returns success (default in mock), it skips install.
    
    assert!(calls.iter().any(|c| c.args.iter().any(|a| a.contains("command -v nix"))));
    assert!(calls.iter().any(|c| c.cmd == "git" && c.args[0] == "clone"));
    assert!(calls.iter().any(|c| c.args.iter().any(|a| a.contains("--authorized-keys"))));
    
    // Cleanup
    let _ = std::fs::remove_dir_all(temp_root);
    unsafe {
        std::env::remove_var("STEADYSTATE_SESSION_ROOT");
    }
}

#[tokio::test]
async fn test_terminate_session() {
    // Set temp session root for test environment
    let temp_root = std::env::temp_dir().join("steady-test-kill");
    unsafe {
        std::env::set_var("STEADYSTATE_SESSION_ROOT", &temp_root);
    }

    let executor = Box::new(MockCommandExecutor::new());
    let provider = LocalComputeProvider::new_with_executor(
        PathBuf::from("/tmp/flake"),
        executor.clone()
    );

    // Manually inject a session into state (this is tricky because state is private/internal)
    // But we can start a session first to populate state.
    
    // Mock upterm output for start_session
    executor.add_response("upterm", 0, "Invite: ssh://user@host:22\n");

    let mut session = Session {
        id: "test-session-kill".into(),
        _repo_url: "repo".into(),
        _branch: None,
        _environment: None,
        compute_provider: "local".into(),
        _creator_login: "user1".into(),
        state: SessionState::Provisioning,
        endpoint: None,
        _created_at: std::time::SystemTime::now(),
        updated_at: std::time::SystemTime::now(),
        error_message: None,
        magic_link: None,
    };
    let request = SessionRequest { 
        repo_url: "repo".into(),
        branch: None,
        environment: None,
        provider_config: None,
        allowed_users: None,
        public: false,
        mode: Some("pair".to_string()),
    };
    
    let result = provider.start_session(&session.id, &request).await.unwrap();
    session.state = SessionState::Running;
    session.endpoint = result.endpoint;

    // Now terminate
    provider.terminate_session(&session).await.unwrap();

    let calls = executor.get_calls();
    assert!(calls.iter().any(|c| c.args.iter().any(|a| a.contains("kill -TERM"))));
    
    // Cleanup
    let _ = std::fs::remove_dir_all(temp_root);
    unsafe {
        std::env::remove_var("STEADYSTATE_SESSION_ROOT");
    }
}

#[tokio::test]
async fn test_capture_upterm_invite_parsing() {
    let output = "=== BFO1HH1SZDG28RVDWPAM
Command:                bash
Force Command:          n/a
Host:                   ssh://uptermd.upterm.dev:22
Authorized Keys:        n/a
SSH Session:            ssh BFO1HH1sZDg28RvdWpam:MTc4MTFlNDBjZTk0ZTgudm0udXB0ZXJtLmludGVrbmFsOjIyMjI=@uptermd.upterm.dev

Run 'upterm session current' to display this screen again
";
    let cursor = Box::new(std::io::Cursor::new(output));
    let result = crate::compute::local_provider::capture_upterm_invite(cursor).await;
    assert!(result.is_ok());
    let (pid, invite, _remaining) = result.unwrap();
    assert_eq!(pid, None);
    assert_eq!(invite, "BFO1HH1sZDg28RvdWpam:MTc4MTFlNDBjZTk0ZTgudm0udXB0ZXJtLmludGVrbmFsOjIyMjI=@uptermd.upterm.dev");
}

#[tokio::test]
async fn test_capture_upterm_invite_parsing_new_format() {
    let output = "
â”‚ Command:         â”‚ bash -c echo 'Session started'; sleep 30             â”‚
â”‚ Force Command:   â”‚ n/a                                                  â”‚
â”‚ Host:            â”‚ ssh://uptermd.upterm.dev:22                          â”‚
â”‚ Authorized Keys: â”‚ /tmp/secure_keys_direct/authorized_keys:             â”‚
â”‚                  â”‚ - SHA256:z6jj++GQTO4xVebN4yTQXD5msN1o1XncTqPzK+Cy4rk â”‚
â”‚                  â”‚                                                      â”‚
â”‚ âž¤ SSH Command:   â”‚ ssh FYjNpo96BizkITTpFfco@uptermd.upterm.dev          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
";
    let cursor = Box::new(std::io::Cursor::new(output));
    let result = crate::compute::local_provider::capture_upterm_invite(cursor).await;
    assert!(result.is_ok());
    let (pid, invite, _remaining) = result.unwrap();
    assert_eq!(pid, None);
    assert_eq!(invite, "FYjNpo96BizkITTpFfco@uptermd.upterm.dev");
}

#[tokio::test]
async fn test_start_session_git_failure() {
    let executor = Box::new(MockCommandExecutor::new());
    // Mock git clone failure
    executor.add_response("git", 1, "fatal: repository not found");

    let provider = LocalComputeProvider::new_with_executor(
        PathBuf::from("/tmp/flake"),
        executor.clone()
    );

    let session = Session {
        id: "test-session-git-fail".into(),
        _repo_url: "https://github.com/user/repo".into(),
        _branch: None,
        _environment: None,
        compute_provider: "local".into(),
        _creator_login: "user1".into(),
        state: SessionState::Provisioning,
        endpoint: None,
        _created_at: std::time::SystemTime::now(),
        updated_at: std::time::SystemTime::now(),
        error_message: None,
        magic_link: None,
    };

    let request = SessionRequest {
        repo_url: "https://github.com/user/repo".into(),
        branch: None,
        environment: None,
        provider_config: None,
        allowed_users: None,
        public: false,
        mode: Some("pair".to_string()),
    };

    let result = provider.start_session(&session.id, &request).await;
    assert!(result.is_err());
    // The provider does not update state on failure (caller does), so it should remain Provisioning
    assert!(matches!(session.state, SessionState::Provisioning));
}

#[tokio::test]
async fn test_start_session_upterm_failure() {
    let executor = Box::new(MockCommandExecutor::new());
    // Mock upterm failure (non-zero exit code)
    executor.add_response("upterm", 1, "Error: failed to start session");

    let provider = LocalComputeProvider::new_with_executor(
        PathBuf::from("/tmp/flake"),
        executor.clone()
    );

    let session = Session {
        id: "test-session-upterm-fail".into(),
        _repo_url: "https://github.com/user/repo".into(),
        _branch: None,
        _environment: None,
        compute_provider: "local".into(),
        _creator_login: "user1".into(),
        state: SessionState::Provisioning,
        endpoint: None,
        _created_at: std::time::SystemTime::now(),
        updated_at: std::time::SystemTime::now(),
        error_message: None,
        magic_link: None,
    };

    let request = SessionRequest {
        repo_url: "https://github.com/user/repo".into(),
        branch: None,
        environment: None,
        provider_config: None,
        allowed_users: None,
        public: false,
        mode: Some("pair".to_string()),
    };

    let result = provider.start_session(&session.id, &request).await;
    assert!(result.is_err());
    assert!(matches!(session.state, SessionState::Provisioning));
}

#[tokio::test]
async fn test_start_session_nix_failure() {
    let executor = Box::new(MockCommandExecutor::new());
    // Mock nix check failure
    executor.add_response("command -v nix", 1, "");
    // And mock installer failure too, otherwise it tries to install
    executor.add_response("sh", 1, "Installer failed"); 

    let provider = LocalComputeProvider::new_with_executor(
        PathBuf::from("/tmp/flake"),
        executor.clone()
    );

    let session = Session {
        id: "test-session-nix-fail".into(),
        _repo_url: "https://github.com/user/repo".into(),
        _branch: None,
        _environment: None,
        compute_provider: "local".into(),
        _creator_login: "user1".into(),
        state: SessionState::Provisioning,
        endpoint: None,
        _created_at: std::time::SystemTime::now(),
        updated_at: std::time::SystemTime::now(),
        error_message: None,
        magic_link: None,
    };

    let request = SessionRequest {
        repo_url: "https://github.com/user/repo".into(),
        branch: None,
        environment: None,
        provider_config: None,
        allowed_users: None,
        public: false,
        mode: Some("pair".to_string()),
    };

    let result = provider.start_session(&session.id, &request).await;
    assert!(result.is_err());
    assert!(matches!(session.state, SessionState::Provisioning));
}

########## ./backend/src/compute/local_provider.rs

// backend/src/compute/local_provider.rs

use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};

use anyhow::{anyhow, Context, Result};
use async_trait::async_trait;
use dashmap::DashMap;
use futures::StreamExt;
use std::os::unix::fs::PermissionsExt;
use tokio::io::{AsyncBufReadExt, BufReader, AsyncReadExt};
use tokio::process::Command;
use tokio::time::{timeout, Duration};
use tracing::instrument;


use crate::compute::ComputeProvider;
use crate::models::{Session, SessionRequest, SessionState};

#[async_trait::async_trait]
pub trait CommandExecutor: Send + Sync + std::fmt::Debug {
    async fn run_status(&self, cmd: &str, args: &[&str]) -> Result<std::process::ExitStatus>;
    async fn run_capture(&self, cmd: &str, args: &[&str]) -> Result<(u32, Box<dyn tokio::io::AsyncRead + Unpin + Send>, Box<dyn tokio::io::AsyncRead + Unpin + Send>)>;
    async fn run_shell(&self, script: &str) -> Result<std::process::ExitStatus>;
}

#[derive(Debug, Clone)]
pub struct RealCommandExecutor;

#[async_trait::async_trait]
impl CommandExecutor for RealCommandExecutor {
    async fn run_status(&self, cmd: &str, args: &[&str]) -> Result<std::process::ExitStatus> {
        Command::new(cmd)
            .args(args)
            .stdin(std::process::Stdio::null())
            .process_group(0)
            .status()
            .await
            .context(format!("Failed to execute {}", cmd))
    }

    async fn run_capture(&self, cmd: &str, args: &[&str]) -> Result<(u32, Box<dyn tokio::io::AsyncRead + Unpin + Send>, Box<dyn tokio::io::AsyncRead + Unpin + Send>)> {
        let mut c = Command::new(cmd);
        c.args(args);
        c.stdin(std::process::Stdio::null());
        c.process_group(0);
        c.stdout(std::process::Stdio::piped());
        c.stderr(std::process::Stdio::piped());

        let mut child = c.spawn().context(format!("Failed to spawn {}", cmd))?;
        let pid = child.id().ok_or_else(|| anyhow!("Failed to get PID"))?;
        let stdout = child.stdout.take().ok_or_else(|| anyhow!("Failed to capture stdout"))?;
        let stderr = child.stderr.take().ok_or_else(|| anyhow!("Failed to capture stderr"))?;
        
        Ok((pid, Box::new(stdout), Box::new(stderr)))
    }

    async fn run_shell(&self, script: &str) -> Result<std::process::ExitStatus> {
        Command::new("sh")
            .arg("-c")
            .arg(script)
            .stdin(std::process::Stdio::null())
            .process_group(0)
            .status()
            .await
            .context("Failed to execute shell script")
    }
}

#[derive(Debug)]
struct LocalSession {
    pid: u32,
    event_daemon_pid: Option<u32>,
    workspace_root: PathBuf,
}

#[derive(Debug, Default)]
pub struct LocalProviderState {
    live_sessions: DashMap<String, LocalSession>,
}

#[derive(Debug)]
pub struct LocalComputeProvider {
    flake_path: PathBuf,
    state: Arc<LocalProviderState>,
    executor: Box<dyn CommandExecutor>,
}

impl LocalComputeProvider {
    pub fn new(flake_path: PathBuf) -> Self {
        Self {
            flake_path,
            state: Arc::new(LocalProviderState::default()),
            executor: Box::new(RealCommandExecutor),
        }
    }

    /// Constructor for testing with a mock executor
    pub fn new_with_executor(flake_path: PathBuf, executor: Box<dyn CommandExecutor>) -> Self {
        Self {
            flake_path,
            state: Arc::new(LocalProviderState::default()),
            executor,
        }
    }

    fn create_workspace(&self, session_id: &str) -> Result<(PathBuf, PathBuf)> {
        // ------------------------------------------------------------
        // FIX: Never put SSHD or session files under /tmp.
        // OpenSSH refuses host keys or configs under any insecure path.
        // Move session root to: $HOME/.steadystate/sessions/<session_id>
        // ------------------------------------------------------------

        let home = dirs::home_dir()
            .ok_or_else(|| anyhow!("No HOME directory found"))?;

        let base_root = std::env::var("STEADYSTATE_SESSION_ROOT")
            .map(PathBuf::from)
            .unwrap_or_else(|_| home.join(".steadystate").join("sessions"));

        // Ensure two-level dirs (~/.steadystate/sessions) exist and secure
        std::fs::create_dir_all(&base_root)
            .with_context(|| format!("Failed to create {:?}", base_root))?;
        
        use std::os::unix::fs::PermissionsExt;
        std::fs::set_permissions(&base_root, std::fs::Permissions::from_mode(0o700))
            .context("Failed to chmod session root directory")?;

        // Create this session's directory
        let base = base_root.join(session_id);
        std::fs::create_dir_all(&base)
            .with_context(|| format!("Failed to create {:?}", base))?;
        std::fs::set_permissions(&base, std::fs::Permissions::from_mode(0o700))
            .context("Failed to chmod session directory")?;

        // Repo path inside session
        let repo_path = base.join("repo");
        std::fs::create_dir_all(&repo_path)
            .with_context(|| format!("Failed to create repo dir at {}", repo_path.display()))?;
        std::fs::set_permissions(&repo_path, std::fs::Permissions::from_mode(0o700))
            .context("Failed to chmod repo directory")?;

        Ok((base, repo_path))
    }

    async fn ensure_nix_installed(&self) -> Result<()> {
        // 1. Check if nix is already in PATH
        let status = self.executor.run_status("sh", &["-c", "command -v nix >/dev/null 2>&1"]).await
            .context("Failed to check for nix")?;

        if status.success() {
            return Ok(());
        }

        tracing::info!("Nix not found; installing Lix...");
        
        // 2. Install Lix non-interactively
        let install_cmd = r#"curl --proto '=https' --tlsv1.2 -sSf -L https://install.lix.systems/lix | sh -s -- install --no-confirm"#;
        let status = self.executor.run_shell(install_cmd).await
            .context("Failed to spawn Lix installer")?;

        if !status.success() {
            return Err(anyhow!("Lix installer failed"));
        }
        
        tracing::info!("Lix installation completed successfully");
        Ok(())
    }



    async fn init_canonical_git_repo(&self, session_root: &Path, git_repo: &Path, session_id: &str) -> Result<PathBuf> {
        let canonical_path = session_root.join("canonical");
        tracing::info!("Initializing canonical Git repository at {}", canonical_path.display());
        
        // Create bare clone from git_repo
        let git_repo_str = git_repo.to_str().ok_or_else(|| anyhow!("Invalid path encoding"))?;
        let canonical_str = canonical_path.to_str().ok_or_else(|| anyhow!("Invalid path encoding"))?;
        
        // 1. Clone non-bare (so we have a working tree for sync operations)
        let status = self.executor.run_status("git", &["clone", git_repo_str, canonical_str]).await
            .context("Failed to create canonical git repo")?;
            
        if !status.success() {
            return Err(anyhow!("git clone failed"));
        }

        // 2. Create and checkout session branch
        let branch_name = format!("steadystate/collab/{}", session_id);
        // git -C canonical checkout -b <branch>
        let status = self.executor.run_status("git", &["-C", canonical_str, "checkout", "-b", &branch_name]).await
            .context("Failed to create session branch")?;

        if !status.success() {
             return Err(anyhow!("Failed to create session branch {}", branch_name));
        }
        
        // No need to manually update HEAD or symbolic-ref, checkout -b does it.
        
        Ok(canonical_path)
    }

    fn install_steadystate_commands(&self, session_root: &Path, repo_name: &str) -> Result<()> {
        let bin_dir = session_root.join("bin");
        std::fs::create_dir_all(&bin_dir).context("Failed to create bin directory")?;
        
        // 1. Copy steadystate binary as steadystate-cli
        let current_exe = std::env::current_exe().context("Failed to get current executable path")?;
        let target = bin_dir.join("steadystate-cli");
        std::fs::copy(&current_exe, &target).context("Failed to copy steadystate binary")?;
        
        // 2. Create sync script
        let sync_script = r#"#!/bin/bash
set -e

USER_ID="${STEADYSTATE_USERNAME:-${USER:-unknown}}"
# Default to PWD if USER_WORKSPACE not set (fallback)
WORKSPACE="${USER_WORKSPACE:-$PWD}"
# We need to find session root if not set
if [ -z "$SESSION_ROOT" ]; then
    # Try to deduce from workspace path (assuming /tmp/steadystate/sessions/{id}/{user})
    SESSION_ROOT=$(dirname "$WORKSPACE")
fi

CANONICAL="${CANONICAL_REPO:-$SESSION_ROOT/canonical}"
SYNC_LOG="${SYNC_LOG:-$SESSION_ROOT/sync-log}"

log_activity() {
    local action="$1"
    if [ -f "$SYNC_LOG" ]; then
        echo "$(date -Iseconds),$USER_ID,$action" >> "$SYNC_LOG"
    fi
}

echo "Syncing changes..."
log_activity "syncing"

cd "$WORKSPACE"

# 1. Check for changes
if [ -n "$(git status --porcelain)" ]; then
    # Record local changes
    git add -A
    git commit -m "Auto-sync by $USER_ID" --author "$USER_ID <$USER_ID@steadystate.local>"
    echo "âœ“ Recorded your changes"
    log_activity "recorded"
fi

# 2. Pull from canonical (rebase)
echo "Pulling changes from collaborators..."
if ! git pull --rebase canonical HEAD >/dev/null 2>&1; then
    echo "Warning: Pull failed (conflict?), checking..."
fi

# 3. Check for conflicts
if [ -f .git/rebase-merge/git-rebase-todo ] || [ -d .git/rebase-apply ]; then
     echo ""
    echo "âš ï¸  MERGE CONFLICTS DETECTED"
    echo ""
    echo "Please resolve conflicts and run 'git rebase --continue'"
    # TODO: Better conflict handling?
    # For now, just exit
    exit 1
fi

# 4. Push to canonical
if git push canonical HEAD >/dev/null 2>&1; then
    echo "âœ“ Pushed to canonical repository"
fi

# 5. Log sync completion
log_activity "synced"
echo ""
echo "âœ“ Sync complete!"
"#;

        let sync_path = bin_dir.join("steadystate-sync");
        std::fs::write(&sync_path, sync_script).context("Failed to write sync script")?;
        
        // Make executable
        use std::os::unix::fs::PermissionsExt;
        std::fs::set_permissions(&sync_path, std::fs::Permissions::from_mode(0o755))?;

        // 2.5 Create initial sync-log
        let sync_log_path = session_root.join("sync-log");
        if !sync_log_path.exists() {
             let timestamp = std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs();
             let initial_log = format!("{} system Session started\n", timestamp);
             std::fs::write(&sync_log_path, initial_log).context("Failed to create initial sync-log")?;
             std::fs::set_permissions(&sync_log_path, std::fs::Permissions::from_mode(0o644))?;
        }
        
        // 3. Create wrapper script (steadystate)
        let wrapper_script = format!(r#"#!/bin/bash
# Export REPO_ROOT so CLI knows where to look for sync-log
export REPO_ROOT="$(dirname "$PWD")"
export REPO_NAME="{}"

case "$1" in
    sync)
        exec steadystate-sync
        ;;
    diff)
        git diff
        ;;
    status)
        git status
        ;;
    finalize)
        # TODO: Implement finalize
        echo "Finalize not implemented yet"
        ;;
    *)
        # Fallback to real CLI or error
        if command -v steadystate-cli >/dev/null; then
            exec steadystate-cli "$@"
        else
            echo "Unknown command: $1"
            echo "Available: sync, diff, status"
            exit 1
        fi
        ;;
esac
"#, repo_name);
        let wrapper_path = bin_dir.join("steadystate");
        std::fs::write(&wrapper_path, wrapper_script)?;
        std::fs::set_permissions(&wrapper_path, std::fs::Permissions::from_mode(0o755))?;
        

        Ok(())
    }


    async fn clone_repo(&self, repo_url: &str, dest: &Path) -> Result<()> {
        tracing::info!("Cloning repo {} into {}", repo_url, dest.display());
        
        let dest_str = dest.to_str().ok_or_else(|| anyhow!("Invalid path encoding for dest"))?;
        let status = self.executor.run_status("git", &["clone", "--depth=1", repo_url, dest_str]).await
            .context("Failed to spawn git clone")?;

        if !status.success() {
            return Err(anyhow!("git clone failed for {}", repo_url));
        }
        Ok(())
    }

    async fn launch_upterm_in_noenv(
        &self,
        flake_path: &Path,
        working_dir: &Path,
        github_user: Option<&str>,
        allowed_users: Option<&[String]>,
        public: bool,
        environment: Option<&str>,
        session_id: &str,
        github_token: Option<&str>,
    ) -> Result<(u32, String)> {
        // We run upterm from the host (backend environment), which wraps the nix develop session.
        // This ensures upterm is found (since it's in the host env) and the user lands in the nix dev shell.
        
        // The command that upterm will run *inside* the session.
        // We want to run this command in the `working_dir`.
        // We will handle the directory change in the outer wrapper script.
        
        // We use shell_escape to safely insert paths into the shell string.
        let safe_workdir = shell_escape::escape(working_dir.to_string_lossy().into());
        // let safe_flake = shell_escape::escape(flake_path.to_string_lossy().into());
        
        // Choose flake/nix file based on --env flag
        let env_str = environment.unwrap_or("flake"); // Should be enforced by CLI, but default to flake
        
        let (cmd_prog, cmd_args) = if env_str == "noenv" {
            // Use curated minimal environment
            tracing::info!("Using --env=noenv: minimal curated environment");
            (
                "nix",
                vec![
                    "develop".to_string(),
                    "github:The-Exact-Computing-Company/steadystate?dir=backend/flakes/noenv".to_string(),
                    "--command".to_string(),
                    "bash".to_string(),
                ],
            )
        } else if env_str == "flake" {
            // Use repository's own flake
            tracing::info!("Using --env=flake: repository's flake.nix");
            let path_str = flake_path.to_string_lossy();
            let flake_ref = if path_str.starts_with("github:") {
                path_str.to_string()
            } else {
                // For local testing: point to repo's flake
                // Since we cd into working_dir in the wrapper script, we can just use "."
                ".".to_string()
            };
            (
                "nix",
                vec![
                    "develop".to_string(),
                    flake_ref,
                    "--command".to_string(),
                    "bash".to_string(),
                ],
            )
        } else if env_str.starts_with("legacy-nix") {
             // Handle legacy-nix (nix-shell)
             let filename = if env_str == "legacy-nix" {
                 "default.nix"
             } else {
                 // Parse legacy-nix[filename]
                 let start = env_str.find('[').unwrap_or(0) + 1;
                 let end = env_str.find(']').unwrap_or(env_str.len());
                 &env_str[start..end]
             };
             
             tracing::info!("Using --env={}: nix-shell {}", env_str, filename);
             (
                 "nix-shell",
                 vec![
                     filename.to_string(),
                     "--command".to_string(),
                     "bash".to_string(),
                 ],
             )
        } else {
            // Fallback (should be caught by CLI)
            tracing::warn!("Unknown environment: {}, defaulting to flake", env_str);
             (
                "nix",
                vec![
                    "develop".to_string(),
                    ".".to_string(),
                    "--command".to_string(),
                    "bash".to_string(),
                ],
            )
        };

        // Construct upterm host command
        let mut upterm_args = vec!["host".to_string()];
        
        // 1. Server configuration
        if let Ok(server) = std::env::var("STEADYSTATE_UPTERM_SERVER") {
            upterm_args.push("--server".to_string());
            upterm_args.push(server);
        }

        // Always accept connections automatically since we are running headless
        upterm_args.push("--accept".to_string());

        // 2. Authorization
        let authorized_keys = fetch_authorized_keys(github_user, allowed_users, github_token).await;
        
        // Write keys to a temporary file
        let key_file_path = format!("/tmp/steadystate_authorized_keys_{}", session_id);
        
        if let Ok(mut file) = std::fs::File::create(&key_file_path) {
            use std::os::unix::fs::PermissionsExt;
            let mut perms = file.metadata()?.permissions();
            perms.set_mode(0o600);
            let _ = file.set_permissions(perms);
            
            use std::io::Write;
            for ak in &authorized_keys {
                if let Err(e) = writeln!(file, "{}", ak.key) {
                    tracing::error!("Failed to write key: {}", e);
                }
            }
        }
        
        upterm_args.push("--authorized-keys".to_string());
        upterm_args.push(key_file_path);

        // 5. Host command
        // upterm host [flags] -- <command> [args...]
        upterm_args.push("--".to_string());
        upterm_args.push(cmd_prog.to_string());
        for arg in cmd_args {
            upterm_args.push(arg);
        }

        tracing::info!("Upterm args: github_user={:?}, allowed_users={:?}, public={}", github_user, allowed_users, public);
        
        // Use setsid to detach from TTY and avoid SIGTTIN when running in background.
        // We use `setsid` to create a new session.
        // We pipe `tail -f /dev/null` to `upterm` to keep it alive (prevent EOF on stdin).
        // We use a wrapper shell script to handle the pipe and setsid while passing arguments safely.
        // We also cd into the working directory here.
        let wrapper_script = r#"
            if [ -f /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh ]; then
              . /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh
            fi
            unset SSH_ASKPASS
            unset DISPLAY
            unset SSH_AUTH_SOCK
            
            WORK_DIR="$1"
            shift
            
            cd "$WORK_DIR" || exit 1
            
            # Execute setsid with a helper shell that sets up the pipe and executes the args
            # "$@" contains the upterm command and its arguments
            exec setsid sh -c 'echo "PID: $$"; tail -f /dev/null | exec "$@"' -- "$@"
        "#;
        
        // Convert args to &str
        let upterm_args_str: Vec<&str> = upterm_args.iter().map(|s| s.as_str()).collect();
        
        // Construct the full arguments for the outer sh
        // sh -c script -- workdir upterm [args...]
        let mut full_args = vec!["-c", wrapper_script, "--", safe_workdir.as_ref(), "upterm"];
        full_args.extend(upterm_args_str);
        
        tracing::info!("Spawning upterm command via wrapper...");
        let (wrapper_pid, stdout, stderr) = self.executor.run_capture("sh", &full_args).await
            .context("Failed to spawn upterm wrapper")?;

        tracing::info!("Upterm wrapper spawned with PID {}", wrapper_pid);

        // Spawn a task to log stderr
        let stderr_reader = BufReader::new(stderr);
        tokio::spawn(async move {
            let mut lines = tokio_stream::wrappers::LinesStream::new(stderr_reader.lines());
            while let Some(line_res) = lines.next().await {
                match line_res {
                    Ok(line) => eprintln!("UPTERM STDERR: {}", line),
                    Err(e) => eprintln!("Error reading upterm stderr: {}", e),
                }
            }
        });

        // We wait for the invite link to appear in stdout.
        let invite_result = timeout(Duration::from_secs(30), capture_upterm_invite(stdout)).await;

        match invite_result {
            Ok(Ok((captured_pid, invite, remaining_stdout))) => {
                // Spawn a task to continue draining stdout to prevent SIGPIPE
                tokio::spawn(async move {
                    let mut lines = tokio_stream::wrappers::LinesStream::new(remaining_stdout.lines());
                    while let Some(line_res) = lines.next().await {
                        match line_res {
                            Ok(line) => tracing::debug!(upterm_stdout = %line, "upterm stdout (post-invite)"),
                            Err(_) => break,
                        }
                    }
                });
                
                // If we captured a PID from setsid, use it. Otherwise fall back to wrapper PID (less reliable for kill).
                let final_pid = captured_pid.unwrap_or(wrapper_pid);
                tracing::info!("Session ready. Wrapper PID: {}, Final PID: {}", wrapper_pid, final_pid);
                Ok((final_pid, invite))
            },
            Ok(Err(e)) => {
                tracing::error!("Upterm failed to provide invite: {:#}", e);
                let _ = self.kill_pid(wrapper_pid).await;
                Err(e)
            }
            Err(_) => {
                tracing::error!("Timed out waiting for upterm invite");
                let _ = self.kill_pid(wrapper_pid).await;
                Err(anyhow!("Timed out waiting for upterm invite"))
            }
        }
    }

    async fn kill_pid(&self, pid: u32) -> Result<()> {
        tracing::info!("Killing local session process with pid={}", pid);
        let cmd = format!("kill -TERM {}", pid);
        let status = self.executor.run_shell(&cmd).await
            .context("Failed to spawn kill")?;
            
        if !status.success() {
            tracing::warn!("kill returned non-zero for pid={}", pid);
        }
        Ok(())
    }
}









pub(crate) async fn capture_upterm_invite(stdout: impl tokio::io::AsyncRead + Unpin + Send + 'static) -> Result<(Option<u32>, String, BufReader<Box<dyn tokio::io::AsyncRead + Unpin + Send>>)> {
    let reader = BufReader::new(stdout);
    let mut lines = tokio_stream::wrappers::LinesStream::new(reader.lines());

    let mut pid = None;

    while let Some(line_res) = lines.next().await {
        let line = line_res?;
        tracing::warn!(upterm_stdout = %line, "upterm stdout");
        let trimmed = line.trim();
        
        if trimmed.starts_with("PID: ") {
            if let Ok(p) = trimmed.strip_prefix("PID: ").unwrap_or("").trim().parse::<u32>() {
                tracing::info!("Captured upterm PID: {}", p);
                pid = Some(p);
            }
        } else if trimmed.starts_with("Invite: ") {
            let invite = trimmed.strip_prefix("Invite: ").unwrap_or(trimmed).trim();
            tracing::info!("Captured upterm invite: {}", invite);
            let remaining: Box<dyn tokio::io::AsyncRead + Unpin + Send> = Box::new(lines.into_inner().into_inner());
            return Ok((pid, invite.to_string(), BufReader::new(remaining)));
        } else if trimmed.starts_with("SSH Session:") {
            // Format: "SSH Session:            ssh ..."
            if let Some(ssh_cmd) = trimmed.strip_prefix("SSH Session:") {
                let ssh_cmd = ssh_cmd.trim();
                let address = ssh_cmd.strip_prefix("ssh ").unwrap_or(ssh_cmd).trim();
                tracing::info!("Captured upterm invite: {}", address);
                let remaining: Box<dyn tokio::io::AsyncRead + Unpin + Send> = Box::new(lines.into_inner().into_inner());
                return Ok((pid, address.to_string(), BufReader::new(remaining)));
            }
        } else if line.contains("SSH Command:") {
            // New format (v0.18.0+): "â”‚ âž¤ SSH Command:   â”‚ ssh ... â”‚"
            // We look for "ssh " and take everything until the next "â”‚" or end of line
            if let Some(idx) = line.find("ssh ") {
                let rest = &line[idx..];
                // If there's a trailing "â”‚", strip it
                let ssh_cmd = if let Some(end_idx) = rest.find('â”‚') {
                    rest[..end_idx].trim()
                } else {
                    rest.trim()
                };
                let address = ssh_cmd.strip_prefix("ssh ").unwrap_or(ssh_cmd).trim();
                tracing::info!("Captured upterm invite (new format): {}", address);
                let remaining: Box<dyn tokio::io::AsyncRead + Unpin + Send> = Box::new(lines.into_inner().into_inner());
                return Ok((pid, address.to_string(), BufReader::new(remaining)));
            }
        }
    }
    Err(anyhow!("Upterm did not print an invite line"))
}



#[derive(Debug, serde::Deserialize)]
struct GitHubComputeConfig {
    login: String,
    access_token: String,
}

fn parse_github_repo(repo_url: &str) -> Option<(String, String)> {
    // Supports https://github.com/owner/repo(.git)
    let url = repo_url.strip_prefix("https://github.com/")?;
    let url = url.strip_suffix(".git").unwrap_or(url);
    let mut parts = url.split('/');
    let owner = parts.next()?.to_string();
    let repo = parts.next()?.to_string();
    Some((owner, repo))
}

async fn ensure_fork_and_clone(
    http: &reqwest::Client,
    gh: &GitHubComputeConfig,
    original_repo_url: &str,
    dest: &Path,
    executor: &dyn CommandExecutor,
) -> Result<()> {
    let (owner, repo) = parse_github_repo(original_repo_url)
        .ok_or_else(|| anyhow!("Unsupported GitHub repo URL: {}", original_repo_url))?;

    let user = &gh.login;

    // 1. Check if fork already exists
    let fork_url = format!("https://api.github.com/repos/{}/{}", user, repo);
    let resp = http
        .get(&fork_url)
        .bearer_auth(&gh.access_token)
        .header("User-Agent", "steadystate-backend/0.1")
        .send()
        .await?;

    if resp.status() == reqwest::StatusCode::NOT_FOUND {
        // 2. Create fork
        let create_url = format!("https://api.github.com/repos/{}/{}", owner, repo);
        tracing::info!("Creating fork of {}/{} for {}", owner, repo, user);

        let fork_resp = http
            .post(format!("{}/forks", create_url))
            .bearer_auth(&gh.access_token)
            .header("User-Agent", "steadystate-backend/0.1")
            .send()
            .await?
            .error_for_status()
            .context("Failed to create fork via GitHub API")?;

        tracing::debug!("Fork response: {:?}", fork_resp.status());

        // Simple backoff: give GitHub a moment to materialize the fork
        tokio::time::sleep(std::time::Duration::from_secs(3)).await;
    }

    // 3. Clone the *fork* into dest
    // Inject token into URL for authenticated clone
    let fork_clone_url = format!("https://x-access-token:{}@github.com/{}/{}.git", gh.access_token, user, repo);
    tracing::info!("Cloning fork {} into {}", fork_clone_url.replace(&gh.access_token, "***"), dest.display());
    
    let dest_str = dest.to_str().ok_or_else(|| anyhow!("Invalid path encoding for dest"))?;
    let status = executor.run_status("git", &["clone", "--depth=1", &fork_clone_url, dest_str]).await
        .context("Failed to spawn git clone")?;

    if !status.success() {
        return Err(anyhow!("git clone failed for {}", fork_clone_url.replace(&gh.access_token, "***")));
    }

    // 4. Add upstream remote pointing at the original repo (best effort)
    let upstream_url = format!("https://github.com/{}/{}.git", owner, repo);
    let status = executor.run_status("git", &["-C", dest_str, "remote", "add", "upstream", &upstream_url]).await
        .context("Failed to add upstream remote")?;

    if !status.success() {
        tracing::warn!("git remote add upstream failed for {}", dest.display());
    }

    Ok(())
}

#[async_trait::async_trait]
impl ComputeProvider for LocalComputeProvider {
    fn id(&self) -> &'static str { "local" }

    async fn start_session(&self, session_id: &str, request: &SessionRequest) -> Result<crate::models::SessionStartResult> {
        eprintln!("DEBUG: start_session CALLED for session {}", session_id);
        
        tracing::info!("Starting local session: id={} repo={}", session_id, request.repo_url);

        // Check mode
        if let Some(mode) = &request.mode {
            if mode != "pair" && mode != "collab" {
                return Err(anyhow!("Invalid mode: {}", mode));
            }
        }
        
        self.ensure_nix_installed().await?;

        let (workspace_root, repo_path) = self.create_workspace(session_id)?;
        
        // --- GitHub Fork/Clone Logic ---
        if let Some(cfg) = &request.provider_config {
            if let Some(gh_val) = cfg.get("github") {
                if let Ok(gh) = serde_json::from_value::<GitHubComputeConfig>(gh_val.clone()) {
                    let http = reqwest::Client::new();
                    ensure_fork_and_clone(&http, &gh, &request.repo_url, &repo_path, self.executor.as_ref()).await?;
                } else {
                    self.clone_repo(&request.repo_url, &repo_path).await?;
                }
            } else {
                self.clone_repo(&request.repo_url, &repo_path).await?;
            }
        } else {
            self.clone_repo(&request.repo_url, &repo_path).await?;
        }
        
        // Extract GitHub login and token if available
        let mut github_login = None;
        let mut github_token = None;
        if let Some(cfg) = &request.provider_config {
             if let Some(gh_val) = cfg.get("github") {
                if let Ok(gh) = serde_json::from_value::<GitHubComputeConfig>(gh_val.clone()) {
                    github_login = Some(gh.login);
                    github_token = Some(gh.access_token);
                }
             }
        }

        // Determine allowed users
        let mut allowed_users_list = request.allowed_users.clone();
        
        // If no allowed users specified (and not explicitly "none"), default to all collaborators
        if allowed_users_list.is_none() {
            if let Some(token) = &github_token {
                let mut repo_name = request.repo_url.split('/').last().unwrap_or("unknown");
                if repo_name.ends_with(".git") {
                    repo_name = &repo_name[..repo_name.len() - 4];
                }
                let owner = request.repo_url.split('/').nth_back(1).unwrap_or("unknown");
                
                if owner != "unknown" && repo_name != "unknown" {
                    match fetch_github_collaborators(owner, repo_name, token).await {
                        Ok(collaborators) => {
                            allowed_users_list = Some(collaborators);
                        },
                        Err(e) => {
                            tracing::warn!("Failed to fetch collaborators: {}. Defaulting to host only.", e);
                        }
                    }
                }
            }
        } else if let Some(users) = &allowed_users_list {
            // If "none" is specified, clear the list (host only)
            if users.contains(&"none".to_string()) {
                allowed_users_list = Some(Vec::new());
            }
        }

        // 4. Initialize Canonical Git Repo (if collab mode)
        if request.mode.as_deref() == Some("collab") {
             self.init_canonical_git_repo(&workspace_root, &repo_path, session_id).await?;
             
             let real_repo_name = request.repo_url.split('/').last()
                .map(|s| s.trim_end_matches(".git"))
                .unwrap_or("unknown");
                
             self.install_steadystate_commands(&workspace_root, real_repo_name)?;

            let (pid, invite) = self.launch_sshd_for_collab(
                &workspace_root,
                github_login.as_deref(),
                allowed_users_list.as_deref(),
                session_id,
                real_repo_name,
                github_token.as_deref(),
            ).await?;

            let event_daemon_pid = self.start_event_daemon(&workspace_root).await?;

            // Store session info in local state
            self.state.live_sessions.insert(session_id.to_string(), LocalSession {
                pid,
                event_daemon_pid,
                workspace_root: workspace_root.clone(),
            });

            // 6. Generate Result
            let endpoint = if invite.starts_with("ssh://") {
                invite.clone()
            } else {
                format!("ssh://{}", invite)
            };
            
            // Generate Magic Link
            let magic_link = if let Some(mode) = &request.mode {
                let mut url = url::Url::parse(&format!("steadystate://{}", mode)).unwrap();
                
                if mode == "pair" {
                    url.query_pairs_mut().append_pair("upterm", &endpoint);
                } else if mode == "collab" {
                    url.query_pairs_mut().append_pair("ssh", &endpoint);
                }
                Some(url.to_string())
            } else {
                None
            };

            Ok(crate::models::SessionStartResult {
                endpoint: Some(endpoint),
                magic_link,
            })
        } else {
            // Legacy/Pair mode (Upterm)
            
            // Launch Upterm
             let (pid, invite) = self.launch_upterm_in_noenv(
                &self.flake_path,
                &repo_path,
                github_login.as_deref(),
                allowed_users_list.as_deref(),
                request.public,
                request.environment.as_deref(),
                session_id,
                github_token.as_deref(),
            ).await?;

            let local_session = LocalSession {
                pid,
                event_daemon_pid: None,
                workspace_root: repo_path,
            };
            self.state.live_sessions.insert(session_id.to_string(), local_session);

            let endpoint = if invite.starts_with("ssh://") {
                invite.clone()
            } else {
                format!("ssh://{}", invite)
            };
            
            // Generate Magic Link for Pair
            let magic_link = format!("steadystate://pair/{}?ssh={}", session_id, urlencoding::encode(&invite));
            
            Ok(crate::models::SessionStartResult {
                endpoint: Some(endpoint),
                magic_link: Some(magic_link),
            })
        }
    }

    async fn terminate_session(&self, session: &Session) -> Result<()> {
        tracing::info!("Terminating local NOENV session: id={}", session.id);
        
        if let Some((_, local_session)) = self.state.live_sessions.remove(&session.id) {
            if local_session.pid != 0 {
                let _ = self.kill_pid(local_session.pid).await;
            }
            if let Some(ed_pid) = local_session.event_daemon_pid {
                let _ = self.kill_pid(ed_pid).await;
            }
            if let Err(e) = std::fs::remove_dir_all(&local_session.workspace_root) {
                tracing::warn!("Failed to remove workspace at {}: {:#}", local_session.workspace_root.display(), e);
            }
            let key_file_path = format!("/tmp/steadystate_authorized_keys_{}", session.id);
            if let Err(e) = std::fs::remove_file(&key_file_path) {
                tracing::warn!("Failed to remove key file at {}: {:#}", key_file_path, e);
            }
        } else {
            tracing::warn!("terminate_session called, but no live session state found for id={}", session.id);
        }
        Ok(())
    }
}

impl LocalComputeProvider {
    async fn start_event_daemon(&self, session_root: &Path) -> Result<Option<u32>> {
        tracing::info!("Starting event daemon for session: {}", session_root.display());
        
        // In a real deployment, we'd expect steady-eventd to be in PATH or relative to exe.
        // For dev, we can try to run it via cargo or look in target dir.
        
        let exe_path = std::env::current_exe()?;
        let bin_dir = exe_path.parent().unwrap();
        let daemon_path = bin_dir.join("steady-eventd");
        
        let cmd_path = if daemon_path.exists() {
            daemon_path
        } else {
            // Fallback for dev environment (running from source root?)
            // Try to find it in target/release or target/debug
            let mut p = std::env::current_dir()?.join("target/release/steady-eventd");
            if !p.exists() {
                p = std::env::current_dir()?.join("target/debug/steady-eventd");
            }
            p
        };
        
        if !cmd_path.exists() {
            tracing::warn!("steady-eventd binary not found at {}, skipping event daemon", cmd_path.display());
            return Ok(None);
        }

        let child = tokio::process::Command::new(cmd_path)
            .arg("--session-root")
            .arg(session_root)
            .stdout(std::process::Stdio::null())
            .stderr(std::process::Stdio::null())
            .spawn()
            .context("Failed to spawn steady-eventd")?;
            
        let pid = child.id();
        tracing::info!("Started steady-eventd with PID: {:?}", pid);
        
        Ok(pid)
    }

    async fn launch_sshd_for_collab(
        &self,
        session_root: &Path,
        github_user: Option<&str>,
        allowed_users: Option<&[String]>,
        session_id: &str,
        repo_name: &str,
        github_token: Option<&str>,
    ) -> Result<(u32, String)> {
        use tokio::process::Command;
        use tokio::time::timeout;
        tracing::info!("Launching SSHD for collab session {}", session_id);

        // ------------------------------------------------------------
        // NEW: Create secure sshd runtime directory
        // ------------------------------------------------------------
        let sshd_dir = session_root.join("sshd");
        std::fs::create_dir_all(&sshd_dir)
            .context("Failed to create sshd directory")?;
        use std::os::unix::fs::PermissionsExt;
        std::fs::set_permissions(&sshd_dir, std::fs::Permissions::from_mode(0o700))
            .context("Failed to chmod sshd directory")?;

        // 1. Generate Host Keys
        let host_key_path = sshd_dir.join("host_key");
        if !host_key_path.exists() {
            let key_str = host_key_path.to_string_lossy();
            let status = self.executor.run_status(
                "ssh-keygen", 
                &["-t", "ed25519", "-f", &key_str, "-N", ""]
            ).await.context("Failed to generate host key")?;
            
            if !status.success() {
                return Err(anyhow!("ssh-keygen failed"));
            }
        }
        
        // Ensure Host Key permissions are strict (0600)
        std::fs::set_permissions(&host_key_path, std::fs::Permissions::from_mode(0o600))
            .context("Failed to set host key permissions")?;

        // ------------------------------------------------------------
        // NEW: Paths for auth keys, config, logs, pid
        // ------------------------------------------------------------
        let auth_keys_path = sshd_dir.join("authorized_keys");
        let config_path = sshd_dir.join("sshd_config");
        let log_path = sshd_dir.join("sshd.log");
        let pid_path = sshd_dir.join("sshd.pid");

        // 2. Copy steadystate CLI binary to session bin
        let bin_dir = session_root.join("bin");
        std::fs::create_dir_all(&bin_dir).context("Failed to create bin dir")?;
        
        tracing::info!("Locating steadystate CLI binary...");
        
        let target_exe = bin_dir.join("steadystate");
        
        tokio::task::spawn_blocking(move || -> Result<()> {
            // Try to find steadystate CLI in the same directory as the backend first
            let current_exe = std::env::current_exe()?;
            let current_dir = current_exe.parent().ok_or_else(|| anyhow!("Failed to get current exe directory"))?;
            let mut cli_exe = current_dir.join("steadystate");
            
            // If not found, try to find it in PATH
            if !cli_exe.exists() {
                tracing::info!("steadystate binary not found in {:?}, searching PATH...", current_dir);
                if let Ok(output) = std::process::Command::new("which").arg("steadystate").output() {
                    if output.status.success() {
                        let path_str = String::from_utf8_lossy(&output.stdout).trim().to_string();
                        let path = PathBuf::from(path_str);
                        if path.exists() {
                            cli_exe = path;
                        }
                    }
                }
            }
            
            if !cli_exe.exists() {
                return Err(anyhow!("steadystate CLI binary not found at {} or in PATH. Ensure it is built and available.", cli_exe.display()));
            }

            tracing::info!("Found steadystate CLI at: {:?}", cli_exe);
            std::fs::copy(&cli_exe, &target_exe).context("Failed to copy steadystate binary")?;


            Ok(())
        }).await??;
        
        tracing::info!("steadystate CLI binary copied successfully.");
        
        // 3. Prepare Authorized Keys with ForceCommand
        let authorized_keys = fetch_authorized_keys(github_user, allowed_users, github_token).await;
        
        if authorized_keys.is_empty() {
            return Err(anyhow!("No authorized keys found. Cannot start SSHD without at least one key."));
        }
        
        // ------------------------------------------------------------
        // FIX: wrapper script also lives in secure directory
        // ------------------------------------------------------------
        let wrapper_path = sshd_dir.join("wrapper.sh");
        
        // Generate port early so we can include it in the magic link
        let port = 20000 + (rand::random::<u16>() % 10000);
        let user = std::env::var("USER").unwrap_or("steadystate".into());
        
        // Try to detect hostname/IP
        let hostname = if let Ok(host) = std::env::var("STEADYSTATE_ADVERTISED_HOST") {
            eprintln!("Using advertised host from env: {}", host);
            host
        } else {
            eprintln!("Attempting to detect public IP via 'ip route get 1.1.1.1'...");
            match tokio::process::Command::new("ip").args(&["route", "get", "1.1.1.1"]).output().await {
                Ok(output) => {
                    if output.status.success() {
                        let out = String::from_utf8_lossy(&output.stdout);
                        eprintln!("'ip route' output: {}", out.trim());
                        // Output format: "1.1.1.1 via ... src 192.168.178.42 ..."
                        if let Some(start) = out.find("src ") {
                            let rest = &out[start + 4..];
                            let ip = rest.split_whitespace().next().unwrap_or("localhost").to_string();
                            eprintln!("Parsed IP: {}", ip);
                            ip
                        } else {
                            eprintln!("Could not find 'src' in 'ip route' output");
                            "localhost".to_string()
                        }
                    } else {
                        let stderr = String::from_utf8_lossy(&output.stderr);
                        eprintln!("'ip route' failed with status {}: {}", output.status, stderr);
                        "localhost".to_string()
                    }
                },
                Err(e) => {
                    eprintln!("Failed to execute 'ip' command: {}", e);
                    "localhost".to_string()
                }
            }
        };

        let invite = format!("ssh://{}@{}:{}", user, hostname, port);
        let magic_link = format!("steadystate://collab/{}?ssh={}", session_id, urlencoding::encode(&invite));
        
        // Use /usr/bin/env bash for portability (NixOS doesn't always have /bin/bash)
        let wrapper_content = format!(r#"#!/usr/bin/env bash
set -e

USER_ID="$1"
export REPO_ROOT="{session_root}"
export PATH="$REPO_ROOT/bin:$PATH"
ACTIVE_USERS_FILE="$REPO_ROOT/active-users"
ACTIVITY_LOG="$REPO_ROOT/activity-log"
SYNC_LOG="$REPO_ROOT/sync-log"
export ACTIVITY_LOG
export SYNC_LOG
export SESSION_ROOT="$REPO_ROOT"
export NOENV_FLAKE_PATH="{noenv_flake_path}"
export JWT_SECRET="{jwt_secret}"
export SESSION_ID="{session_id}"
export MAGIC_LINK="{magic_link}"
export REPO_NAME="{repo_name}"

# Add user to active-users
echo "$USER_ID" >> "$ACTIVE_USERS_FILE"

# Cleanup function
cleanup() {{
    if [ -f "$ACTIVE_USERS_FILE" ]; then
        grep -v "^$USER_ID$" "$ACTIVE_USERS_FILE" > "$ACTIVE_USERS_FILE.tmp" 2>/dev/null || true
        mv "$ACTIVE_USERS_FILE.tmp" "$ACTIVE_USERS_FILE" 2>/dev/null || true
    fi
}}
trap cleanup EXIT

# Create worktree if not exists
WORKTREE="$REPO_ROOT/worktrees/$USER_ID"
if [ ! -d "$WORKTREE" ]; then
    echo "Creating workspace for $USER_ID..."
    mkdir -p "$REPO_ROOT/worktrees"
    
    # Clone from canonical repo
    echo "Cloning workspace..." >> "$REPO_ROOT/clone.log"
    # Use git clone with session branch
    git clone --branch steadystate/collab/{session_id} "$REPO_ROOT/canonical" "$WORKTREE" >> "$REPO_ROOT/clone.log" 2>&1 || {{
        echo "Failed to clone workspace" >> "$REPO_ROOT/clone.log"
        cat "$REPO_ROOT/clone.log"
        exit 1
    }}
    
    # Configure remote 'canonical'
    cd "$WORKTREE"
    git remote rename origin canonical
    
    # Configure user identity
    git config user.name "$USER_ID"
    git config user.email "$USER_ID@steadystate.local"
    
    # Initialize metadata
    mkdir -p "$WORKTREE/.worktree"
    HEAD_COMMIT=$(git rev-parse HEAD)
    echo "{{\"session_branch\": \"steadystate/collab/{session_id}\", \"last_synced_commit\": \"$HEAD_COMMIT\"}}" > "$WORKTREE/.worktree/steadystate.json"
    
    echo "Clone finished." >> "$REPO_ROOT/clone.log"
fi

# Set HOME to workspace for isolation
export HOME="$WORKTREE"
export USER_WORKSPACE="$WORKTREE"
export CANONICAL_REPO="$REPO_ROOT/canonical"

cd "$WORKTREE" || exit 1

# Define welcome message
print_welcome() {{
    cat << WELCOME
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Welcome to SteadyState Collaboration Mode          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your workspace: $WORKTREE

Commands:
  steadystate sync      - Sync your changes
  steadystate diff      - Show changes
  steadystate status    - Check status

WELCOME
}}

# Handle SSH_ORIGINAL_COMMAND
if [ -n "$SSH_ORIGINAL_COMMAND" ]; then
    # If the command is just setting up the shell (interactive), print welcome
    # We check for STEADYSTATE_USERNAME which is injected by the CLI for interactive sessions
    if [[ "$SSH_ORIGINAL_COMMAND" == *"STEADYSTATE_USERNAME"* ]]; then
        print_welcome
    fi
    exec bash -c "$SSH_ORIGINAL_COMMAND"
else
    # Default to shell
    print_welcome
    exec bash -l
fi
"#, 
        session_root = session_root.display(),
        noenv_flake_path = self.flake_path.display(),
        jwt_secret = std::env::var("JWT_SECRET").unwrap_or_else(|_| "placeholder_secret".to_string()),
        session_id = session_id,
        magic_link = magic_link,
        repo_name = repo_name
        );

        std::fs::write(&wrapper_path, wrapper_content).context("Failed to write wrapper script")?;
        std::fs::set_permissions(&wrapper_path, std::fs::Permissions::from_mode(0o755))?;

        // Write authorized keys
        {
            use std::io::Write;
            let mut file = std::fs::File::create(&auth_keys_path).context("Failed to create authorized_keys")?;
            std::fs::set_permissions(&auth_keys_path, std::fs::Permissions::from_mode(0o600))?;
            
            for ak in &authorized_keys {
                writeln!(file, "command=\"{} {}\" {}", wrapper_path.display(), ak.user, ak.key)?;
            }
        }

        // 4. Generate sshd_config with better settings
        // 4. Generate sshd_config with better settings
        // Port is already generated above
        
        // Find sshd path
        let sshd_path = if Path::new("/usr/sbin/sshd").exists() {
            "/usr/sbin/sshd".to_string()
        } else if Path::new("/usr/bin/sshd").exists() {
            "/usr/bin/sshd".to_string()
        } else {
            // Try to find absolute path using `which`
            match Command::new("which").arg("sshd").output().await {
                Ok(output) if output.status.success() => {
                    String::from_utf8_lossy(&output.stdout).trim().to_string()
                }
                _ => {
                    tracing::warn!("Could not find absolute path for sshd, using 'sshd'");
                    "sshd".to_string()
                }
            }
        };
        
        let sshd_config = format!(r#"# SteadyState SSH Configuration
Port {port}
ListenAddress 0.0.0.0
HostKey {host_key}
PidFile {pid}

# Authentication
PubkeyAuthentication yes
AuthorizedKeysFile {auth_keys}
PasswordAuthentication no
ChallengeResponseAuthentication no
UsePAM no
PermitRootLogin no

# Security
StrictModes no
PermitUserEnvironment yes
UsePrivilegeSeparation no

# Logging
SyslogFacility USER
LogLevel DEBUG3

# Session
PrintMotd no
PrintLastLog no
AcceptEnv LANG LC_*

# Subsystems
Subsystem sftp internal-sftp
"#, 
            port = port,
            host_key = host_key_path.display(),
            auth_keys = auth_keys_path.display(),
            pid = pid_path.display()
        );

        std::fs::write(&config_path, sshd_config)?;
        
        tracing::info!("SSHD config written to: {}", config_path.display());
        tracing::info!("Using SSHD binary: {}", sshd_path);
        tracing::info!("Listening on port: {}", port);

        // 5. Test the config first
        let test_output = Command::new(&sshd_path)
            .args(&["-t", "-f", config_path.to_str().unwrap()])
            .output()
            .await
            .context("Failed to test sshd config")?;
        
        if !test_output.status.success() {
            let stderr = String::from_utf8_lossy(&test_output.stderr);
            tracing::error!("SSHD config test failed: {}", stderr);
            return Err(anyhow!("Invalid SSHD configuration: {}", stderr));
        }
        
        tracing::info!("SSHD config test passed");

        // 6. Spawn sshd with explicit paths and logging to file
        // -D: no detach
        // -E log_file: append debug logs to log_file
        let args = vec!["-f", config_path.to_str().unwrap(), "-D", "-E", log_path.to_str().unwrap()];

        tracing::info!("Spawning sshd: {} {}", sshd_path, args.join(" "));
        
        let (pid, stdout, stderr) = self.executor.run_capture(&sshd_path, &args).await
            .context("Failed to spawn sshd")?;

        tracing::info!("SSHD spawned with PID {}", pid);
        
        // Spawn stderr logger to capture immediate startup errors
        let stderr_reader = BufReader::new(stderr);
        let stderr_lines = Arc::new(Mutex::new(Vec::new()));
        let stderr_lines_clone = stderr_lines.clone();
        
        tokio::spawn(async move {
            let mut lines = tokio_stream::wrappers::LinesStream::new(stderr_reader.lines());
            while let Some(line_res) = lines.next().await {
                if let Ok(line) = line_res {
                    tracing::warn!("SSHD STDERR: {}", line);
                    stderr_lines_clone.lock().unwrap().push(line);
                }
            }
        });
        
        // Wait longer for sshd to start
        tokio::time::sleep(std::time::Duration::from_secs(2)).await;
        
        // Check if process is still alive
        let check_cmd = format!("kill -0 {} 2>/dev/null", pid);
        match self.executor.run_status("sh", &["-c", &check_cmd]).await {
            Ok(status) if status.success() => {
                tracing::info!("SSHD process {} is running", pid);
            }
            _ => {
                tracing::error!("SSHD process {} died after launch", pid);
                
                // Collect captured stderr
                let captured_stderr = stderr_lines.lock().unwrap().join("\n");
                
                // Read the log file
                let log_content = std::fs::read_to_string(&log_path).unwrap_or_default();
                
                let error_msg = format!(
                    "SSHD process {} died immediately.\nCaptured STDERR:\n{}\nLog File Content:\n{}", 
                    pid, captured_stderr, log_content
                );
                
                tracing::error!("{}", error_msg);
                return Err(anyhow!(error_msg));
            }
        }

        // Try to connect to the port to verify it's listening
        let port_check = format!("nc -z localhost {} 2>/dev/null || (sleep 1 && nc -z localhost {})", port, port);
        match timeout(
            std::time::Duration::from_secs(5),
            self.executor.run_status("sh", &["-c", &port_check])
        ).await {
            Ok(Ok(status)) if status.success() => {
                tracing::info!("SSHD is listening on port {}", port);
            }
            _ => {
                tracing::warn!("Could not verify SSHD is listening on port {} (nc might not be available)", port);
                // Don't fail here, just warn
            }
        }

        // Use current user for invite link
        let current_user = std::env::var("USER").unwrap_or_else(|_| "steady".to_string());
        
        // Hostname is already detected above
        let invite = format!("ssh://{}@{}:{}", current_user, hostname, port);
        
        tracing::info!("SSHD ready. Connect with: {}", invite);
        
        Ok((pid, invite))
    }
}

struct AuthorizedKey {
    user: String,
    key: String,
}

async fn fetch_authorized_keys(github_user: Option<&str>, allowed_users: Option<&[String]>, github_token: Option<&str>) -> Vec<AuthorizedKey> {
    tracing::info!("Starting fetch_authorized_keys");
    // Use a Set to deduplicate keys
    let mut unique_keys = std::collections::HashSet::new();
    let mut result = Vec::new();
    
    let mut client_builder = reqwest::Client::builder()
        .timeout(std::time::Duration::from_secs(3))
        .user_agent("steadystate-backend");

    if let Some(token) = github_token {
        let mut headers = reqwest::header::HeaderMap::new();
        headers.insert(reqwest::header::AUTHORIZATION, format!("Bearer {}", token).parse().unwrap());
        client_builder = client_builder.default_headers(headers);
    }

    let client = client_builder.build().unwrap_or_default();

    // 1. Add local keys
    let home = std::env::var("HOME").unwrap_or_else(|_| "/root".to_string());
    tracing::info!("fetch_authorized_keys: HOME={}", home);
    
    let host_user_id = github_user.map(|s| s.to_string())
        .or_else(|| std::env::var("USER").ok())
        .unwrap_or_else(|| "host".to_string());
    
    let local_key_paths = vec![
        format!("{}/.ssh/id_ed25519.pub", home),
        format!("{}/.ssh/id_rsa.pub", home),
    ];

    for path in local_key_paths {
        tracing::info!("Checking for local key at: {}", path);
        if let Ok(content) = std::fs::read_to_string(&path) {
            for line in content.lines() {
                let trimmed = line.trim();
                if !trimmed.is_empty() && !trimmed.starts_with('#') {
                    if unique_keys.insert(trimmed.to_string()) {
                        result.push(AuthorizedKey { user: host_user_id.clone(), key: trimmed.to_string() });
                    }
                }
            }
            tracing::info!("Added local key from {}", path);
        } else {
            tracing::warn!("Could not read local key at {}", path);
        }
    }

    // 2. Fetch GitHub keys
    let mut users_to_fetch = Vec::new();
    if let Some(user) = github_user {
        users_to_fetch.push(user.to_string());
    }
    if let Some(users) = allowed_users {
        users_to_fetch.extend(users.iter().cloned());
    }

    tracing::info!("Fetching keys for users: {:?}", users_to_fetch);
    eprintln!("DEBUG: fetch_authorized_keys fetching for: {:?}", users_to_fetch);

    for user in users_to_fetch {
        let url = format!("https://github.com/{}.keys", user);
        tracing::info!("Fetching keys from {}", url);
        match client.get(&url).send().await {
            Ok(resp) => {
                if resp.status().is_success() {
                    if let Ok(text) = resp.text().await {
                        let mut count = 0;
                        for line in text.lines() {
                            let trimmed = line.trim();
                            if !trimmed.is_empty() && !trimmed.starts_with('#') {
                                if unique_keys.insert(trimmed.to_string()) {
                                    result.push(AuthorizedKey { user: user.to_string(), key: trimmed.to_string() });
                                    count += 1;
                                }
                            }
                        }
                        tracing::info!("Fetched {} keys for GitHub user {}", count, user);
                        eprintln!("DEBUG: Fetched {} keys for {}", count, user);
                    }
                } else {
                    tracing::warn!("Failed to fetch keys for {}: HTTP {}", user, resp.status());
                    eprintln!("DEBUG: Failed to fetch keys for {}: HTTP {}", user, resp.status());
                }
            }
            Err(e) => {
                tracing::warn!("Failed to fetch keys for {}: {}", user, e);
                eprintln!("DEBUG: Failed to fetch keys for {}: {}", user, e);
            }
        }
    }

    tracing::info!("fetch_authorized_keys completed. Found {} keys total.", result.len());
    result
}

#[derive(serde::Deserialize)]
struct GitHubCollaborator {
    login: String,
}

#[derive(serde::Deserialize)]
struct GitHubRepo {
    fork: bool,
    parent: Option<Box<GitHubRepoParent>>,
}

#[derive(serde::Deserialize)]
struct GitHubRepoParent {
    owner: GitHubOwner,
    name: String,
}

#[derive(serde::Deserialize)]
struct GitHubOwner {
    login: String,
}

async fn fetch_github_collaborators(owner: &str, repo: &str, token: &str) -> Result<Vec<String>> {
    tracing::info!("Fetching collaborators for {}/{}", owner, repo);
    eprintln!("DEBUG: fetch_github_collaborators called for {}/{}", owner, repo);
    
    let client = reqwest::Client::builder()
        .user_agent("steadystate-backend")
        .timeout(std::time::Duration::from_secs(10))
        .build()?;

    // 1. Fetch Repository Details to check for fork
    let repo_url = format!("https://api.github.com/repos/{}/{}", owner, repo);
    let repo_resp = client.get(&repo_url)
        .header("Authorization", format!("Bearer {}", token))
        .header("Accept", "application/vnd.github+json")
        .send()
        .await?;

    let mut upstream_owner_repo = None;

    if repo_resp.status().is_success() {
        if let Ok(repo_info) = repo_resp.json::<GitHubRepo>().await {
            if repo_info.fork {
                if let Some(parent) = repo_info.parent {
                    upstream_owner_repo = Some((parent.owner.login, parent.name));
                    eprintln!("DEBUG: Repository is a fork. Upstream: {}/{}", 
                        upstream_owner_repo.as_ref().unwrap().0, 
                        upstream_owner_repo.as_ref().unwrap().1
                    );
                }
            }
        }
    } else {
        eprintln!("DEBUG: Failed to fetch repo details: {}", repo_resp.status());
    }

    // Helper to fetch collaborators for a specific repo
    let fetch_for_repo = |o: &str, r: &str| {
        let o = o.to_string();
        let r = r.to_string();
        let client = client.clone();
        let token = token.to_string();
        async move {
            let url = format!("https://api.github.com/repos/{}/{}/collaborators?affiliation=all&per_page=100", o, r);
            let resp = client.get(&url)
                .header("Authorization", format!("Bearer {}", token))
                .header("Accept", "application/vnd.github+json")
                .send()
                .await?;
                
            if !resp.status().is_success() {
                let status = resp.status();
                let text = resp.text().await.unwrap_or_default();
                eprintln!("DEBUG: Failed to fetch collaborators for {}/{}: {} - {}", o, r, status, text);
                return Err(anyhow::anyhow!("HTTP {}", status));
            }
            
            let collaborators: Vec<GitHubCollaborator> = resp.json().await?;
            let logins: Vec<String> = collaborators.into_iter().map(|c| c.login).collect();
            Ok(logins)
        }
    };

    let mut all_collaborators = std::collections::HashSet::new();

    // 2. Fetch for current repo
    match fetch_for_repo(owner, repo).await {
        Ok(cols) => {
            eprintln!("DEBUG: Fetched {} collaborators from {}/{}", cols.len(), owner, repo);
            all_collaborators.extend(cols);
        },
        Err(e) => eprintln!("DEBUG: Error fetching current repo collaborators: {}", e),
    }

    // 3. Fetch for upstream if exists
    if let Some((up_owner, up_repo)) = upstream_owner_repo {
        match fetch_for_repo(&up_owner, &up_repo).await {
            Ok(cols) => {
                eprintln!("DEBUG: Fetched {} collaborators from upstream {}/{}", cols.len(), up_owner, up_repo);
                all_collaborators.extend(cols);
            },
            Err(e) => eprintln!("DEBUG: Error fetching upstream collaborators: {}", e),
        }
    }

    let result: Vec<String> = all_collaborators.into_iter().collect();
    tracing::info!("Found {} unique collaborators total", result.len());
    eprintln!("DEBUG: Found {} unique collaborators total: {:?}", result.len(), result);
    
    Ok(result)
}

########## ./backend/src/compute/mod.rs

// backend/src/compute/mod.rs

use crate::models::{Session, SessionRequest};

pub mod local_provider;

#[cfg(test)]
mod tests;

#[async_trait::async_trait]
pub trait ComputeProvider: Send + Sync + std::fmt::Debug {
    fn id(&self) -> &'static str;

    async fn start_session(
        &self,
        session_id: &str,
        request: &SessionRequest,
    ) -> anyhow::Result<crate::models::SessionStartResult>;

    async fn terminate_session(
        &self,
        session: &Session,
    ) -> anyhow::Result<()>;
}

########## ./backend/LICENSE

                    GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<https://www.gnu.org/licenses/>.

########## ./backend/.env

# Required
GITHUB_CLIENT_ID=Ov23linCEITb1YYs1AO2
GITHUB_CLIENT_SECRET=63e52112849fb9d27f1ab4e4126d0af9d1971276
JWT_SECRET=super_secret_32+_bytes_min

# Optional
DEVICE_POLL_MAX_INTERVAL_SECS=15
JWT_ISSUER=steadystate
JWT_TTL_SECS=900
REFRESH_TTL_SECS=1209600

########## ./.gitignore

# Generated by Cargo
# will have compiled files and executables
debug
target

# These are backup files generated by rustfmt
**/*.rs.bk

# MSVC Windows builds of rustc generate these, which store debugging information
*.pdb

# Generated by cargo mutants
# Contains mutation testing data
**/mutants.out*/

**/.env